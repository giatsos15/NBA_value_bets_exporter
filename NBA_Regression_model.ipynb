{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f2a745b",
      "metadata": {},
      "source": [
        "## NBA Analytics and Betting Value Analysis Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "07decdf1",
      "metadata": {
        "id": "07decdf1"
      },
      "outputs": [],
      "source": [
        "#--cell 1--#\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For data analysis\n",
        "from scipy import stats\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "01631fd1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching NBA betting data and lineups...\n",
            "Successfully fetched RAW odds rows: 1655 | columns: 263\n",
            "Successfully fetched lineups for 0 games\n",
            "Data successfully saved to nba_betting_data_20251103_160227.xlsx\n",
            "\n",
            "==================================================\n",
            "NBA BETTING DATA SUMMARY\n",
            "==================================================\n",
            "\n",
            "Betting Lines: 0 player-stat combinations\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "#--cell 2--#\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "class NBAOddsAndLineupsScraper:\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.setup_headers()\n",
        "    \n",
        "    def setup_headers(self):\n",
        "        \"\"\"Setup common headers for requests\"\"\"\n",
        "        self.headers = {\n",
        "            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
        "            'accept-language': 'en-US,en;q=0.9',\n",
        "            'cache-control': 'max-age=0',\n",
        "            'priority': 'u=0, i',\n",
        "            'referer': 'https://www.rotowire.com/',\n",
        "            'sec-ch-ua': '\"Chromium\";v=\"130\", \"Google Chrome\";v=\"130\", \"Not?A_Brand\";v=\"99\"',\n",
        "            'sec-ch-ua-mobile': '?0',\n",
        "            'sec-ch-ua-platform': '\"Windows\"',\n",
        "            'sec-fetch-dest': 'document',\n",
        "            'sec-fetch-mode': 'navigate',\n",
        "            'sec-fetch-site': 'same-origin',\n",
        "            'sec-fetch-user': '?1',\n",
        "            'upgrade-insecure-requests': '1',\n",
        "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36'\n",
        "        }\n",
        "\n",
        "    # --------- RAW WIDE ODDS (no aggregation) ---------------------------------\n",
        "    def get_player_props_odds_wide_raw(self, book='mgm'):\n",
        "        \"\"\"\n",
        "        Return the raw 'wide' odds table by scraping Rotowire's player-props page.\n",
        "        This preserves columns like:\n",
        "          name, team, opp, <book>_pts, <book>_ptsUnder, <book>_ptsOver, ...\n",
        "        Works across many books present in the page JSON blocks.\n",
        "        \"\"\"\n",
        "        url = f\"https://www.rotowire.com/betting/nba/player-props.php?book={book}\"\n",
        "        try:\n",
        "            r = self.session.get(url, headers=self.headers)\n",
        "            r.raise_for_status()\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to GET odds page: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Extract ALL JSON lists assigned to \"data:\" in the page\n",
        "        matches = re.findall(r\"data:\\s*(\\[\\{.*?\\}\\])\", r.text, flags=re.DOTALL)\n",
        "        frames = []\n",
        "        for m in matches:\n",
        "            try:\n",
        "                rows = json.loads(m)\n",
        "                if isinstance(rows, list) and rows:\n",
        "                    frames.append(pd.DataFrame(rows))\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        if not frames:\n",
        "            print(\"No odds JSON blocks found.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df = pd.concat(frames, ignore_index=True)\n",
        "        # keep the most useful id/basic columns if present\n",
        "        base_cols = [c for c in [\"name\",\"gameID\",\"playerID\",\"firstName\",\"lastName\",\"team\",\"opp\",\"logo\",\"playerLink\"] if c in df.columns]\n",
        "        other_cols = [c for c in df.columns if c not in base_cols]\n",
        "        df = df[base_cols + other_cols]\n",
        "        # normalize team/opponent field names\n",
        "        if \"opp\" in df.columns and \"opponent\" not in df.columns:\n",
        "            df = df.rename(columns={\"opp\": \"opponent\"})\n",
        "        # add as-of date and (best-guess) game_date if not present\n",
        "        df[\"asof_date\"] = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
        "        if \"game_date\" not in df.columns:\n",
        "            df[\"game_date\"] = df[\"asof_date\"]\n",
        "        print(f\"Successfully fetched RAW odds rows: {len(df)} | columns: {len(df.columns)}\")\n",
        "        return df\n",
        "\n",
        "    # --------- Legacy aggregated method (kept in case you still call it) ------\n",
        "    def get_player_props_odds(self, book='mgm'):\n",
        "        \"\"\"\n",
        "        Old helper that aggregated rows by 'name'.\n",
        "        Prefer get_player_props_odds_wide_raw() for modeling/joins.\n",
        "        \"\"\"\n",
        "        wide = self.get_player_props_odds_wide_raw(book=book)\n",
        "        if wide.empty:\n",
        "            return None\n",
        "        aggregated_df = wide.groupby('name', as_index=False, sort=False).agg(\n",
        "            lambda x: ', '.join(pd.Series(x).dropna().astype(str).unique())\n",
        "        )\n",
        "        aggregated_df = aggregated_df.dropna(axis=1, how='all')\n",
        "        print(f\"Successfully aggregated odds for {len(aggregated_df)} players\")\n",
        "        return aggregated_df\n",
        "\n",
        "    # --------- Lineups scraping (unchanged logic, made a bit sturdier) --------\n",
        "    def get_expected_lineups(self):\n",
        "        \"\"\"Get expected lineups from Rotowire\"\"\"\n",
        "        url = \"https://www.rotowire.com/basketball/nba-lineups.php\"\n",
        "        try:\n",
        "            r = self.session.get(url, headers=self.headers)\n",
        "            r.raise_for_status()\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to retrieve lineup page: {e}\")\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        game_containers = soup.find_all('div', class_='lineup__main')\n",
        "        if not game_containers:\n",
        "            print(\"No lineup data found\")\n",
        "            return None\n",
        "\n",
        "        lineups_data = []\n",
        "        for game in game_containers:\n",
        "            game_info = self._parse_game_info(game)\n",
        "            if game_info:\n",
        "                lineups_data.append(game_info)\n",
        "\n",
        "        print(f\"Successfully fetched lineups for {len(lineups_data)} games\")\n",
        "        return lineups_data\n",
        "\n",
        "    def _parse_game_info(self, game_container):\n",
        "        \"\"\"Parse individual game information and lineups\"\"\"\n",
        "        try:\n",
        "            game_data = {}\n",
        "            header = game_container.find('div', class_='lineup__hdr')\n",
        "            if header:\n",
        "                teams = header.find_all('div', class_='lineup__team')\n",
        "                if len(teams) >= 2:\n",
        "                    game_data['away_team'] = teams[0].get_text(strip=True)\n",
        "                    game_data['home_team'] = teams[1].get_text(strip=True)\n",
        "            time_info = header.find('div', class_='lineup__time') if header else None\n",
        "            if time_info:\n",
        "                game_data['game_time'] = time_info.get_text(strip=True)\n",
        "            lineup_containers = game_container.find_all('div', class_='lineup__box')\n",
        "            if len(lineup_containers) >= 2:\n",
        "                game_data['away_starters'] = self._parse_team_lineup(lineup_containers[0])\n",
        "                game_data['home_starters'] = self._parse_team_lineup(lineup_containers[1])\n",
        "            return game_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing game info: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _parse_team_lineup(self, team_container):\n",
        "        \"\"\"Parse individual team lineup\"\"\"\n",
        "        starters = []\n",
        "        try:\n",
        "            starters_section = team_container.find('div', class_='lineup__list')\n",
        "            if starters_section:\n",
        "                player_elements = starters_section.find_all('div', class_='lineup__player')\n",
        "                for player_elem in player_elements:\n",
        "                    player_info = self._parse_player_info(player_elem)\n",
        "                    if player_info:\n",
        "                        starters.append(player_info)\n",
        "            return starters\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing team lineup: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _parse_player_info(self, player_elem):\n",
        "        \"\"\"Parse individual player information\"\"\"\n",
        "        try:\n",
        "            player_data = {}\n",
        "            name_elem = player_elem.find('a', class_='lineup__player-link')\n",
        "            if name_elem:\n",
        "                player_data['name'] = name_elem.get_text(strip=True)\n",
        "                player_data['player_link'] = name_elem.get('href', '')\n",
        "            pos_elem = player_elem.find('span', class_='lineup__pos')\n",
        "            if pos_elem:\n",
        "                player_data['position'] = pos_elem.get_text(strip=True)\n",
        "            injury_elem = player_elem.find('span', class_='lineup__inj')\n",
        "            player_data['injury_status'] = injury_elem.get_text(strip=True) if injury_elem else 'Active'\n",
        "            confirmed_elem = player_elem.find('span', class_='lineup__confirm')\n",
        "            player_data['confirmed_starter'] = confirmed_elem is not None\n",
        "            return player_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing player info: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_comprehensive_data(self):\n",
        "        \"\"\"Get both odds and lineups data\"\"\"\n",
        "        print(\"Fetching NBA betting data and lineups...\")\n",
        "        odds_data = self.get_player_props_odds_wide_raw()  # <-- use RAW wide\n",
        "        lineups_data = self.get_expected_lineups()\n",
        "        combined_data = {\n",
        "            'odds': odds_data,\n",
        "            'lineups': lineups_data,\n",
        "            'last_updated': datetime.now().isoformat()\n",
        "        }\n",
        "        return combined_data\n",
        "    \n",
        "    def save_to_excel(self, data, filename=None):\n",
        "        \"\"\"Save the scraped data to Excel files\"\"\"\n",
        "        if filename is None:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f'nba_betting_data_{timestamp}.xlsx'\n",
        "        try:\n",
        "            with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "                if isinstance(data.get('odds'), pd.DataFrame) and not data['odds'].empty:\n",
        "                    data['odds'].to_excel(writer, sheet_name='Player_Odds', index=False)\n",
        "                if data.get('lineups') is not None:\n",
        "                    lineups_list = []\n",
        "                    for game in data['lineups']:\n",
        "                        for starter_type in ['away_starters', 'home_starters']:\n",
        "                            team = game.get('away_team' if starter_type == 'away_starters' else 'home_team', 'Unknown')\n",
        "                            starters = game.get(starter_type, [])\n",
        "                            for starter in starters:\n",
        "                                lineups_list.append({\n",
        "                                    'game_time': game.get('game_time', ''),\n",
        "                                    'team': team,\n",
        "                                    'player_name': starter.get('name', ''),\n",
        "                                    'position': starter.get('position', ''),\n",
        "                                    'injury_status': starter.get('injury_status', ''),\n",
        "                                    'confirmed_starter': starter.get('confirmed_starter', False),\n",
        "                                    'player_link': starter.get('player_link', '')\n",
        "                                })\n",
        "                    if lineups_list:\n",
        "                        lineups_df = pd.DataFrame(lineups_list)\n",
        "                        lineups_df.to_excel(writer, sheet_name='Expected_Lineups', index=False)\n",
        "                metadata = pd.DataFrame([{\n",
        "                    'last_updated': data.get('last_updated', ''),\n",
        "                    'total_games': len(data.get('lineups', [])) if isinstance(data.get('lineups'), list) else 0,\n",
        "                    'total_players_odds': len(data.get('odds', [])) if isinstance(data.get('odds'), pd.DataFrame) else 0\n",
        "                }])\n",
        "                metadata.to_excel(writer, sheet_name='Metadata', index=False)\n",
        "            print(f\"Data successfully saved to {filename}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving to Excel: {e}\")\n",
        "            return False\n",
        "\n",
        "# Usage example and integration with your existing analytics\n",
        "def integrate_with_analytics():\n",
        "    \"\"\"Integrate the scraper with your existing analytics\"\"\"\n",
        "    scraper = NBAOddsAndLineupsScraper()\n",
        "    nba_data = scraper.get_comprehensive_data()\n",
        "    scraper.save_to_excel(nba_data)\n",
        "    processed_data = process_for_analytics(nba_data)\n",
        "    return processed_data\n",
        "\n",
        "def process_for_analytics(nba_data):\n",
        "    \"\"\"Process the scraped data for use in analytics\"\"\"\n",
        "    processed = {}\n",
        "    # Odds data â†’ extract basic lines for PTS/REB/AST if present\n",
        "    if isinstance(nba_data.get('odds'), pd.DataFrame) and not nba_data['odds'].empty:\n",
        "        odds_df = nba_data['odds']\n",
        "        def pick_line(row, market):\n",
        "            # Look for any <book>_<marketLower> columns (line, Under, Over)\n",
        "            m = market.lower()\n",
        "            line = None\n",
        "            over = None\n",
        "            under = None\n",
        "            for col in row.index:\n",
        "                c = col.lower()\n",
        "                if c.endswith(f\"_{m}\"):\n",
        "                    line = row[col]\n",
        "                elif c.endswith(f\"_{m}over\"):\n",
        "                    over = row[col]\n",
        "                elif c.endswith(f\"_{m}under\"):\n",
        "                    under = row[col]\n",
        "            try:\n",
        "                line = float(line) if line is not None and str(line).strip() not in (\"\", \"None\", \"nan\") else None\n",
        "            except Exception:\n",
        "                line = None\n",
        "            return line, over, under\n",
        "\n",
        "        betting_lines = []\n",
        "        for _, r in odds_df.iterrows():\n",
        "            player_name = r.get('name', '')\n",
        "            for mk in [\"pts\", \"reb\", \"ast\"]:\n",
        "                line, over, under = pick_line(r, mk)\n",
        "                if line is not None:\n",
        "                    betting_lines.append({\n",
        "                        \"player\": player_name,\n",
        "                        \"stat\": {\"pts\":\"points\",\"reb\":\"rebounds\",\"ast\":\"assists\"}[mk],\n",
        "                        \"line\": line,\n",
        "                        \"over_odds\": over,\n",
        "                        \"under_odds\": under\n",
        "                    })\n",
        "        processed['betting_lines'] = pd.DataFrame(betting_lines)\n",
        "\n",
        "    # Lineups\n",
        "    if nba_data.get('lineups') is not None:\n",
        "        lineups = nba_data['lineups']\n",
        "        team_players = {}\n",
        "        for game in lineups:\n",
        "            away_team = game.get('away_team')\n",
        "            if away_team and away_team not in team_players:\n",
        "                team_players[away_team] = []\n",
        "            for starter in game.get('away_starters', []):\n",
        "                if away_team and starter.get('name'):\n",
        "                    team_players[away_team].append({\n",
        "                        'name': starter['name'],\n",
        "                        'position': starter.get('position', ''),\n",
        "                        'status': starter.get('injury_status', 'Active'),\n",
        "                        'confirmed': starter.get('confirmed_starter', False)\n",
        "                    })\n",
        "            home_team = game.get('home_team')\n",
        "            if home_team and home_team not in team_players:\n",
        "                team_players[home_team] = []\n",
        "            for starter in game.get('home_starters', []):\n",
        "                if home_team and starter.get('name'):\n",
        "                    team_players[home_team].append({\n",
        "                        'name': starter['name'],\n",
        "                        'position': starter.get('position', ''),\n",
        "                        'status': starter.get('injury_status', 'Active'),\n",
        "                        'confirmed': starter.get('confirmed_starter', False)\n",
        "                    })\n",
        "        processed['team_lineups'] = team_players\n",
        "        processed['games_today'] = lineups\n",
        "    return processed\n",
        "\n",
        "def extract_betting_line(player_row, stat_type):\n",
        "    \"\"\"Extract betting line for specific stat type (legacy helper)\"\"\"\n",
        "    line_col = over_odds_col = under_odds_col = None\n",
        "    for col in player_row.index:\n",
        "        col_lower = col.lower()\n",
        "        if stat_type in col_lower and 'line' in col_lower:\n",
        "            line_col = col\n",
        "        elif stat_type in col_lower and 'over' in col_lower and 'odds' in col_lower:\n",
        "            over_odds_col = col\n",
        "        elif stat_type in col_lower and 'under' in col_lower and 'odds' in col_lower:\n",
        "            under_odds_col = col\n",
        "    line_value = player_row.get(line_col) if line_col else None\n",
        "    if line_value and str(line_value).replace('.', '').isdigit():\n",
        "        return {\n",
        "            'line': float(line_value),\n",
        "            'over_odds': player_row.get(over_odds_col) if over_odds_col else None,\n",
        "            'under_odds': player_row.get(under_odds_col) if under_odds_col else None\n",
        "        }\n",
        "    return None\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    data = integrate_with_analytics()\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"NBA BETTING DATA SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    if data.get('betting_lines') is not None:\n",
        "        print(f\"\\nBetting Lines: {len(data['betting_lines'])} player-stat combinations\")\n",
        "        print(data['betting_lines'].head(10))\n",
        "    if data.get('team_lineups'):\n",
        "        print(f\"\\nTeams with Lineups: {len(data['team_lineups'])}\")\n",
        "        for team, players in list(data['team_lineups'].items())[:3]:\n",
        "            print(f\"{team}: {len(players)} players\")\n",
        "            for player in players[:3]:\n",
        "                print(f\"  - {player['name']} ({player['position']}) - {player['status']}\")\n",
        "    if data.get('games_today'):\n",
        "        print(f\"\\nGames Today: {len(data['games_today'])}\")\n",
        "        for game in data['games_today'][:3]:\n",
        "            print(f\"{game.get('away_team', 'TBD')} @ {game.get('home_team', 'TBD')} - {game.get('game_time', 'Time TBD')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "25a19799",
      "metadata": {},
      "outputs": [],
      "source": [
        "#--cell 3--#\n",
        "def get_daily_matchups(date=None):\n",
        "    \"\"\"Get NBA games for a specific date\"\"\"\n",
        "    if date is None:\n",
        "        date = datetime.now().strftime('%Y-%m-%d')\n",
        "    # Placeholder demo; replace with a real schedule API if desired\n",
        "    sample_matchups = [\n",
        "        {'home_team': 'GSW', 'away_team': 'LAL', 'time': '7:30 PM ET'},\n",
        "        {'home_team': 'BOS', 'away_team': 'MIA', 'time': '8:00 PM ET'},\n",
        "        {'home_team': 'DEN', 'away_team': 'DAL', 'time': '9:00 PM ET'},\n",
        "    ]\n",
        "    return sample_matchups\n",
        "\n",
        "def calculate_player_correlations(player_a_logs, player_b_logs):\n",
        "    \"\"\"Calculate correlation between two players' performances\"\"\"\n",
        "    merged = pd.merge(player_a_logs, player_b_logs, on='GAME_DATE', suffixes=('_a', '_b'))\n",
        "    correlations = {}\n",
        "    for stat in ['PTS', 'REB', 'AST']:\n",
        "        if f'{stat}_a' in merged.columns and f'{stat}_b' in merged.columns:\n",
        "            corr = merged[f'{stat}_a'].corr(merged[f'{stat}_b'])\n",
        "            correlations[stat] = corr\n",
        "    return correlations\n",
        "\n",
        "# Export results to Excel\n",
        "def export_analysis(results, filename='nba_betting_analysis.xlsx'):\n",
        "    \"\"\"Export analysis results to Excel\"\"\"\n",
        "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "        if 'value_bets' in results:\n",
        "            pd.DataFrame(results['value_bets']).to_excel(writer, sheet_name='Value_Bets', index=False)\n",
        "        if 'predictions' in results:\n",
        "            predictions_df = pd.DataFrame.from_dict(results['predictions'], orient='index')\n",
        "            predictions_df.to_excel(writer, sheet_name='Player_Predictions')\n",
        "    print(f\"Analysis exported to {filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c0368a",
      "metadata": {},
      "source": [
        "## NBA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f4896f17",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ€ Fetching NBA stats for 2023-24...\n",
            "â†’ Attempt 1 fetching 2023-24 data...\n",
            "âœ… 2023-24: saved 572 player records to 'nba_player_stats_2023_24.csv'\n",
            "\n",
            "ðŸ€ Fetching NBA stats for 2024-25...\n",
            "â†’ Attempt 1 fetching 2024-25 data...\n",
            "âœ… 2024-25: saved 569 player records to 'nba_player_stats_2024_25.csv'\n",
            "\n",
            "ðŸŽ‰ Done! Both 2023-24 and 2024-25 seasons downloaded.\n"
          ]
        }
      ],
      "source": [
        "#--cell 4--#\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "url = \"https://stats.nba.com/stats/leaguedashplayerstats\"\n",
        "\n",
        "base_params = {\n",
        "    \"College\": \"\",\n",
        "    \"Conference\": \"\",\n",
        "    \"Country\": \"\",\n",
        "    \"DateFrom\": \"\",\n",
        "    \"DateTo\": \"\",\n",
        "    \"Division\": \"\",\n",
        "    \"DraftPick\": \"\",\n",
        "    \"DraftYear\": \"\",\n",
        "    \"GameScope\": \"\",\n",
        "    \"GameSegment\": \"\",\n",
        "    \"Height\": \"\",\n",
        "    \"ISTRound\": \"\",\n",
        "    \"LastNGames\": \"0\",\n",
        "    \"LeagueID\": \"00\",\n",
        "    \"Location\": \"\",\n",
        "    \"MeasureType\": \"Base\",\n",
        "    \"Month\": \"0\",\n",
        "    \"OpponentTeamID\": \"0\",\n",
        "    \"Outcome\": \"\",\n",
        "    \"PORound\": \"0\",\n",
        "    \"PaceAdjust\": \"N\",\n",
        "    \"PerMode\": \"PerGame\",\n",
        "    \"Period\": \"0\",\n",
        "    \"PlayerExperience\": \"\",\n",
        "    \"PlayerPosition\": \"\",\n",
        "    \"PlusMinus\": \"N\",\n",
        "    \"Rank\": \"N\",\n",
        "    \"SeasonSegment\": \"\",\n",
        "    \"SeasonType\": \"Regular Season\",\n",
        "    \"ShotClockRange\": \"\",\n",
        "    \"StarterBench\": \"\",\n",
        "    \"TeamID\": \"0\",\n",
        "    \"VsConference\": \"\",\n",
        "    \"VsDivision\": \"\",\n",
        "    \"Weight\": \"\"\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    \"Accept\": \"application/json, text/plain, */*\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "    \"Origin\": \"https://www.nba.com\",\n",
        "    \"Referer\": \"https://www.nba.com/\",\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/141.0.0.0 Safari/537.36\",\n",
        "    \"x-nba-stats-origin\": \"stats\",\n",
        "    \"x-nba-stats-token\": \"true\"\n",
        "}\n",
        "\n",
        "seasons = [\"2023-24\", \"2024-25\"]\n",
        "\n",
        "def fetch_season_data(season, retries=3):\n",
        "    \"\"\"Fetch one seasonâ€™s player stats, retrying if timeout or network error.\"\"\"\n",
        "    params = base_params.copy()\n",
        "    params[\"Season\"] = season\n",
        "\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            print(f\"â†’ Attempt {attempt} fetching {season} data...\")\n",
        "            response = requests.get(url, headers=headers, params=params, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"âš ï¸ Timeout on attempt {attempt}/{retries} for {season}. Retrying...\")\n",
        "            time.sleep(3 * attempt)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"âŒ Error on attempt {attempt}/{retries}: {e}\")\n",
        "            time.sleep(3 * attempt)\n",
        "    raise RuntimeError(f\"Failed to fetch {season} data after {retries} attempts.\")\n",
        "\n",
        "# Main loop\n",
        "for season in seasons:\n",
        "    print(f\"\\nðŸ€ Fetching NBA stats for {season}...\")\n",
        "    data = fetch_season_data(season)\n",
        "\n",
        "    headers_list = data[\"resultSets\"][0][\"headers\"]\n",
        "    rows = data[\"resultSets\"][0][\"rowSet\"]\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=headers_list)\n",
        "    filename = f\"nba_player_stats_{season.replace('-', '_')}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\"âœ… {season}: saved {len(df)} player records to '{filename}'\")\n",
        "\n",
        "    # Wait 3â€“6 seconds before next season to avoid throttling\n",
        "    time.sleep(random.uniform(3, 6))\n",
        "\n",
        "print(\"\\nðŸŽ‰ Done! Both 2023-24 and 2024-25 seasons downloaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f33a76b7",
      "metadata": {},
      "source": [
        "## GAME LOGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7e080d50",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching 2023-24...\n",
            "âœ… Saved 26401 records for 2023-24\n",
            "Fetching 2024-25...\n",
            "âœ… Saved 26306 records for 2024-25\n",
            "Fetching 2025-26...\n",
            "âœ… Saved 2115 records for 2025-26\n"
          ]
        }
      ],
      "source": [
        "#--cell 5--#\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def get_box_scores(season, season_type=\"Regular Season\"):\n",
        "    url = \"https://stats.nba.com/stats/leaguegamelog\"\n",
        "    params = {\n",
        "        \"Counter\": 1000,\n",
        "        \"DateFrom\": \"\",\n",
        "        \"DateTo\": \"\",\n",
        "        \"Direction\": \"DESC\",\n",
        "        \"ISTRound\": \"\",\n",
        "        \"LeagueID\": \"00\",\n",
        "        \"PlayerOrTeam\": \"P\",\n",
        "        \"Season\": season,\n",
        "        \"SeasonType\": season_type,\n",
        "        \"Sorter\": \"DATE\"\n",
        "    }\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36\",\n",
        "        \"Referer\": \"https://www.nba.com/\",\n",
        "        \"Origin\": \"https://www.nba.com\",\n",
        "        \"Accept\": \"application/json, text/plain, */*\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params, headers=headers, timeout=30)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    data = response.json()[\"resultSets\"][0]\n",
        "    df = pd.DataFrame(data[\"rowSet\"], columns=data[\"headers\"])\n",
        "    return df\n",
        "\n",
        "# Get all three seasons\n",
        "seasons = [\"2023-24\", \"2024-25\", \"2025-26\"]\n",
        "for season in seasons:\n",
        "    print(f\"Fetching {season}...\")\n",
        "    df = get_box_scores(season)\n",
        "    df.to_csv(f\"nba_boxscores_{season}.csv\", index=False)\n",
        "    print(f\"âœ… Saved {len(df)} records for {season}\")\n",
        "    time.sleep(2)  # polite delay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a49f8cf8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Saved: nba_player_stats_2023_24_enriched.csv\n",
            "âœ… Saved: nba_player_stats_2024_25_enriched.csv\n",
            "ðŸ€ Combined: nba_player_stats_2023_25_combined.csv\n"
          ]
        }
      ],
      "source": [
        "#--cell 6--#\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import io\n",
        "import unicodedata\n",
        "\n",
        "# ---- Keep/Map settings -------------------------------------------------------\n",
        "\n",
        "ADV_COLS_KEEP = [\n",
        "    \"Player\", \"Pos\", \"Age\", \"Tm\", \"G\", \"MP\",\n",
        "    \"PER\", \"TS%\", \"3PAr\", \"FTr\",\n",
        "    \"ORB%\", \"DRB%\", \"TRB%\",\n",
        "    \"AST%\", \"STL%\", \"BLK%\",\n",
        "    \"TOV%\", \"USG%\",\n",
        "    \"ORtg\", \"DRtg\",\n",
        "    \"OWS\", \"DWS\", \"WS\", \"WS/48\",\n",
        "    \"OBPM\", \"DBPM\", \"BPM\", \"VORP\"\n",
        "]\n",
        "\n",
        "# Basketball-Reference -> NBA/your dataset codes\n",
        "TEAM_ABBR_MAP = {\n",
        "    \"BRK\": \"BKN\",\n",
        "    \"PHO\": \"PHX\",\n",
        "    \"CHO\": \"CHA\",\n",
        "    \"UTH\": \"UTA\",   # rare alias safety\n",
        "    \"NJN\": \"BKN\",   # historical\n",
        "    \"SEA\": \"OKC\",   # historical\n",
        "    \"VAN\": \"MEM\",   # historical\n",
        "}\n",
        "\n",
        "# ---- Helpers -----------------------------------------------------------------\n",
        "\n",
        "def normalize_name(s):\n",
        "    \"\"\"Normalize player names for consistent joining (lowercase, no accents/punct).\"\"\"\n",
        "    if pd.isna(s):\n",
        "        return s\n",
        "    s = s.strip().lower()\n",
        "    s = unicodedata.normalize(\"NFKD\", s)\n",
        "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
        "    for ch in [\".\", \"'\", \"`\", \"â€™\", \"â€œ\", \"â€\", \",\"]:\n",
        "        s = s.replace(ch, \"\")\n",
        "    s = \" \".join(s.split())\n",
        "    return s\n",
        "\n",
        "# ---- Fetch advanced table from Basketball-Reference --------------------------\n",
        "\n",
        "def fetch_advanced_table(season=2026):\n",
        "    \"\"\"\n",
        "    Fetch and clean Basketball-Reference advanced stats table for a given season.\n",
        "    Example: season=2025 -> https://www.basketball-reference.com/leagues/NBA_2025_advanced.html\n",
        "    \"\"\"\n",
        "    url = f\"https://www.basketball-reference.com/leagues/NBA_{season}_advanced.html\"\n",
        "    headers = {\n",
        "        \"User-Agent\": (\n",
        "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
        "        ),\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
        "    }\n",
        "    resp = requests.get(url, headers=headers, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "\n",
        "    tables = pd.read_html(io.StringIO(resp.text), header=0)\n",
        "    if not tables:\n",
        "        raise RuntimeError(\"No tables found on Basketball-Reference page.\")\n",
        "\n",
        "    df = tables[0].copy()\n",
        "\n",
        "    # Remove duplicate header rows\n",
        "    if \"Rk\" in df.columns:\n",
        "        df = df[df[\"Rk\"] != \"Rk\"].copy()\n",
        "        df.drop(columns=[\"Rk\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    # Normalize column names (strip and upper-case for easy access)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    # Basketball Reference sometimes labels the team column differently â€” make sure it exists\n",
        "    team_col = None\n",
        "    for c in df.columns:\n",
        "        if c.lower() in [\"tm\", \"team\", \"team_name\"]:\n",
        "            team_col = c\n",
        "            break\n",
        "    if not team_col:\n",
        "        raise KeyError(f\"Could not find a team column in advanced table. Found: {df.columns.tolist()}\")\n",
        "    df.rename(columns={team_col: \"Tm\"}, inplace=True)\n",
        "\n",
        "    # Keep relevant columns if present\n",
        "    keep = [c for c in ADV_COLS_KEEP if c in df.columns]\n",
        "    df = df[keep].copy()\n",
        "\n",
        "    # Convert numeric columns\n",
        "    non_numeric = {\"Player\", \"Pos\", \"Tm\"}\n",
        "    for c in [c for c in df.columns if c not in non_numeric]:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    # Map team abbreviations to match your dataset\n",
        "    df[\"Tm\"] = df[\"Tm\"].replace(TEAM_ABBR_MAP)\n",
        "\n",
        "    # Add join keys\n",
        "    df[\"player_key\"] = df[\"Player\"].map(normalize_name)\n",
        "    df[\"team_key\"] = df[\"Tm\"].astype(str).str.strip().str.upper()\n",
        "\n",
        "    return df\n",
        "\n",
        "# ---- Load your averages CSV and align columns --------------------------------\n",
        "\n",
        "def load_averages_csv(path):\n",
        "    \"\"\"\n",
        "    Load your NBA averages CSV (with headers like PLAYER_NAME, TEAM_ABBREVIATION).\n",
        "    Renames to canonical 'Player' and 'Team' and adds join keys.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Auto-map your headers to canonical names\n",
        "    col_map = {}\n",
        "    for c in df.columns:\n",
        "        cl = c.strip().lower()\n",
        "        if cl == \"player_name\":\n",
        "            col_map[c] = \"Player\"\n",
        "        elif cl in (\"team_abbreviation\", \"tm\", \"team\"):\n",
        "            col_map[c] = \"Team\"\n",
        "        # keep other columns as-is\n",
        "\n",
        "    df = df.rename(columns=col_map)\n",
        "\n",
        "    if \"Player\" not in df.columns or \"Team\" not in df.columns:\n",
        "        raise ValueError(\n",
        "            \"Couldn't find columns for 'Player' and 'Team'. \"\n",
        "            f\"Available columns: {list(df.columns)}\"\n",
        "        )\n",
        "\n",
        "    # Join keys\n",
        "    df[\"player_key\"] = df[\"Player\"].map(normalize_name)\n",
        "    df[\"team_key\"] = df[\"Team\"].astype(str).str.strip().str.upper()\n",
        "\n",
        "    return df\n",
        "\n",
        "# ---- Merge logic (with TOT fallback for traded players) ----------------------\n",
        "\n",
        "def merge_advanced_into_averages(df_avg, df_adv):\n",
        "    \"\"\"\n",
        "    Merge advanced metrics into averages.\n",
        "    1) Exact Player+Team match (ignore TOT).\n",
        "    2) For remaining NaNs, fill from TOT row by Player.\n",
        "    \"\"\"\n",
        "    adv_team = df_adv[df_adv[\"Tm\"] != \"TOT\"].copy()\n",
        "    adv_tot  = df_adv[df_adv[\"Tm\"] == \"TOT\"].copy()\n",
        "\n",
        "    adv_cols_to_add = [c for c in df_adv.columns if c not in {\"Player\", \"Pos\", \"Age\", \"Tm\", \"player_key\", \"team_key\"}]\n",
        "    meta_cols = [c for c in [\"Pos\", \"Age\"] if c in df_adv.columns]\n",
        "    join_cols_full = meta_cols + adv_cols_to_add\n",
        "\n",
        "    merged = df_avg.merge(\n",
        "        adv_team[[\"player_key\", \"team_key\"] + join_cols_full],\n",
        "        on=[\"player_key\", \"team_key\"],\n",
        "        how=\"left\",\n",
        "        suffixes=(\"\", \"_adv\"),\n",
        "    )\n",
        "\n",
        "    # Determine \"missing\" based on a representative advanced column\n",
        "    probe_col = \"PER\" if \"PER\" in merged.columns else (\"WS/48\" if \"WS/48\" in merged.columns else None)\n",
        "    missing_mask = merged[probe_col].isna() if probe_col else merged.isna().any(axis=1)\n",
        "\n",
        "    if missing_mask.any() and not adv_tot.empty:\n",
        "        fallback = merged[missing_mask].merge(\n",
        "            adv_tot[[\"player_key\"] + join_cols_full],\n",
        "            on=\"player_key\",\n",
        "            how=\"left\",\n",
        "            suffixes=(\"\", \"_tot\"),\n",
        "        )\n",
        "        for col in join_cols_full:\n",
        "            if col in merged.columns and col in fallback.columns:\n",
        "                merged.loc[missing_mask, col] = merged.loc[missing_mask, col].fillna(fallback[col])\n",
        "\n",
        "    return merged\n",
        "\n",
        "# ==============================================================================\n",
        "# Example usage for your two files\n",
        "# ==============================================================================\n",
        "\n",
        "# ---- 2023â€“24 (Basketball-Reference season code = 2024) -----------------------\n",
        "df_avg_2024 = load_averages_csv(\"nba_player_stats_2023_24.csv\")\n",
        "df_adv_2024 = fetch_advanced_table(season=2024)\n",
        "df_enriched_2024 = merge_advanced_into_averages(df_avg_2024, df_adv_2024)\n",
        "df_enriched_2024.to_csv(\"nba_player_stats_2023_24_enriched.csv\", index=False)\n",
        "print(\"âœ… Saved: nba_player_stats_2023_24_enriched.csv\")\n",
        "\n",
        "# ---- 2024â€“25 (Basketball-Reference season code = 2025) -----------------------\n",
        "df_avg_2025 = load_averages_csv(\"nba_player_stats_2024_25.csv\")\n",
        "df_adv_2025 = fetch_advanced_table(season=2025)\n",
        "df_enriched_2025 = merge_advanced_into_averages(df_avg_2025, df_adv_2025)\n",
        "df_enriched_2025.to_csv(\"nba_player_stats_2024_25_enriched.csv\", index=False)\n",
        "print(\"âœ… Saved: nba_player_stats_2024_25_enriched.csv\")\n",
        "\n",
        "# ---- (Optional) Combine both seasons into one file ---------------------------\n",
        "df_combined = pd.concat([df_enriched_2024, df_enriched_2025], ignore_index=True)\n",
        "df_combined.to_csv(\"nba_player_stats_2023_25_combined.csv\", index=False)\n",
        "print(\"ðŸ€ Combined: nba_player_stats_2023_25_combined.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87786090",
      "metadata": {},
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "05885691",
      "metadata": {},
      "outputs": [],
      "source": [
        "#--cell 7--#\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Input file assumptions:\n",
        "# - You have player game logs with at least:\n",
        "#   ['GAME_DATE', 'PLAYER_NAME', 'TEAM_ABBREVIATION', 'OPPONENT_ABBREVIATION',\n",
        "#    'MIN', 'PTS', 'REB', 'AST', 'FGM', 'FGA', 'FG3M', 'FG3A', 'FTM', 'FTA', 'TOV', 'BLK', 'STL', 'PLUS_MINUS', 'START_POSITION']\n",
        "#   Column names can be auto-mapped below if they differ slightly (e.g., 'TEAM_ID' not needed).\n",
        "# -----------------------------\n",
        "\n",
        "def standardize_logs_cols(df_logs: pd.DataFrame) -> pd.DataFrame:\n",
        "    colmap = {}\n",
        "    for c in df_logs.columns:\n",
        "        cl = c.strip().lower()\n",
        "        if cl in [\"game_date\", \"game_date_est\", \"date\"]:\n",
        "            colmap[c] = \"GAME_DATE\"\n",
        "        elif cl in [\"player\", \"player_name\"]:\n",
        "            colmap[c] = \"PLAYER_NAME\"\n",
        "        elif cl in [\"team\", \"team_abbreviation\", \"tm\"]:\n",
        "            colmap[c] = \"TEAM_ABBREVIATION\"\n",
        "        elif cl in [\"opp\", \"opponent\", \"opponent_abbreviation\"]:\n",
        "            colmap[c] = \"OPPONENT_ABBREVIATION\"\n",
        "        elif cl in [\"min\", \"minutes\"]:\n",
        "            colmap[c] = \"MIN\"\n",
        "    df = df_logs.rename(columns=colmap).copy()\n",
        "    # types\n",
        "    df[\"GAME_DATE\"] = pd.to_datetime(df[\"GAME_DATE\"])\n",
        "    df = df.sort_values([\"PLAYER_NAME\", \"GAME_DATE\"])\n",
        "    return df\n",
        "\n",
        "def add_shooting_efficiency(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Compute TS% from game logs (per game)\n",
        "    # TS% = PTS / (2*(FGA + 0.44*FTA))\n",
        "    for col in [\"FGA\", \"FTA\", \"PTS\"]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = 0.0\n",
        "    denom = 2 * (df[\"FGA\"].astype(float) + 0.44 * df[\"FTA\"].astype(float))\n",
        "    df[\"TS_game\"] = np.where(denom > 0, df[\"PTS\"].astype(float) / denom, np.nan)\n",
        "    return df\n",
        "\n",
        "def rolling_player_form(df: pd.DataFrame, windows=(3,5,10,20)) -> pd.DataFrame:\n",
        "    # Rolling stats per player before each game\n",
        "    df = df.sort_values([\"PLAYER_NAME\", \"GAME_DATE\"]).copy()\n",
        "    group = df.groupby(\"PLAYER_NAME\", group_keys=False)\n",
        "    for w in windows:\n",
        "        for stat in [\"PTS\", \"REB\", \"AST\", \"MIN\", \"TS_game\"]:\n",
        "            if stat not in df.columns:\n",
        "                df[stat] = np.nan\n",
        "            col = f\"{stat}_roll{w}\"\n",
        "            df[col] = group[stat].shift(1).rolling(w, min_periods=1).mean()\n",
        "    # recent usage proxy: last-5 share of team FGA\n",
        "    if {\"FGA\",\"TEAM_ABBREVIATION\"}.issubset(df.columns):\n",
        "        df[\"teamFGA_game\"] = df.groupby([\"TEAM_ABBREVIATION\",\"GAME_DATE\"])[\"FGA\"].transform(\"sum\")\n",
        "        df[\"usage_share\"] = np.where(df[\"teamFGA_game\"]>0, df[\"FGA\"]/df[\"teamFGA_game\"], np.nan)\n",
        "        df[\"usage_share_roll5\"] = group[\"usage_share\"].shift(1).rolling(5, min_periods=1).mean()\n",
        "    return df\n",
        "\n",
        "def team_daily_ratings(df: pd.DataFrame, windows=(5,10)):\n",
        "    # Build team-level ORtg/DRtg/Pace rolling using box score approximations\n",
        "    # Possessions â‰ˆ FGA + 0.44*FTA - OREB + TOV (OREB optional if present)\n",
        "    need_cols = [\"TEAM_ABBREVIATION\",\"OPPONENT_ABBREVIATION\",\"GAME_DATE\",\"PTS\",\"FGA\",\"FTA\",\"TOV\",\"OREB\"]\n",
        "    for c in need_cols:\n",
        "        if c not in df.columns:\n",
        "            df[c] = 0.0\n",
        "    # aggregate team totals per game\n",
        "    g = df.groupby([\"GAME_DATE\",\"TEAM_ABBREVIATION\"], as_index=False).agg(\n",
        "        PTS_team=(\"PTS\",\"sum\"), FGA=(\"FGA\",\"sum\"), FTA=(\"FTA\",\"sum\"),\n",
        "        TOV=(\"TOV\",\"sum\"), OREB=(\"OREB\",\"sum\")\n",
        "    )\n",
        "    g[\"poss\"] = g[\"FGA\"] + 0.44*g[\"FTA\"] - g[\"OREB\"] + g[\"TOV\"]\n",
        "    # opponent join to get DRtg inputs\n",
        "    opp = g.rename(columns={\n",
        "        \"TEAM_ABBREVIATION\":\"OPPONENT_ABBREVIATION\",\n",
        "        \"PTS_team\":\"PTS_opp\",\n",
        "        \"poss\":\"poss_opp\"\n",
        "    })[[\"GAME_DATE\",\"OPPONENT_ABBREVIATION\",\"PTS_opp\",\"poss_opp\"]]\n",
        "    g2 = g.merge(opp, on=[\"GAME_DATE\"], how=\"left\")\n",
        "    # approximate per-team DRtg from opponent scoring\n",
        "    g2[\"ORtg_g\"] = np.where(g2[\"poss\"]>0, 100*g2[\"PTS_team\"]/g2[\"poss\"], np.nan)\n",
        "    g2[\"DRtg_g\"] = np.where(g2[\"poss_opp\"]>0, 100*g2[\"PTS_opp\"]/g2[\"poss_opp\"], np.nan)\n",
        "    g2[\"Pace_g\"] = (g2[\"poss\"] + g2[\"poss_opp\"]) / 2.0\n",
        "    g2 = g2.sort_values([\"TEAM_ABBREVIATION\",\"GAME_DATE\"])\n",
        "    # rolling\n",
        "    out = g2.copy()\n",
        "    for w in windows:\n",
        "        for stat in [\"ORtg_g\",\"DRtg_g\",\"Pace_g\"]:\n",
        "            out[f\"{stat}_roll{w}\"] = out.groupby(\"TEAM_ABBREVIATION\")[stat].shift(1).rolling(w, min_periods=1).mean()\n",
        "    return out[[\"GAME_DATE\",\"TEAM_ABBREVIATION\",\"ORtg_g_roll5\",\"DRtg_g_roll5\",\"Pace_g_roll5\",\n",
        "                \"ORtg_g_roll10\",\"DRtg_g_roll10\",\"Pace_g_roll10\"]]\n",
        "\n",
        "def opponent_position_allowances(df: pd.DataFrame, window=10):\n",
        "    # How many points/assists/rebounds a team allows per opponent position (rolling)\n",
        "    if \"START_POSITION\" not in df.columns:\n",
        "        df[\"START_POSITION\"] = np.nan  # if not available, this will be sparse\n",
        "    base = df.groupby([\"GAME_DATE\",\"OPPONENT_ABBREVIATION\",\"START_POSITION\"], as_index=False)\\\n",
        "             .agg(PTS_allowed=(\"PTS\",\"sum\"), AST_allowed=(\"AST\",\"sum\"), REB_allowed=(\"REB\",\"sum\"))\n",
        "    base = base.sort_values([\"OPPONENT_ABBREVIATION\",\"START_POSITION\",\"GAME_DATE\"])\n",
        "    for w in [window]:\n",
        "        for stat in [\"PTS_allowed\",\"AST_allowed\",\"REB_allowed\"]:\n",
        "            base[f\"{stat}_roll{w}\"] = base.groupby([\"OPPONENT_ABBREVIATION\",\"START_POSITION\"])[stat]\\\n",
        "                                            .shift(1).rolling(w, min_periods=3).mean()\n",
        "    # pivot to wide per opponent (columns per position)\n",
        "    wide = base.pivot_table(index=[\"GAME_DATE\",\"OPPONENT_ABBREVIATION\"],\n",
        "                            columns=\"START_POSITION\",\n",
        "                            values=[f\"PTS_allowed_roll{window}\",f\"AST_allowed_roll{window}\",f\"REB_allowed_roll{window}\"])\n",
        "    wide.columns = [f\"{a}_{b}\" for a,b in wide.columns.to_flat_index()]\n",
        "    wide = wide.reset_index()\n",
        "    return wide\n",
        "\n",
        "def assemble_player_game_features(df_logs: pd.DataFrame, df_enriched_season: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = standardize_logs_cols(df_logs)\n",
        "    df = add_shooting_efficiency(df)\n",
        "    df = rolling_player_form(df)\n",
        "\n",
        "    # Team rolling ratings\n",
        "    tr = team_daily_ratings(df)\n",
        "    df = df.merge(tr, on=[\"GAME_DATE\",\"TEAM_ABBREVIATION\"], how=\"left\")\n",
        "\n",
        "    # Opponent allowances by position\n",
        "    oppw = opponent_position_allowances(df)\n",
        "    df = df.merge(oppw, left_on=[\"GAME_DATE\",\"OPPONENT_ABBREVIATION\"], right_on=[\"GAME_DATE\",\"OPPONENT_ABBREVIATION\"], how=\"left\")\n",
        "\n",
        "    # Merge season-enriched averages (PER/TS%/USG%/ORtg/DRtg etc.)\n",
        "    tmp = df_enriched_season.copy()\n",
        "    # normalize keys like before\n",
        "    def _norm(s):\n",
        "        import unicodedata\n",
        "        s = str(s).strip().lower()\n",
        "        s = unicodedata.normalize(\"NFKD\", s)\n",
        "        s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
        "        for ch in [\".\",\"'\",\"`\",\"â€™\",\"â€œ\",\"â€\",\",\"]:\n",
        "            s = s.replace(ch,\"\")\n",
        "        return \" \".join(s.split())\n",
        "    df[\"player_key\"] = df[\"PLAYER_NAME\"].map(_norm)\n",
        "    df[\"team_key\"] = df[\"TEAM_ABBREVIATION\"].str.upper()\n",
        "\n",
        "    tmp[\"player_key\"] = tmp[\"Player\"].map(_norm)\n",
        "    tmp[\"team_key\"] = tmp[\"Team\"].astype(str).str.upper()\n",
        "\n",
        "    keep_adv = [c for c in [\"PER\",\"TS%\",\"USG%\",\"ORtg\",\"DRtg\",\"WS/48\",\"BPM\",\"VORP\",\"Pos\",\"Age\"] if c in tmp.columns]\n",
        "    df = df.merge(tmp[[\"player_key\",\"team_key\"] + keep_adv], on=[\"player_key\",\"team_key\"], how=\"left\")\n",
        "\n",
        "    # simple situational flags\n",
        "    if \"MATCHUP\" in df.columns:\n",
        "        df[\"HOME\"] = df[\"MATCHUP\"].str.contains(\" vs. \", regex=False).astype(int)\n",
        "    else:\n",
        "        df[\"HOME\"] = np.nan  # placeholder\n",
        "\n",
        "    # Days rest\n",
        "    df[\"prev_date\"] = df.groupby(\"PLAYER_NAME\")[\"GAME_DATE\"].shift(1)\n",
        "    df[\"days_rest\"] = (df[\"GAME_DATE\"] - df[\"prev_date\"]).dt.days\n",
        "\n",
        "    # Targets: next-game points, rebounds, assists\n",
        "    df = df.sort_values([\"PLAYER_NAME\",\"GAME_DATE\"])\n",
        "    for target, src in [(\"PTS_next\",\"PTS\"), (\"REB_next\",\"REB\"), (\"AST_next\",\"AST\")]:\n",
        "        if src not in df.columns:\n",
        "            df[src] = np.nan\n",
        "        df[target] = df.groupby(\"PLAYER_NAME\")[src].shift(-1)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c6fc8992",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost Player PTS MAE (TimeSeries CV): 4.60 Â± 0.06\n",
            "XGBoost Player REB MAE (TimeSeries CV): 1.97 Â± 0.04\n",
            "XGBoost Player AST MAE (TimeSeries CV): 1.38 Â± 0.03\n"
          ]
        }
      ],
      "source": [
        "#--cell 8--#\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Load your logs and enriched season files ---\n",
        "logs_2324 = pd.read_csv(\"nba_boxscores_2023-24.csv\")\n",
        "logs_2425 = pd.read_csv(\"nba_boxscores_2024-25.csv\")\n",
        "enriched_2324 = pd.read_csv(\"nba_player_stats_2023_24_enriched.csv\")\n",
        "enriched_2425 = pd.read_csv(\"nba_player_stats_2024_25_enriched.csv\")\n",
        "\n",
        "# --- Build feature tables per season and concatenate ---\n",
        "feat_2324 = assemble_player_game_features(logs_2324, enriched_2324)\n",
        "feat_2425 = assemble_player_game_features(logs_2425, enriched_2425)\n",
        "features_all = pd.concat([feat_2324, feat_2425], ignore_index=True)\n",
        "\n",
        "# --- Base feature pool ---\n",
        "BASE_FEATURES = [\n",
        "    \"MIN_roll5\", \"MIN_roll10\", \"TS_game_roll5\", \"TS_game_roll10\", \"usage_share_roll5\",\n",
        "    \"ORtg_g_roll5\", \"DRtg_g_roll5\", \"Pace_g_roll5\",\n",
        "    \"PER\", \"TS%\", \"USG%\", \"ORtg\", \"DRtg\", \"WS/48\", \"BPM\", \"VORP\",\n",
        "    \"days_rest\", \"HOME\"\n",
        "]\n",
        "STAT_ROLLING = {\n",
        "    \"PTS\": [\"PTS_roll5\", \"PTS_roll10\"],\n",
        "    \"REB\": [\"REB_roll5\", \"REB_roll10\"],\n",
        "    \"AST\": [\"AST_roll5\", \"AST_roll10\"],\n",
        "}\n",
        "TARGETS = {\n",
        "    \"PTS\": \"PTS_next\",\n",
        "    \"REB\": \"REB_next\",\n",
        "    \"AST\": \"AST_next\",\n",
        "}\n",
        "\n",
        "models = {}\n",
        "feature_cols_by_stat = {}\n",
        "cv_scores = {}\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "for stat, target_col in TARGETS.items():\n",
        "    cand_feats = BASE_FEATURES + STAT_ROLLING[stat]\n",
        "    feat_cols = [c for c in cand_feats if c in features_all.columns]\n",
        "    feature_cols_by_stat[stat] = feat_cols\n",
        "\n",
        "    data = features_all.dropna(subset=feat_cols + [target_col]).copy()\n",
        "    if data.empty:\n",
        "        print(f\"âš ï¸ No training data for {stat}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    data_sorted = data.sort_values(\"GAME_DATE\")\n",
        "    X = data_sorted[feat_cols]\n",
        "    y = data_sorted[target_col]\n",
        "\n",
        "    maes = []\n",
        "    for train_idx, test_idx in tscv.split(X):\n",
        "        Xtr, Xte = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        ytr, yte = y.iloc[train_idx], y.iloc[test_idx]\n",
        "        model = XGBRegressor(\n",
        "            n_estimators=300,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=4,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            verbosity=0\n",
        "        )\n",
        "        model.fit(Xtr, ytr)\n",
        "        pred = model.predict(Xte)\n",
        "        maes.append(mean_absolute_error(yte, pred))\n",
        "\n",
        "    cv_scores[stat] = (float(np.mean(maes)), float(np.std(maes)))\n",
        "    print(f\"XGBoost Player {stat} MAE (TimeSeries CV): {np.mean(maes):.2f} Â± {np.std(maes):.2f}\")\n",
        "\n",
        "    final_model = XGBRegressor(\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbosity=0\n",
        "    )\n",
        "    final_model.fit(X, y)\n",
        "    models[stat] = final_model\n",
        "\n",
        "if \"PTS\" in models:\n",
        "    model = models[\"PTS\"]\n",
        "    feature_cols = feature_cols_by_stat[\"PTS\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3de4ad9e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 15 importances â€” PTS:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "usage_share_roll5    0.341711\n",
              "PTS_roll5            0.240363\n",
              "USG%                 0.133724\n",
              "MIN_roll5            0.074681\n",
              "VORP                 0.036369\n",
              "PER                  0.034423\n",
              "ORtg_g_roll5         0.028132\n",
              "TS%                  0.020535\n",
              "PTS_roll10           0.015073\n",
              "MIN_roll10           0.012854\n",
              "BPM                  0.012456\n",
              "Pace_g_roll5         0.012371\n",
              "WS/48                0.009794\n",
              "days_rest            0.007447\n",
              "HOME                 0.006383\n",
              "dtype: float32"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 15 importances â€” REB:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "REB_roll5            0.408288\n",
              "REB_roll10           0.261173\n",
              "PER                  0.075007\n",
              "VORP                 0.032947\n",
              "WS/48                0.032153\n",
              "MIN_roll5            0.026863\n",
              "USG%                 0.019745\n",
              "BPM                  0.018665\n",
              "usage_share_roll5    0.016841\n",
              "TS_game_roll10       0.016282\n",
              "TS%                  0.016151\n",
              "MIN_roll10           0.015538\n",
              "Pace_g_roll5         0.014731\n",
              "ORtg_g_roll5         0.013715\n",
              "TS_game_roll5        0.013573\n",
              "dtype: float32"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 15 importances â€” AST:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AST_roll5            0.447778\n",
              "AST_roll10           0.209741\n",
              "VORP                 0.066292\n",
              "USG%                 0.049795\n",
              "ORtg_g_roll5         0.032706\n",
              "PER                  0.025270\n",
              "MIN_roll5            0.024635\n",
              "BPM                  0.022984\n",
              "TS%                  0.021929\n",
              "WS/48                0.016182\n",
              "MIN_roll10           0.013911\n",
              "TS_game_roll10       0.013528\n",
              "usage_share_roll5    0.013322\n",
              "HOME                 0.011061\n",
              "TS_game_roll5        0.010715\n",
              "dtype: float32"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#--cell 9--#\n",
        "import pandas as pd\n",
        "\n",
        "if not models:\n",
        "    raise RuntimeError(\"No models trained in Cell 7. Ensure features/targets exist and rerun Cell 7.\")\n",
        "\n",
        "all_imps = {}\n",
        "for stat, mdl in models.items():\n",
        "    feat_cols = feature_cols_by_stat.get(stat, [])\n",
        "    if hasattr(mdl, \"feature_importances_\"):\n",
        "        imp_series = pd.Series(mdl.feature_importances_, index=feat_cols).sort_values(ascending=False)\n",
        "        all_imps[stat] = imp_series\n",
        "        print(f\"\\nTop 15 importances â€” {stat}:\")\n",
        "        display(imp_series.head(15))\n",
        "    else:\n",
        "        print(f\"\\nModel for {stat} has no feature_importances_ attribute.\")\n",
        "\n",
        "# Keep the most recently shown importances in 'imp' for backward compatibility\n",
        "if \"PTS\" in all_imps:\n",
        "    imp = all_imps[\"PTS\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a40247d",
      "metadata": {},
      "source": [
        "## team-level predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "73bf580f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Team PTS baseline R^2: 0.15181475578664994\n"
          ]
        }
      ],
      "source": [
        "#--cell 10--#\n",
        "# Team game table\n",
        "team_games = features_all.groupby([\"GAME_DATE\",\"TEAM_ABBREVIATION\"], as_index=False)\\\n",
        "    .agg(\n",
        "        team_pts=(\"PTS\",\"sum\"),\n",
        "        team_pts_next=(\"PTS_next\",\"sum\"),\n",
        "        or5=(\"ORtg_g_roll5\",\"mean\"),\n",
        "        dr5=(\"DRtg_g_roll5\",\"mean\"),\n",
        "        pace5=(\"Pace_g_roll5\",\"mean\"),\n",
        "    )\n",
        "\n",
        "# Join opponent features (same date)\n",
        "opp = team_games.rename(columns={\n",
        "    \"TEAM_ABBREVIATION\":\"OPPONENT_ABBREVIATION\",\n",
        "    \"team_pts\":\"opp_pts\",\n",
        "    \"team_pts_next\":\"opp_pts_next\",\n",
        "    \"or5\":\"opp_or5\",\"dr5\":\"opp_dr5\",\"pace5\":\"opp_pace5\"\n",
        "})\n",
        "team_matchups = team_games.merge(opp, on=[\"GAME_DATE\"], how=\"inner\")\n",
        "\n",
        "# Simple features for team total prediction\n",
        "team_feature_cols = [\"or5\",\"dr5\",\"pace5\",\"opp_or5\",\"opp_dr5\",\"opp_pace5\"]\n",
        "tm = team_matchups.dropna(subset=team_feature_cols + [\"team_pts_next\"]).copy()\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "X_tm = tm[team_feature_cols]\n",
        "y_tm = tm[\"team_pts_next\"]\n",
        "ridge = Ridge(alpha=5.0).fit(X_tm, y_tm)\n",
        "print(\"Team PTS baseline R^2:\", ridge.score(X_tm, y_tm))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d849f19",
      "metadata": {},
      "source": [
        "## Lineups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a505d3a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install selenium webdriver-manager bs4 pandas lxml\n",
        "\n",
        "import os, re, time, pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "\n",
        "def _clean_list(xs):\n",
        "    return [re.sub(r\"\\s+\\(.*?\\)\\s*$\", \"\", x) for x in xs]\n",
        "\n",
        "def _try_click_consent(driver, timeout=6):\n",
        "    XPATHS = [\n",
        "        \"//button[contains(.,'Accept')]\",\n",
        "        \"//button[contains(.,'I Agree')]\",\n",
        "        \"//button[contains(.,'Agree')]\",\n",
        "        \"//button[contains(.,'Î‘Ï€Î¿Î´Î¿Ï‡Î®')]\",\n",
        "        \"//button[contains(.,'Î£Ï…Î¼Ï†Ï‰Î½ÏŽ')]\",\n",
        "    ]\n",
        "    end = time.time() + timeout\n",
        "    for xp in XPATHS:\n",
        "        try:\n",
        "            btn = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.XPATH, xp)))\n",
        "            btn.click()\n",
        "            return True\n",
        "        except Exception:\n",
        "            if time.time() > end: break\n",
        "    return False\n",
        "\n",
        "def _progress_scroll(driver, steps=10, pause=0.8):\n",
        "    h = driver.execute_script(\"return document.body.scrollHeight || document.documentElement.scrollHeight;\")\n",
        "    for i in range(1, steps + 1):\n",
        "        y = int(h * i / steps)\n",
        "        driver.execute_script(f\"window.scrollTo(0, {y});\")\n",
        "        time.sleep(pause)\n",
        "\n",
        "def _extract_team(side):\n",
        "    team_el = side.select_one(\".lineup__abbr, .lineup__team-name, .lineup__name\")\n",
        "    if team_el:\n",
        "        return team_el.get_text(strip=True)\n",
        "    logo = side.select_one(\"img[alt]\")\n",
        "    return (logo.get(\"alt\") or \"\").strip() if logo else \"\"\n",
        "\n",
        "def _extract_status(side):\n",
        "    status_el = side.select_one(\".lineup__status\")\n",
        "    txt = (status_el.get_text(\" \", strip=True) if status_el else \"\").upper()\n",
        "    if \"CONFIRM\" in txt:  return \"CONFIRMED\"\n",
        "    if \"EXPECT\" in txt or \"PROBABLE\" in txt: return \"EXPECTED\"\n",
        "    return \"UNKNOWN\"\n",
        "\n",
        "def _extract_starters(side):\n",
        "    # Try several variants for starters content\n",
        "    containers = side.select(\".lineup__list--starters, .lineup__list, .lineup__players\")\n",
        "    if not containers:\n",
        "        containers = [side]\n",
        "\n",
        "    names = []\n",
        "    for blk in containers:\n",
        "        for a in blk.select(\"a.lineup__player-link, .lineup__player a\"):\n",
        "            t = a.get_text(\" \", strip=True)\n",
        "            if t: names.append(t)\n",
        "        if not names:\n",
        "            for row in blk.select(\".lineup__player\"):\n",
        "                t = row.get_text(\" \", strip=True)\n",
        "                if re.match(r\"^(PG|SG|SF|PF|C)\\b\", t): names.append(t)\n",
        "        if not names:\n",
        "            for li in blk.select(\"li\"):\n",
        "                t = li.get_text(\" \", strip=True)\n",
        "                if re.match(r\"^(PG|SG|SF|PF|C)\\b\", t): names.append(t)\n",
        "\n",
        "    if not names:\n",
        "        txt = side.get_text(\"\\n\", strip=True)\n",
        "        names = re.findall(r\"(?:^|\\n)(?:PG|SG|SF|PF|C)\\s+[^\\n]+\", txt)\n",
        "\n",
        "    return _clean_list(names)[:5]\n",
        "\n",
        "# ---------------- main ----------------\n",
        "\n",
        "def fetch_rotowire_lineups_selenium(date: str | None = None,\n",
        "                                    wait_sec: float = 14.0,\n",
        "                                    headless: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Render Rotowire lineups & parse BOTH sides per game (global side selectors).\n",
        "    Returns:\n",
        "      game_time, team, side (AWAY/HOME), lineup_status, starters,\n",
        "      starter_1..starter_5, lineup_confirmed (0/1)\n",
        "    \"\"\"\n",
        "    base = \"https://www.rotowire.com/basketball/nba-lineups.php\"\n",
        "    url = base if not date else f\"{base}?date={date}\"\n",
        "\n",
        "    opts = Options()\n",
        "    if headless: opts.add_argument(\"--headless=new\")\n",
        "    opts.add_argument(\"--disable-gpu\")\n",
        "    opts.add_argument(\"--no-sandbox\")\n",
        "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
        "    opts.add_argument(\"--window-size=1400,1000\")\n",
        "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    opts.add_experimental_option(\"useAutomationExtension\", False)\n",
        "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "    opts.add_argument(\"--lang=en-US,en;q=0.9\")\n",
        "    opts.add_argument(\n",
        "        \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "\n",
        "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
        "    driver.get(url)\n",
        "\n",
        "    _try_click_consent(driver, timeout=6)\n",
        "    time.sleep(1.2)\n",
        "    try:\n",
        "        WebDriverWait(driver, int(wait_sec)).until(\n",
        "            EC.presence_of_element_located((By.CSS_SELECTOR, \".lineup, .lineup.is-nba\"))\n",
        "        )\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    _progress_scroll(driver, steps=10, pause=0.8)\n",
        "    time.sleep(1.0)\n",
        "\n",
        "    # quick diagnostics\n",
        "    blocks = driver.find_elements(By.CSS_SELECTOR, \".lineup.is-nba, .lineup\")\n",
        "    players = driver.find_elements(By.CSS_SELECTOR, \".lineup__player, a.lineup__player-link\")\n",
        "    print(f\"diagnostics: lineup blocks={len(blocks)}, player nodes={len(players)}\")\n",
        "\n",
        "    html = driver.page_source\n",
        "    os.makedirs(\"_rotowire_debug\", exist_ok=True)\n",
        "    with open(\"_rotowire_debug/last_lineups.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html)\n",
        "    try:\n",
        "        driver.save_screenshot(\"_rotowire_debug/last_lineups.png\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    driver.quit()\n",
        "\n",
        "    # -------- parse globally by side classes ----------\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "    # game time map: find each game container time\n",
        "    game_time_map = {}\n",
        "    for gi, g in enumerate(soup.select(\".lineup__main, .lineup.is-nba, .lineup\")):\n",
        "        t = g.select_one(\".lineup__time, .game-time\")\n",
        "        game_time_map[id(g)] = t.get_text(strip=True) if t else \"\"\n",
        "\n",
        "    # Select **visit/away** & **home** side boxes explicitly\n",
        "    visit_sel = (\n",
        "        '[class*=\"lineup__box\"][class*=\"is-visit\"], '\n",
        "        '[class*=\"lineup__team\"][class*=\"is-visit\"], '\n",
        "        '[class*=\"lineup__side\"][class*=\"is-visit\"], '\n",
        "        '[class*=\"visit\"]'\n",
        "    )\n",
        "    home_sel = (\n",
        "        '[class*=\"lineup__box\"][class*=\"is-home\"], '\n",
        "        '[class*=\"lineup__team\"][class*=\"is-home\"], '\n",
        "        '[class*=\"lineup__side\"][class*=\"is-home\"], '\n",
        "        '[class*=\"home\"]'\n",
        "    )\n",
        "\n",
        "    visit_boxes = soup.select(visit_sel)\n",
        "    home_boxes  = soup.select(home_sel)\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    def add_rows(boxes, side_label):\n",
        "        for box in boxes:\n",
        "            # nearest parent game container for time\n",
        "            parent = box.find_parent(lambda tag: tag.has_attr(\"class\") and any(\n",
        "                c in {\"lineup__main\",\"lineup\",\"lineup is-nba\"} for c in tag.get(\"class\", [])\n",
        "            ))\n",
        "            game_time = game_time_map.get(id(parent), \"\") if parent else \"\"\n",
        "            team = _extract_team(box)\n",
        "            starters = _extract_starters(box)\n",
        "            status = _extract_status(box)\n",
        "            if starters or team:\n",
        "                rows.append({\n",
        "                    \"game_time\": game_time,\n",
        "                    \"team\": team,\n",
        "                    \"side\": side_label,\n",
        "                    \"lineup_status\": status,\n",
        "                    \"starters\": starters,\n",
        "                    \"starter_1\": starters[0] if len(starters)>0 else None,\n",
        "                    \"starter_2\": starters[1] if len(starters)>1 else None,\n",
        "                    \"starter_3\": starters[2] if len(starters)>2 else None,\n",
        "                    \"starter_4\": starters[3] if len(starters)>3 else None,\n",
        "                    \"starter_5\": starters[4] if len(starters)>4 else None,\n",
        "                    \"lineup_confirmed\": int(status == \"CONFIRMED\"),\n",
        "                })\n",
        "\n",
        "    add_rows(visit_boxes, \"AWAY\")\n",
        "    add_rows(home_boxes,  \"HOME\")\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    if not df.empty:\n",
        "        df = df.drop_duplicates(\n",
        "            subset=[\"game_time\",\"team\",\"side\",\"starter_1\",\"starter_2\",\"starter_3\",\"starter_4\",\"starter_5\"]\n",
        "        )\n",
        "        all_na = df[[\"starter_1\",\"starter_2\",\"starter_3\",\"starter_4\",\"starter_5\"]].isna().all(axis=1)\n",
        "        df = df[~all_na].reset_index(drop=True)\n",
        "    else:\n",
        "        print(\"âš ï¸ Parsed zero rows. Check _rotowire_debug/last_lineups.html & .png\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cc3237d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diagnostics: lineup blocks=12, player nodes=147\n",
            "âœ… Shape: (18, 11)\n",
            "game_time team side lineup_status                                                                starters     starter_1     starter_2     starter_3        starter_4     starter_5  lineup_confirmed\n",
            "               AWAY      EXPECTED    [Ryan Rollins, AJ Green, Gary Trent, G. Antetokounmpo, Myles Turner]  Ryan Rollins      AJ Green    Gary Trent G. Antetokounmpo  Myles Turner                 0\n",
            "               AWAY      EXPECTED  [D. DiVincenzo, Mike Conley, J. McDaniels, Julius Randle, Rudy Gobert] D. DiVincenzo   Mike Conley  J. McDaniels    Julius Randle   Rudy Gobert                 0\n",
            "               AWAY      EXPECTED     [K. George, S. Mykhailiuk, L. Markkanen, K. Filipowski, W. Kessler]     K. George S. Mykhailiuk  L. Markkanen    K. Filipowski    W. Kessler                 0\n",
            "               AWAY      EXPECTED         [CJ McCollum, K. George, K. Middleton, B. Coulibaly, Alex Sarr]   CJ McCollum     K. George  K. Middleton     B. Coulibaly     Alex Sarr                 0\n",
            "               AWAY      EXPECTED  [Cooper Flagg, Max Christie, Klay Thompson, P. Washington, D. Gafford]  Cooper Flagg  Max Christie Klay Thompson    P. Washington    D. Gafford                 0\n",
            "               AWAY      EXPECTED       [C. Cunningham, D. Robinson, A. Thompson, T. Harris, Jalen Duren] C. Cunningham   D. Robinson   A. Thompson        T. Harris   Jalen Duren                 0\n",
            "               AWAY      EXPECTED       [D. Schroder, R. Westbrook, Z. LaVine, DeMar DeRozan, D. Sabonis]   D. Schroder  R. Westbrook     Z. LaVine    DeMar DeRozan    D. Sabonis                 0\n",
            "               AWAY      EXPECTED     [Luka Doncic, Austin Reaves, Marcus Smart, Rui Hachimura, D. Ayton]   Luka Doncic Austin Reaves  Marcus Smart    Rui Hachimura      D. Ayton                 0\n",
            "               AWAY      EXPECTED          [D. Mitchell, N. Powell, A. Wiggins, Bam Adebayo, Kel'el Ware]   D. Mitchell     N. Powell    A. Wiggins      Bam Adebayo   Kel'el Ware                 0\n",
            "               HOME      EXPECTED   [Q. Jackson, Aaron Nesmith, Jarace Walker, Pascal Siakam, I. Jackson]    Q. Jackson Aaron Nesmith Jarace Walker    Pascal Siakam    I. Jackson                 0\n",
            "               HOME      EXPECTED           [Ben Saraf, Cam Thomas, Terance Mann, M. Porter, Nic Claxton]     Ben Saraf    Cam Thomas  Terance Mann        M. Porter   Nic Claxton                 0\n",
            "               HOME      EXPECTED [Derrick White, P. Pritchard, Jaylen Brown, Josh Minott, Neemias Queta] Derrick White  P. Pritchard  Jaylen Brown      Josh Minott Neemias Queta                 0\n"
          ]
        }
      ],
      "source": [
        "# ---------- run it ----------\n",
        "df_lineups = fetch_rotowire_lineups_selenium(wait_sec=14.0, headless=False)\n",
        "print(\"âœ… Shape:\", df_lineups.shape)\n",
        "print(df_lineups.sort_values([\"game_time\",\"side\"]).head(12).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "202f0cd8",
      "metadata": {},
      "source": [
        "## Selenium rotowire search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b0a3dc4a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DOM diagnostics: {'lineup__teams': 9, 'ul.lineup__list': 18, 'ul.is-visit': 9, 'ul.is-home': 9, 'see-proj-minutes buttons': 18, 'header abbr': 0, 'header team': 0, 'player anchors': 147, 'MNP titles': 18}\n",
            "Fallback B: scanning all ul.lineup__list globally...\n",
            "â†’ Parsed rows: 18\n",
            "\n",
            "âœ… Preview:\n",
            "game_time team side lineup_status  may_not_play_count        starter_1     starter_2     starter_3     starter_4     starter_5\n",
            "           DAL AWAY      EXPECTED                   9     Cooper Flagg  Max Christie Klay Thompson P. Washington    D. Gafford\n",
            "           DET AWAY      EXPECTED                   8    C. Cunningham   D. Robinson   A. Thompson   Jalen Duren C. Cunningham\n",
            "           LAL AWAY      EXPECTED                  10      Luka Doncic Austin Reaves  Marcus Smart Rui Hachimura   Luka Doncic\n",
            "           MIA AWAY      EXPECTED                   9      D. Mitchell    A. Wiggins   Bam Adebayo   Kel'el Ware   D. Mitchell\n",
            "           MIL AWAY      EXPECTED                   7     Ryan Rollins      AJ Green    Gary Trent  Myles Turner  Ryan Rollins\n",
            "           MIN AWAY      EXPECTED                   6    D. DiVincenzo   Mike Conley  J. McDaniels Julius Randle   Rudy Gobert\n",
            "           SAC AWAY      EXPECTED                  10     R. Westbrook DeMar DeRozan    D. Sabonis   D. Schroder  R. Westbrook\n",
            "           UTA AWAY      EXPECTED                   8        K. George S. Mykhailiuk  L. Markkanen K. Filipowski     K. George\n",
            "           WAS AWAY      EXPECTED                   6      CJ McCollum     K. George  B. Coulibaly     Alex Sarr   CJ McCollum\n",
            "           BKN HOME      EXPECTED                   8        Ben Saraf    Cam Thomas  Terance Mann     M. Porter   Nic Claxton\n",
            "           BOS HOME      EXPECTED                   6    Derrick White  P. Pritchard  Jaylen Brown   Josh Minott Neemias Queta\n",
            "           DEN HOME      EXPECTED                   7         C. Braun  Aaron Gordon  Nikola Jokic     J. Murray      C. Braun\n",
            "           HOU HOME      EXPECTED                   8    Amen Thompson   Josh Okogie  Kevin Durant     A. Sengun Amen Thompson\n",
            "           IND HOME      EXPECTED                  13       Q. Jackson Aaron Nesmith Jarace Walker Pascal Siakam    I. Jackson\n",
            "           LAC HOME      EXPECTED                   2     James Harden  Bradley Beal Kawhi Leonard Derrick Jones   Ivica Zubac\n",
            "           MEM HOME      EXPECTED                  10 K. Caldwell-Pope  Jaylen Wells Jaren Jackson  Jock Landale     Ja Morant\n",
            "           NYK HOME      EXPECTED                   5    Jalen Brunson Mikal Bridges    OG Anunoby      K. Towns Jalen Brunson\n",
            "           POR HOME      EXPECTED                  10     Jrue Holiday     T. Camara   Deni Avdija    D. Clingan  Jrue Holiday\n"
          ]
        }
      ],
      "source": [
        "# pip install bs4 lxml pandas\n",
        "import re, os, pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def _txt(x):\n",
        "    return re.sub(r\"\\s+\", \" \", x.get_text(\" \", strip=True)) if x else \"\"\n",
        "\n",
        "def _clean_player(n):\n",
        "    if not n: return n\n",
        "    n = re.sub(r\"\\s+\\(.*?\\)\\s*$\", \"\", n).strip()\n",
        "    n = re.sub(r\"^(PG|SG|SF|PF|C)\\s+\", \"\", n, flags=re.I)\n",
        "    return n\n",
        "\n",
        "def _get_mnp_from_ul(ul):\n",
        "    \"\"\"Extract 'May Not Play' entries from a team UL.\"\"\"\n",
        "    mnp = []\n",
        "    # Strategy 1: find the title li inside this UL, then collect following player lis until next title\n",
        "    title = ul.find(\"li\", class_=lambda c: c and \"lineup__title\" in c and re.search(r\"may\\s+not\\s+play\", _txt(ul.find(\"li\", class_=c)) if ul.find(\"li\", class_=c) else \"\", re.I))\n",
        "    if title:\n",
        "        for li in title.find_all_next(\"li\"):\n",
        "            # stop if next section title\n",
        "            if \"lineup__title\" in (li.get(\"class\") or []):\n",
        "                break\n",
        "            if \"lineup__player\" in (li.get(\"class\") or []):\n",
        "                a = li.select_one(\"a\")\n",
        "                tag = li.select_one(\".lineup__inj\")\n",
        "                nm = _txt(a) if a else \"\"\n",
        "                if nm:\n",
        "                    mnp.append(f\"{nm} ({_txt(tag)})\" if tag else nm)\n",
        "        # normalize\n",
        "        return [_clean_player(x) for x in mnp if x and x.lower() != \"none\"]\n",
        "\n",
        "    # Strategy 2: common MNP containers inside UL\n",
        "    for li in ul.select(\".lineup__notplay li, .lineup__status--out, .lineup__inj-list li\"):\n",
        "        nm = _txt(li)\n",
        "        if nm: mnp.append(_clean_player(nm))\n",
        "    return [x for x in mnp if x and x.lower() != \"none\"]\n",
        "\n",
        "def _extract_starters_from_ul(ul):\n",
        "    \"\"\"Try multiple ways to get five starters out of a team UL.\"\"\"\n",
        "    names = []\n",
        "    # Most reliable: 100% rows\n",
        "    for li in ul.select(\"li.lineup__player.is-pct-play-100 a\"):\n",
        "        nm = _txt(li)\n",
        "        if nm: names.append(nm)\n",
        "    # Fallback: any lineup__player anchors in first list group\n",
        "    if len(names) < 5:\n",
        "        for li in ul.select(\"li.lineup__player a\"):\n",
        "            nm = _txt(li)\n",
        "            if nm: names.append(nm)\n",
        "            if len(names) >= 5: break\n",
        "    # Final cleanup + trim\n",
        "    names = [_clean_player(n) for n in names]\n",
        "    return names[:5]\n",
        "\n",
        "def _lineup_status(ul):\n",
        "    st = _txt(ul.select_one(\".lineup__status\"))\n",
        "    stU = st.upper()\n",
        "    if \"CONFIRM\" in stU: return \"CONFIRMED\"\n",
        "    if \"EXPECT\" in stU or \"PROBABLE\" in stU: return \"EXPECTED\"\n",
        "    return \"UNKNOWN\"\n",
        "\n",
        "def parse_rotowire_lineups_flexible(html_path: str) -> pd.DataFrame:\n",
        "    with open(html_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        html = f.read()\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "    # --- Diagnostics to understand the DOM you have ---\n",
        "    diag = {\n",
        "        \"lineup__teams\": len(soup.select(\"div.lineup__teams\")),\n",
        "        \"ul.lineup__list\": len(soup.select(\"ul.lineup__list\")),\n",
        "        \"ul.is-visit\": len(soup.select(\"ul.lineup__list.is-visit\")),\n",
        "        \"ul.is-home\": len(soup.select(\"ul.lineup__list.is-home\")),\n",
        "        \"see-proj-minutes buttons\": len(soup.select(\"button.see-proj-minutes\")),\n",
        "        \"header abbr\": len(soup.select(\".lineup__hdr .lineup__abbr\")),\n",
        "        \"header team\": len(soup.select(\".lineup__hdr .lineup__team\")),\n",
        "        \"player anchors\": len(soup.select(\"a.lineup__player-link, .lineup__player a\")),\n",
        "        \"MNP titles\": len(soup.find_all(string=re.compile(r\"^\\s*may\\s+not\\s+play\\s*$\", re.I))),\n",
        "    }\n",
        "    print(\"DOM diagnostics:\", diag)\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    # ========== STRATEGY A: by matchup blocks ==========\n",
        "    for teams_div in soup.select(\"div.lineup__teams\"):\n",
        "        # game time near this block (looks upward for a sibling header)\n",
        "        time_el = teams_div.find_previous(\"div\", class_=\"lineup__time\")\n",
        "        game_time = _txt(time_el)\n",
        "\n",
        "        # find both team ULs inside this matchup\n",
        "        uls = teams_div.select(\"ul.lineup__list\")\n",
        "        if len(uls) < 1:\n",
        "            continue\n",
        "\n",
        "        # Try to pair AWAY then HOME by class flags; else preserve order\n",
        "        away_ul = None\n",
        "        home_ul = None\n",
        "        for ul in uls:\n",
        "            classes = \" \".join(ul.get(\"class\", [])).lower()\n",
        "            if \"is-visit\" in classes or \"visit\" in classes or \"away\" in classes:\n",
        "                away_ul = ul\n",
        "            if \"is-home\" in classes or \"home\" in classes:\n",
        "                home_ul = home_ul or ul  # keep the first\n",
        "\n",
        "        if away_ul is None and home_ul is None and len(uls) >= 2:\n",
        "            away_ul, home_ul = uls[0], uls[1]\n",
        "        elif away_ul is None and len(uls) >= 1:\n",
        "            away_ul = uls[0]\n",
        "        elif home_ul is None and len(uls) >= 2:\n",
        "            # pick the other UL as home\n",
        "            home_ul = next((u for u in uls if u is not away_ul), None)\n",
        "\n",
        "        pairs = [(\"AWAY\", away_ul), (\"HOME\", home_ul)]\n",
        "        # Extract team code (prefer button data-team; else header abbrs in the same matchup)\n",
        "        header_abbrs = [ _txt(el) for el in teams_div.select(\".lineup__abbr\") if _txt(el) ]\n",
        "        # If header not inside teams_div, try its parent block\n",
        "        if not header_abbrs:\n",
        "            parent_main = teams_div.find_parent([\"div\",\"section\"])\n",
        "            if parent_main:\n",
        "                header_abbrs = [ _txt(el) for el in parent_main.select(\".lineup__abbr\") if _txt(el) ]\n",
        "\n",
        "        for idx, (side, ul) in enumerate(pairs):\n",
        "            if not ul: continue\n",
        "            btn = ul.select_one(\"button.see-proj-minutes\")\n",
        "            team = btn[\"data-team\"].strip().upper() if btn and btn.has_attr(\"data-team\") else None\n",
        "            if not team and header_abbrs and idx < len(header_abbrs):\n",
        "                team = header_abbrs[idx].upper()\n",
        "\n",
        "            starters = _extract_starters_from_ul(ul)\n",
        "            mnp = _get_mnp_from_ul(ul)\n",
        "            status = _lineup_status(ul)\n",
        "\n",
        "            # Only add if we have at least a team or any player info\n",
        "            if team or starters or mnp:\n",
        "                rows.append({\n",
        "                    \"game_time\": game_time,\n",
        "                    \"team\": team,\n",
        "                    \"side\": side,\n",
        "                    \"lineup_status\": status,\n",
        "                    \"starters\": starters,\n",
        "                    \"may_not_play\": mnp,\n",
        "                    \"may_not_play_count\": len(mnp),\n",
        "                    \"lineup_confirmed\": int(status == \"CONFIRMED\"),\n",
        "                })\n",
        "\n",
        "    # ========== STRATEGY B: fall back to any lineup ULs globally ==========\n",
        "    if not rows:\n",
        "        print(\"Fallback B: scanning all ul.lineup__list globally...\")\n",
        "        for ul in soup.select(\"ul.lineup__list\"):\n",
        "            # Guess side by class or position among siblings\n",
        "            side = \"AWAY\" if \"is-visit\" in (ul.get(\"class\") or []) else (\"HOME\" if \"is-home\" in (ul.get(\"class\") or []) else None)\n",
        "            # Team from button\n",
        "            btn = ul.select_one(\"button.see-proj-minutes\")\n",
        "            team = btn[\"data-team\"].strip().upper() if btn and btn.has_attr(\"data-team\") else None\n",
        "            starters = _extract_starters_from_ul(ul)\n",
        "            mnp = _get_mnp_from_ul(ul)\n",
        "            status = _lineup_status(ul)\n",
        "\n",
        "            if side and (team or starters or mnp):\n",
        "                rows.append({\n",
        "                    \"game_time\": \"\",  # unknown at this scope\n",
        "                    \"team\": team,\n",
        "                    \"side\": side,\n",
        "                    \"lineup_status\": status,\n",
        "                    \"starters\": starters,\n",
        "                    \"may_not_play\": mnp,\n",
        "                    \"may_not_play_count\": len(mnp),\n",
        "                    \"lineup_confirmed\": int(status == \"CONFIRMED\"),\n",
        "                })\n",
        "\n",
        "    # ========== STRATEGY C: header-driven pairing (very defensive) ==========\n",
        "    if not rows:\n",
        "        print(\"Fallback C: pairing by header labels and nearest lists...\")\n",
        "        for block in soup.select(\".lineup, .lineup__main\"):\n",
        "            hdr = block.select(\".lineup__hdr .lineup__abbr, .lineup__hdr .lineup__team\")\n",
        "            labels = [ _txt(x) for x in hdr if _txt(x) ]\n",
        "            if len(labels) < 2:\n",
        "                continue\n",
        "            away_label, home_label = labels[:2]\n",
        "            lists = block.select(\"ul.lineup__list\")\n",
        "            if len(lists) < 2:\n",
        "                continue\n",
        "            for side, lab, ul in [(\"AWAY\", away_label, lists[0]), (\"HOME\", home_label, lists[1])]:\n",
        "                starters = _extract_starters_from_ul(ul)\n",
        "                mnp = _get_mnp_from_ul(ul)\n",
        "                status = _lineup_status(ul)\n",
        "                rows.append({\n",
        "                    \"game_time\": _txt(block.select_one(\".lineup__time, .game-time\")),\n",
        "                    \"team\": lab.upper(),\n",
        "                    \"side\": side,\n",
        "                    \"lineup_status\": status,\n",
        "                    \"starters\": starters,\n",
        "                    \"may_not_play\": mnp,\n",
        "                    \"may_not_play_count\": len(mnp),\n",
        "                    \"lineup_confirmed\": int(status == \"CONFIRMED\"),\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    # Expand starters to columns for easier merging\n",
        "    for i in range(5):\n",
        "        col = f\"starter_{i+1}\"\n",
        "        df[col] = df[\"starters\"].apply(lambda xs: xs[i] if isinstance(xs, list) and len(xs) > i else None)\n",
        "\n",
        "    print(f\"â†’ Parsed rows: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "# ---- RUN IT (point to your saved file) ----\n",
        "HTML_PATH = \"_rotowire_debug/last_lineups.html\"  # change if needed\n",
        "if not os.path.exists(HTML_PATH):\n",
        "    # if you uploaded as 'last_lineups.html' in current directory\n",
        "    if os.path.exists(\"last_lineups.html\"):\n",
        "        HTML_PATH = \"last_lineups.html\"\n",
        "\n",
        "df_lineups = parse_rotowire_lineups_flexible(HTML_PATH)\n",
        "\n",
        "# Safe display\n",
        "if df_lineups.empty:\n",
        "    print(\"\\nâš ï¸ Still empty. Please share the values printed in 'DOM diagnostics' (above).\")\n",
        "else:\n",
        "    cols = [\"game_time\",\"team\",\"side\",\"lineup_status\",\"may_not_play_count\",\n",
        "            \"starter_1\",\"starter_2\",\"starter_3\",\"starter_4\",\"starter_5\"]\n",
        "    print(\"\\nâœ… Preview:\")\n",
        "    print(df_lineups[cols].sort_values([\"game_time\",\"side\",\"team\"], na_position=\"last\").to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4e2b9e20",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 9 games in HTML.\n",
            "âœ… Parsed 57 'May Not Play' players across 17 teams.\n",
            "  game_time team side position           player status            title_text  likelihood_pct\n",
            "10:00 PM ET  LAL AWAY        F        A. Thiero    Out Very Unlikely To Play               0\n",
            "10:00 PM ET  LAL AWAY        C         D. Ayton   Ques       Toss Up To Play              50\n",
            "10:00 PM ET  LAL AWAY        G       G. Vincent    Out Very Unlikely To Play               0\n",
            "10:00 PM ET  LAL AWAY        F         L. James    Out Very Unlikely To Play               0\n",
            "10:00 PM ET  LAL AWAY        C        M. Kleber    Out Very Unlikely To Play               0\n",
            "10:00 PM ET  POR HOME        G        B. Wesley    Out Very Unlikely To Play               0\n",
            "10:00 PM ET  POR HOME        G       D. Lillard    OFS   Very Likely To Play               0\n",
            "10:00 PM ET  POR HOME        F      M. Thybulle    Out Very Unlikely To Play               0\n",
            "10:00 PM ET  POR HOME        G     S. Henderson    Out Very Unlikely To Play               0\n",
            "10:00 PM ET  POR HOME        G        S. Sharpe   Prob        Likely To Play              75\n",
            "10:30 PM ET  MIA AWAY        G    K. Jakucionis    Out Very Unlikely To Play               0\n",
            "10:30 PM ET  MIA AWAY        G        N. Powell    Out Very Unlikely To Play               0\n",
            "10:30 PM ET  MIA AWAY        G         T. Herro    Out Very Unlikely To Play               0\n",
            "10:30 PM ET  MIA AWAY        G        T. Rozier    Out Very Unlikely To Play               0\n",
            "10:30 PM ET  LAC HOME        G        J. Miller    Out Very Unlikely To Play               0\n",
            "10:30 PM ET  LAC HOME        F       K. Sanders    Out Very Unlikely To Play               0\n",
            " 7:00 PM ET  MIL AWAY        F G. Antetokounmpo   Prob        Likely To Play              75\n",
            " 7:00 PM ET  MIL AWAY        G        K. Porter    Out Very Unlikely To Play               0\n",
            " 7:00 PM ET  MIN AWAY        G       A. Edwards    Out Very Unlikely To Play               0\n",
            " 7:00 PM ET  BKN HOME        F        D. Powell   Ques       Toss Up To Play              50\n",
            " 7:00 PM ET  BKN HOME        F       Danny Wolf    Out Very Unlikely To Play               0\n",
            " 7:00 PM ET  BKN HOME        F     H. Highsmith    Out Very Unlikely To Play               0\n",
            " 7:00 PM ET  IND HOME        G      A. Nembhard    Out Very Unlikely To Play               0\n",
            " 7:00 PM ET  IND HOME        F      B. Mathurin    Out Very Unlikely To Play               0\n",
            " 7:00 PM ET  IND HOME        F        J. Furphy   Prob        Likely To Play              75\n",
            " 7:00 PM ET  IND HOME        G        Kam Jones    Out Very Unlikely To Play               0\n",
            " 7:00 PM ET  IND HOME        F       Obi Toppin    Out Very Unlikely To Play               0\n",
            " 7:00 PM ET  IND HOME        G        R. Dennis   Ques       Toss Up To Play              50\n",
            " 7:00 PM ET  IND HOME        G    T. Haliburton    OFS   Very Likely To Play               0\n",
            " 7:00 PM ET  IND HOME        G     T. McConnell    Out Very Unlikely To Play               0\n",
            "\n",
            "Saved: may_not_play_players.csv\n"
          ]
        }
      ],
      "source": [
        "# pip install bs4 lxml pandas\n",
        "import os, re, pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "HTML_PATH = \"_rotowire_debug/last_lineups.html\" if os.path.exists(\"_rotowire_debug/last_lineups.html\") else \"last_lineups.html\"\n",
        "\n",
        "LIKELIHOOD_MAP = {\n",
        "    \"is-pct-play-100\": 100, \"is-pct-play-90\": 90, \"is-pct-play-75\": 75,\n",
        "    \"is-pct-play-60\": 60, \"is-pct-play-50\": 50, \"is-pct-play-40\": 40,\n",
        "    \"is-pct-play-25\": 25, \"is-pct-play-10\": 10, \"is-pct-play-0\": 0\n",
        "}\n",
        "\n",
        "def _txt(node): return re.sub(r\"\\s+\", \" \", node.get_text(\" \", strip=True)) if node else \"\"\n",
        "def _likelihood(classes): \n",
        "    for c in classes: \n",
        "        if c in LIKELIHOOD_MAP: \n",
        "            return LIKELIHOOD_MAP[c]\n",
        "    return None\n",
        "\n",
        "def parse_rotowire_mnp_final(html_path: str) -> pd.DataFrame:\n",
        "    with open(html_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
        "\n",
        "    rows = []\n",
        "    games = soup.select(\"div.lineup.is-nba[data-lnum]\")\n",
        "    print(f\"Found {len(games)} games in HTML.\")\n",
        "\n",
        "    for game in games:\n",
        "        game_time = _txt(game.select_one(\".lineup__time\"))\n",
        "        team_blocks = game.select(\".lineup__team\")\n",
        "        teams = []\n",
        "        for tb in team_blocks:\n",
        "            abbr = _txt(tb.select_one(\".lineup__abbr\"))\n",
        "            side = \"AWAY\" if \"is-visit\" in tb.get(\"class\", []) else \"HOME\" if \"is-home\" in tb.get(\"class\", []) else None\n",
        "            teams.append((abbr, side))\n",
        "\n",
        "        ul_lists = game.select(\"ul.lineup__list\")\n",
        "        for idx, ul in enumerate(ul_lists):\n",
        "            if idx >= len(teams):  # mismatch safety\n",
        "                continue\n",
        "            team, side = teams[idx]\n",
        "            mnp_title = ul.find(\"li\", class_=\"lineup__title\", string=lambda s: s and \"MAY NOT PLAY\" in s.upper())\n",
        "            if not mnp_title:\n",
        "                continue\n",
        "\n",
        "            for li in mnp_title.find_next_siblings(\"li\"):\n",
        "                classes = li.get(\"class\") or []\n",
        "                if \"lineup__title\" in classes:\n",
        "                    break\n",
        "                if \"lineup__player\" not in classes:\n",
        "                    continue\n",
        "\n",
        "                pos = _txt(li.select_one(\".lineup__pos\"))\n",
        "                a = li.select_one(\"a\")\n",
        "                player = _txt(a)\n",
        "                if not player:\n",
        "                    continue\n",
        "\n",
        "                status = _txt(li.select_one(\".lineup__inj\"))\n",
        "                title_text = (li.get(\"title\") or \"\").strip()\n",
        "                likelihood_pct = _likelihood(classes)\n",
        "\n",
        "                rows.append({\n",
        "                    \"game_time\": game_time,\n",
        "                    \"team\": team,\n",
        "                    \"side\": side,\n",
        "                    \"position\": pos,\n",
        "                    \"player\": player,\n",
        "                    \"status\": status,\n",
        "                    \"title_text\": title_text,\n",
        "                    \"likelihood_pct\": likelihood_pct\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    if df.empty:\n",
        "        print(\"âš ï¸ No 'May Not Play' players found. Check if Rotowire changed markup.\")\n",
        "    else:\n",
        "        df = df.sort_values([\"game_time\",\"side\",\"team\",\"player\"]).reset_index(drop=True)\n",
        "        print(f\"âœ… Parsed {len(df)} 'May Not Play' players across {df['team'].nunique()} teams.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---- RUN ----\n",
        "mnp_df = parse_rotowire_mnp_final(HTML_PATH)\n",
        "if not mnp_df.empty:\n",
        "    print(mnp_df.head(30).to_string(index=False))\n",
        "    mnp_df.to_csv(\"may_not_play_players.csv\", index=False)\n",
        "    print(\"\\nSaved: may_not_play_players.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecea42b8",
      "metadata": {},
      "source": [
        "## Cell 15: odds math + Excel export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f6c2e17e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "def american_to_prob(odds):\n",
        "    if pd.isna(odds): return np.nan\n",
        "    o = float(odds)\n",
        "    return 100.0/(o+100.0) if o>0 else (-o)/(-o+100.0)\n",
        "\n",
        "def devig_pair(p_over, p_under):\n",
        "    if pd.isna(p_over) or pd.isna(p_under): return (np.nan, np.nan)\n",
        "    s = p_over + p_under\n",
        "    if s <= 0: return (np.nan, np.nan)\n",
        "    return (p_over/s, p_under/s)\n",
        "\n",
        "def kelly_fraction(p, american_odds, cap=0.25):\n",
        "    if pd.isna(p) or pd.isna(american_odds): return 0.0\n",
        "    o = float(american_odds)\n",
        "    b = o/100.0 if o>0 else 100.0/(-o)\n",
        "    f = (p*(b+1)-1)/b\n",
        "    return float(max(0.0, min(f, cap)))\n",
        "\n",
        "def ev_flat_over(p, american_odds):\n",
        "    if pd.isna(p) or pd.isna(american_odds): return np.nan\n",
        "    o = float(american_odds)\n",
        "    win = o/100.0 if o>0 else 100.0/(-o)\n",
        "    lose = 1.0\n",
        "    return p*win - (1-p)*lose\n",
        "\n",
        "# Normal CDF helper (if SciPy available) to turn mean/sd into p_over\n",
        "try:\n",
        "    from scipy.stats import norm\n",
        "    def p_over_from_normal(mu, sd, line):\n",
        "        if pd.isna(mu) or pd.isna(sd) or pd.isna(line) or sd <= 0: return np.nan\n",
        "        return 1.0 - norm.cdf((line - mu)/sd)\n",
        "except Exception:\n",
        "    def p_over_from_normal(mu, sd, line): return np.nan\n",
        "\n",
        "def build_value_bets_excel(\n",
        "    df_projections, df_odds, outfile_path=None,\n",
        "    join_keys=(\"player\",\"team\",\"opponent\",\"market\",\"line\",\"book\",\"game_date\"),\n",
        "    cap_kelly=0.25\n",
        "):\n",
        "    def _norm(x): return None if pd.isna(x) else str(x).strip()\n",
        "    proj, odds = df_projections.copy(), df_odds.copy()\n",
        "    for k in join_keys:\n",
        "        if k in proj: proj[k] = proj[k].map(_norm)\n",
        "        if k in odds: odds[k] = odds[k].map(_norm)\n",
        "\n",
        "    merged = proj.merge(odds, on=list(join_keys), how=\"inner\", suffixes=(\"\", \"_odds\"))\n",
        "\n",
        "    if \"p_over_model\" not in merged.columns or merged[\"p_over_model\"].isna().all():\n",
        "        merged[\"p_over_model\"] = merged.apply(\n",
        "            lambda r: p_over_from_normal(r.get(\"projection_mean\"), r.get(\"projection_sd\"), r.get(\"line\")), axis=1\n",
        "        )\n",
        "\n",
        "    merged[\"p_over_imp\"]  = merged[\"over_odds\"].map(american_to_prob)\n",
        "    merged[\"p_under_imp\"] = merged[\"under_odds\"].map(american_to_prob)\n",
        "    merged[[\"p_over_fair\",\"p_under_fair\"]] = merged.apply(\n",
        "        lambda r: pd.Series(devig_pair(r[\"p_over_imp\"], r[\"p_under_imp\"])), axis=1\n",
        "    )\n",
        "\n",
        "    merged[\"edge_over\"]       = merged[\"p_over_model\"] - merged[\"p_over_fair\"]\n",
        "    merged[\"kelly_frac_over\"] = merged.apply(lambda r: kelly_fraction(r[\"p_over_model\"], r[\"over_odds\"], cap=cap_kelly), axis=1)\n",
        "    merged[\"EV_over_1u\"]      = merged.apply(lambda r: ev_flat_over(r[\"p_over_model\"], r[\"over_odds\"]), axis=1)\n",
        "    merged[\"asof_date\"]       = merged.get(\"asof_date\") if \"asof_date\" in merged else datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    preferred = [\n",
        "        \"asof_date\",\"game_date\",\"book\",\"player\",\"team\",\"opponent\",\"market\",\"line\",\"lineup_status\",\n",
        "        \"over_odds\",\"under_odds\",\"p_over_imp\",\"p_under_imp\",\"p_over_fair\",\"p_under_fair\",\"p_over_model\",\n",
        "        \"edge_over\",\"kelly_frac_over\",\"EV_over_1u\",\n",
        "        \"projected_minutes\",\"projection_mean\",\"projection_sd\",\"start_prob\",\n",
        "        \"opponent_allowance_idx\",\"team_orating\",\"opp_drating\",\n",
        "    ]\n",
        "    cols = [c for c in preferred if c in merged.columns] + [c for c in merged.columns if c not in preferred]\n",
        "    bets = merged[cols].sort_values([\"edge_over\",\"EV_over_1u\"], ascending=False).reset_index(drop=True)\n",
        "\n",
        "    summary = pd.DataFrame({\n",
        "        \"n_bets\":[len(bets)],\n",
        "        \"avg_edge_pp\":[bets[\"edge_over\"].mean()*100.0 if len(bets) else np.nan],\n",
        "        \"avg_kelly_pct\":[bets[\"kelly_frac_over\"].mean()*100.0 if len(bets) else np.nan],\n",
        "        \"avg_ev_1u\":[bets[\"EV_over_1u\"].mean() if len(bets) else np.nan],\n",
        "    })\n",
        "    by_market = bets.groupby(\"market\", dropna=False).agg(\n",
        "        n=(\"player\",\"count\"),\n",
        "        avg_edge_pp=(\"edge_over\", lambda x: 100.0*x.mean()),\n",
        "        avg_kelly_pct=(\"kelly_frac_over\", lambda x: 100.0*x.mean()),\n",
        "        avg_ev_1u=(\"EV_over_1u\",\"mean\")\n",
        "    ).reset_index()\n",
        "    by_book = bets.groupby(\"book\", dropna=False).agg(\n",
        "        n=(\"player\",\"count\"),\n",
        "        avg_edge_pp=(\"edge_over\", lambda x: 100.0*x.mean()),\n",
        "        avg_ev_1u=(\"EV_over_1u\",\"mean\")\n",
        "    ).reset_index()\n",
        "\n",
        "    if outfile_path is None:\n",
        "        outfile_path = f\"nba_value_bets_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
        "    with pd.ExcelWriter(outfile_path, engine=\"openpyxl\") as w:\n",
        "        bets.to_excel(w, sheet_name=\"Bets\", index=False)\n",
        "        summary.to_excel(w, sheet_name=\"Summary\", index=False, startrow=0)\n",
        "        by_market.to_excel(w, sheet_name=\"Summary\", index=False, startrow=5)\n",
        "        by_book.to_excel(w, sheet_name=\"Summary\", index=False, startrow=5+len(by_market)+3)\n",
        "\n",
        "        dd = pd.DataFrame([\n",
        "            (\"asof_date\",\"UTC run date\"), (\"game_date\",\"Game date\"),\n",
        "            (\"player\",\"Player\"), (\"team\",\"Team abbr\"), (\"opponent\",\"Opponent abbr\"),\n",
        "            (\"market\",\"PTS/REB/AST/3PM/PRA etc.\"), (\"line\",\"Book line\"), (\"book\",\"Sportsbook id\"),\n",
        "            (\"lineup_status\",\"EXPECTED/CONFIRMED/UNKNOWN\"),\n",
        "            (\"over_odds\",\"American odds Over\"), (\"under_odds\",\"American odds Under\"),\n",
        "            (\"p_over_imp\",\"Implied prob Over (pre-vig)\"), (\"p_under_imp\",\"Implied prob Under (pre-vig)\"),\n",
        "            (\"p_over_fair\",\"De-vigged prob Over\"), (\"p_under_fair\",\"De-vigged prob Under\"),\n",
        "            (\"p_over_model\",\"Model prob Over\"), (\"edge_over\",\"p_model âˆ’ p_fair\"),\n",
        "            (\"kelly_frac_over\",\"Kelly fraction (cap)\"), (\"EV_over_1u\",\"EV if staking 1u\"),\n",
        "            (\"projected_minutes\",\"Projected minutes\"), (\"projection_mean\",\"Projected mean\"),\n",
        "            (\"projection_sd\",\"Projected stdev\"), (\"start_prob\",\"Start probability\"),\n",
        "            (\"opponent_allowance_idx\",\"Opponent allowance index\"),\n",
        "            (\"team_orating\",\"Team ORtg\"), (\"opp_drating\",\"Opponent DRtg\"),\n",
        "        ], columns=[\"column\",\"description\"])\n",
        "        dd.to_excel(w, sheet_name=\"Data_Dictionary\", index=False)\n",
        "\n",
        "    return bets, outfile_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4afd97de",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 16: raw wide odds + resilient numeric parsing ===\n",
        "import re, json, pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def _first_numeric_float(x):\n",
        "    \"\"\"Return the first decimal number in x (e.g., '23.5, 24.5' -> 23.5).\"\"\"\n",
        "    if x is None: return None\n",
        "    s = str(x)\n",
        "    m = re.search(r\"[-+]?\\d+(?:\\.\\d+)?\", s)\n",
        "    return float(m.group()) if m else None\n",
        "\n",
        "def _first_numeric_int(x):\n",
        "    \"\"\"Return the first integer in x (e.g., '+110, +105' -> 110).\"\"\"\n",
        "    if x is None: return None\n",
        "    s = str(x)\n",
        "    m = re.search(r\"[-+]?\\d+\", s)\n",
        "    return int(m.group()) if m else None\n",
        "\n",
        "# override the helpers used by 16d converter (if defined)\n",
        "def _to_float_or_none(x):  # noqa: F811\n",
        "    return _first_numeric_float(x)\n",
        "\n",
        "def _to_int_or_none(x):    # noqa: F811\n",
        "    return _first_numeric_int(x)\n",
        "\n",
        "def get_player_props_odds_wide_raw(self, book=\"mgm\"):\n",
        "    \"\"\"\n",
        "    Returns the raw 'wide' odds table from Rotowire (no grouping, no aggregation).\n",
        "    Contains columns like mgm_pts, mgm_ptsOver, mgm_ptsUnder, etc.\n",
        "    \"\"\"\n",
        "    url = f\"https://www.rotowire.com/betting/nba/player-props.php?book={book}\"\n",
        "    r = self.session.get(url, headers=self.headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    matches = re.findall(r'data:\\s*(\\[\\{.*?\\}\\])', r.text, flags=re.DOTALL)\n",
        "    frames = []\n",
        "    for blob in matches:\n",
        "        try:\n",
        "            frames.append(pd.DataFrame(json.loads(blob)))\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not frames:\n",
        "        return pd.DataFrame()\n",
        "    # concat all blocks without grouping to preserve raw book columns\n",
        "    wide_raw = pd.concat(frames, ignore_index=True)\n",
        "    return wide_raw\n",
        "\n",
        "# attach to your scraper class\n",
        "NBAOddsAndLineupsScraper.get_player_props_odds_wide_raw = get_player_props_odds_wide_raw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fe88486d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projection rows by market: {'PTS': 694, 'REB': 694, 'AST': 694}\n"
          ]
        }
      ],
      "source": [
        "# === Cell 16: projections for PTS/REB/AST using your trained RF models ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Safety checks\n",
        "if \"models\" not in globals() or not models:\n",
        "    raise RuntimeError(\"No trained models found. Run Cell 7 first to populate `models` and `feature_cols_by_stat`.\")\n",
        "\n",
        "# We'll project for these markets\n",
        "MARKETS = [\"PTS\", \"REB\", \"AST\"]\n",
        "\n",
        "# Latest row per player as basis for \"next game\"\n",
        "latest = features_all.sort_values([\"PLAYER_NAME\",\"GAME_DATE\"]).groupby(\"PLAYER_NAME\").tail(1).copy()\n",
        "\n",
        "# Helper: per-stat stdev from last N actual games\n",
        "def _player_sd_map(stat: str, n=10):\n",
        "    def _sd(g):\n",
        "        s = g[stat].tail(n)\n",
        "        if s.notna().sum() >= 4:\n",
        "            return float(s.std(ddof=1))\n",
        "        return float(features_all[stat].std(ddof=1))\n",
        "    return features_all.groupby(\"PLAYER_NAME\").apply(_sd)\n",
        "\n",
        "# Normalize export keys common to all markets\n",
        "base_cols = {\n",
        "    \"PLAYER_NAME\": \"player\",\n",
        "    \"TEAM_ABBREVIATION\": \"team\",\n",
        "    \"OPPONENT_ABBREVIATION\": \"opponent\",\n",
        "}\n",
        "base_out = latest.rename(columns=base_cols)[[\"player\",\"team\",\"opponent\"]].copy()\n",
        "base_out[\"game_date\"] = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
        "base_out[\"projected_minutes\"] = latest.get(\"MIN_roll5\", pd.Series(index=latest.index)).fillna(30).clip(lower=10, upper=40).values\n",
        "base_out[\"start_prob\"] = 0.90\n",
        "base_out[\"lineup_status\"] = \"EXPECTED\"\n",
        "\n",
        "# Build one projection frame per market\n",
        "proj_frames = {}\n",
        "for stat in MARKETS:\n",
        "    if stat not in models:\n",
        "        print(f\"âš ï¸ Skipping {stat}: model not found in `models`.\")\n",
        "        continue\n",
        "    feat_cols = feature_cols_by_stat.get(stat, [])\n",
        "    if not feat_cols:\n",
        "        print(f\"âš ï¸ Skipping {stat}: no feature columns recorded in `feature_cols_by_stat`.\")\n",
        "        continue\n",
        "\n",
        "    X_pred = latest[feat_cols].fillna(method=\"ffill\").fillna(0)\n",
        "    pred_mean = models[stat].predict(X_pred)\n",
        "\n",
        "    # per-player SD\n",
        "    sd_map = _player_sd_map(stat)\n",
        "    pred_sd = latest[\"PLAYER_NAME\"].map(sd_map)\n",
        "    # conservative fallback SD = 15% of mean (min 1.0)\n",
        "    sd_fallback = np.maximum(np.abs(pred_mean) * 0.15, 1.0)\n",
        "    pred_sd = np.where(np.isnan(pred_sd), sd_fallback, pred_sd)\n",
        "\n",
        "    dfp = base_out.copy()\n",
        "    dfp[\"projection_mean\"] = pred_mean\n",
        "    dfp[\"projection_sd\"] = pred_sd\n",
        "    dfp[\"market\"] = stat\n",
        "\n",
        "    # Expose per-market frames\n",
        "    proj_frames[stat] = dfp[[\"player\",\"team\",\"opponent\",\"game_date\",\"market\",\n",
        "                             \"projection_mean\",\"projection_sd\",\"projected_minutes\",\"start_prob\",\"lineup_status\"]].copy()\n",
        "\n",
        "# Individual frames (kept for backward compatibility)\n",
        "df_projections_pts = proj_frames.get(\"PTS\", pd.DataFrame())\n",
        "df_projections_reb = proj_frames.get(\"REB\", pd.DataFrame())\n",
        "df_projections_ast = proj_frames.get(\"AST\", pd.DataFrame())\n",
        "\n",
        "# Combined projections across markets\n",
        "df_projections_all = pd.concat(list(proj_frames.values()), ignore_index=True) if proj_frames else pd.DataFrame()\n",
        "\n",
        "print(\"Projection rows by market:\",\n",
        "      {k: len(v) for k, v in proj_frames.items()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "25a69d29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Helper: turn wide props (per-book columns) into a long, tidy table ---\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def odds_wide_to_long_from_columns(\n",
        "    wide_df: pd.DataFrame,\n",
        "    *,\n",
        "    books: tuple[str, ...] = (\"mgm\",\"draftkings\",\"fanduel\",\"caesars\",\"betrivers\",\"espnbet\",\"hardrock\"),\n",
        "    markets: tuple[str, ...] = (\"PTS\",\"REB\",\"AST\"),\n",
        "    player_cols=(\"name\",\"player\",\"PLAYER_NAME\"),\n",
        "    team_cols=(\"team\",\"TEAM\",\"team_name\",\"TEAM_ABBREVIATION\"),\n",
        "    opp_cols=(\"opponent\",\"opp\",\"OPPONENT\",\"OPPONENT_ABBREVIATION\"),\n",
        "    date_cols=(\"game_date\",\"GAME_DATE\",\"date\")\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert a 'wide' props frame into a tidy long format:\n",
        "    one row per (player, market, book), with numeric line and American odds.\n",
        "\n",
        "    Expected column patterns (flexible by regex):\n",
        "      <book>_<suffix>                 -> the line (e.g., mgm_pts, fanduel_ast)\n",
        "      <book>_<suffix>_over_odds       -> over odds (American)\n",
        "      <book>_<suffix>_under_odds      -> under odds (American)\n",
        "\n",
        "    Suffixes recognized per market:\n",
        "      PTS:  'pts','points'\n",
        "      REB:  'reb','rebounds'\n",
        "      AST:  'ast','assists'\n",
        "    \"\"\"\n",
        "    df = wide_df.copy()\n",
        "\n",
        "    # Identify reference columns\n",
        "    def _first_col(cands):\n",
        "        for c in cands:\n",
        "            if c in df.columns: return c\n",
        "        return None\n",
        "\n",
        "    player_col = _first_col(player_cols)\n",
        "    team_col   = _first_col(team_cols)\n",
        "    opp_col    = _first_col(opp_cols)\n",
        "    date_col   = _first_col(date_cols)\n",
        "\n",
        "    # Fallbacks if totally missing\n",
        "    if player_col is None:\n",
        "        raise ValueError(\"Could not find a player name column in wide_df. \"\n",
        "                         f\"Tried {player_cols}. Got columns: {list(df.columns)[:20]}...\")\n",
        "\n",
        "    # Normalize helpers\n",
        "    def _num_float(x):\n",
        "        if pd.isna(x): return np.nan\n",
        "        m = re.search(r\"[-+]?\\d+(?:\\.\\d+)?\", str(x))\n",
        "        return float(m.group()) if m else np.nan\n",
        "\n",
        "    def _num_int(x):\n",
        "        if pd.isna(x): return np.nan\n",
        "        m = re.search(r\"[-+]?\\d+\", str(x))\n",
        "        return int(m.group()) if m else np.nan\n",
        "\n",
        "    # Market suffix map (flex)\n",
        "    market_suffixes = {\n",
        "        \"PTS\": (\"pts\",\"points\"),\n",
        "        \"REB\": (\"reb\",\"rebounds\"),\n",
        "        \"AST\": (\"ast\",\"assists\"),\n",
        "    }\n",
        "\n",
        "    # Build long rows\n",
        "    long_rows = []\n",
        "    # Iterate rows once; pull columns per book/market dynamically\n",
        "    for _, row in df.iterrows():\n",
        "        base = {\n",
        "            \"player\": row[player_col],\n",
        "            \"team\": row[team_col] if team_col else np.nan,\n",
        "            \"opponent\": row[opp_col] if opp_col else np.nan,\n",
        "            \"game_date\": row[date_col] if date_col else np.nan,\n",
        "        }\n",
        "        for mkt in markets:\n",
        "            suffixes = market_suffixes.get(mkt, ())\n",
        "            for b in books:\n",
        "                # Find the *line* column by trying allowed suffixes\n",
        "                line_val = np.nan\n",
        "                over_val = np.nan\n",
        "                under_val = np.nan\n",
        "                line_col_used = None\n",
        "\n",
        "                for suf in suffixes:\n",
        "                    # exact line column (most common)\n",
        "                    c_line = f\"{b}_{suf}\"\n",
        "                    if c_line in df.columns and pd.notna(row[c_line]):\n",
        "                        line_val = row[c_line]\n",
        "                        line_col_used = c_line\n",
        "                        # odds columns (several sites use these names)\n",
        "                        for over_name in (f\"{b}_{suf}_over_odds\", f\"{b}_{suf}_o_odds\", f\"{b}_{suf}_over\"):\n",
        "                            if over_name in df.columns:\n",
        "                                over_val = row[over_name]\n",
        "                                break\n",
        "                        for under_name in (f\"{b}_{suf}_under_odds\", f\"{b}_{suf}_u_odds\", f\"{b}_{suf}_under\"):\n",
        "                            if under_name in df.columns:\n",
        "                                under_val = row[under_name]\n",
        "                                break\n",
        "                        break  # found a suffix match\n",
        "\n",
        "                # If not found, try a looser search (e.g., 'mgm_pts_line')\n",
        "                if (isinstance(line_val, float) and np.isnan(line_val)) or line_col_used is None:\n",
        "                    pat = re.compile(rf\"^{re.escape(b)}_({ '|'.join(map(re.escape, suffixes)) })(_line)?$\", re.I)\n",
        "                    for c in df.columns:\n",
        "                        if pat.match(str(c)) and pd.notna(row[c]):\n",
        "                            line_val = row[c]\n",
        "                            line_col_used = c\n",
        "                            # odds columns with same base\n",
        "                            base_prefix = re.sub(r\"(_line)?$\", \"\", c)\n",
        "                            for over_name in (f\"{base_prefix}_over_odds\", f\"{base_prefix}_o_odds\", f\"{base_prefix}_over\"):\n",
        "                                if over_name in df.columns:\n",
        "                                    over_val = row[over_name]\n",
        "                                    break\n",
        "                            for under_name in (f\"{base_prefix}_under_odds\", f\"{base_prefix}_u_odds\", f\"{base_prefix}_under\"):\n",
        "                                if under_name in df.columns:\n",
        "                                    under_val = row[under_name]\n",
        "                                    break\n",
        "                            break\n",
        "\n",
        "                # Only emit a row if we actually found a line\n",
        "                if pd.notna(line_val):\n",
        "                    long_rows.append({\n",
        "                        **base,\n",
        "                        \"market\": mkt,\n",
        "                        \"book\": b,\n",
        "                        \"line\": _num_float(line_val),\n",
        "                        \"over_odds\": _num_int(over_val),\n",
        "                        \"under_odds\": _num_int(under_val),\n",
        "                    })\n",
        "\n",
        "    out = pd.DataFrame(long_rows)\n",
        "\n",
        "    # Clean up: drop obviously invalid lines\n",
        "    if not out.empty:\n",
        "        out = out[pd.notna(out[\"line\"])]\n",
        "        # remove zero/negative lines that can't be real for these markets (optional)\n",
        "        out = out[out[\"line\"] > 0]\n",
        "\n",
        "        # De-duplicate best-effort (sometimes the page contains duplicates per book)\n",
        "        out = (out.sort_values([\"player\",\"market\",\"book\",\"line\"])\n",
        "                  .drop_duplicates(subset=[\"player\",\"market\",\"book\"], keep=\"last\")\n",
        "                  .reset_index(drop=True))\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rows in raw wide: 1655\n",
            "Books present in columns: ['betrivers', 'caesars', 'draftkings', 'espnbet', 'fanduel', 'hardrock', 'mgm']\n",
            "Books with lines: {'PTS': ['betrivers', 'caesars', 'draftkings', 'fanduel', 'hardrock', 'mgm'], 'REB': ['betrivers', 'caesars', 'draftkings', 'fanduel', 'hardrock', 'mgm'], 'AST': ['betrivers', 'caesars', 'draftkings', 'fanduel', 'hardrock', 'mgm']}\n",
            "odds_long rows: 1790\n",
            "odds_long columns: ['player', 'team', 'opponent', 'game_date', 'market', 'book', 'line', 'over_odds', 'under_odds']\n",
            "     player team opponent  game_date market        book  line  over_odds  \\\n",
            "0  AJ Green  MIL     @IND        NaN    AST         mgm   1.5        NaN   \n",
            "1  AJ Green  MIL     @IND        NaN    PTS   betrivers   8.5        NaN   \n",
            "2  AJ Green  MIL     @IND        NaN    PTS     caesars   8.5        NaN   \n",
            "3  AJ Green  MIL     @IND        NaN    PTS  draftkings   9.5        NaN   \n",
            "4  AJ Green  MIL     @IND        NaN    PTS     fanduel   8.5        NaN   \n",
            "\n",
            "   under_odds  \n",
            "0         NaN  \n",
            "1         NaN  \n",
            "2         NaN  \n",
            "3         NaN  \n",
            "4         NaN  \n",
            "\n",
            "Saved value bets to: nba_value_bets_20251103.xlsx\n",
            "331 value bets found across 3 markets.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asof_date</th>\n",
              "      <th>game_date</th>\n",
              "      <th>book</th>\n",
              "      <th>player</th>\n",
              "      <th>team</th>\n",
              "      <th>opponent</th>\n",
              "      <th>market</th>\n",
              "      <th>line</th>\n",
              "      <th>lineup_status</th>\n",
              "      <th>over_odds</th>\n",
              "      <th>...</th>\n",
              "      <th>projection_mean</th>\n",
              "      <th>projection_sd</th>\n",
              "      <th>start_prob</th>\n",
              "      <th>team_odds</th>\n",
              "      <th>opponent_odds</th>\n",
              "      <th>line_odds</th>\n",
              "      <th>book_odds</th>\n",
              "      <th>game_date_odds</th>\n",
              "      <th>over_odds_odds</th>\n",
              "      <th>under_odds_odds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>AJ Green</td>\n",
              "      <td>MIL</td>\n",
              "      <td>@IND</td>\n",
              "      <td>PTS</td>\n",
              "      <td>8.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>9.090041</td>\n",
              "      <td>1.363506</td>\n",
              "      <td>0.9</td>\n",
              "      <td>MIL</td>\n",
              "      <td>@IND</td>\n",
              "      <td>8.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Aaron Gordon</td>\n",
              "      <td>DEN</td>\n",
              "      <td>SAC</td>\n",
              "      <td>PTS</td>\n",
              "      <td>15.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>16.117466</td>\n",
              "      <td>2.417620</td>\n",
              "      <td>0.9</td>\n",
              "      <td>DEN</td>\n",
              "      <td>SAC</td>\n",
              "      <td>15.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Aaron Nesmith</td>\n",
              "      <td>IND</td>\n",
              "      <td>MIL</td>\n",
              "      <td>PTS</td>\n",
              "      <td>15.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>11.377908</td>\n",
              "      <td>1.706686</td>\n",
              "      <td>0.9</td>\n",
              "      <td>IND</td>\n",
              "      <td>MIL</td>\n",
              "      <td>15.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Alex Sarr</td>\n",
              "      <td>WAS</td>\n",
              "      <td>@NYK</td>\n",
              "      <td>PTS</td>\n",
              "      <td>14.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>15.788472</td>\n",
              "      <td>2.368271</td>\n",
              "      <td>0.9</td>\n",
              "      <td>WAS</td>\n",
              "      <td>@NYK</td>\n",
              "      <td>14.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Alperen Sengun</td>\n",
              "      <td>HOU</td>\n",
              "      <td>DAL</td>\n",
              "      <td>PTS</td>\n",
              "      <td>20.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>15.609197</td>\n",
              "      <td>2.341380</td>\n",
              "      <td>0.9</td>\n",
              "      <td>HOU</td>\n",
              "      <td>DAL</td>\n",
              "      <td>20.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Amen Thompson</td>\n",
              "      <td>HOU</td>\n",
              "      <td>DAL</td>\n",
              "      <td>PTS</td>\n",
              "      <td>16.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>14.023068</td>\n",
              "      <td>2.103460</td>\n",
              "      <td>0.9</td>\n",
              "      <td>HOU</td>\n",
              "      <td>DAL</td>\n",
              "      <td>16.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Andrew Wiggins</td>\n",
              "      <td>MIA</td>\n",
              "      <td>@LAC</td>\n",
              "      <td>PTS</td>\n",
              "      <td>16.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>21.194647</td>\n",
              "      <td>3.179197</td>\n",
              "      <td>0.9</td>\n",
              "      <td>MIA</td>\n",
              "      <td>@LAC</td>\n",
              "      <td>16.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Anfernee Simons</td>\n",
              "      <td>BOS</td>\n",
              "      <td>UTA</td>\n",
              "      <td>PTS</td>\n",
              "      <td>15.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>17.163609</td>\n",
              "      <td>2.574541</td>\n",
              "      <td>0.9</td>\n",
              "      <td>BOS</td>\n",
              "      <td>UTA</td>\n",
              "      <td>15.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Ausar Thompson</td>\n",
              "      <td>DET</td>\n",
              "      <td>@MEM</td>\n",
              "      <td>PTS</td>\n",
              "      <td>13.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>13.508487</td>\n",
              "      <td>2.026273</td>\n",
              "      <td>0.9</td>\n",
              "      <td>DET</td>\n",
              "      <td>@MEM</td>\n",
              "      <td>13.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Austin Reaves</td>\n",
              "      <td>LAL</td>\n",
              "      <td>@POR</td>\n",
              "      <td>PTS</td>\n",
              "      <td>24.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>22.698799</td>\n",
              "      <td>3.404820</td>\n",
              "      <td>0.9</td>\n",
              "      <td>LAL</td>\n",
              "      <td>@POR</td>\n",
              "      <td>24.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Bam Adebayo</td>\n",
              "      <td>MIA</td>\n",
              "      <td>@LAC</td>\n",
              "      <td>PTS</td>\n",
              "      <td>19.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>21.228500</td>\n",
              "      <td>3.184275</td>\n",
              "      <td>0.9</td>\n",
              "      <td>MIA</td>\n",
              "      <td>@LAC</td>\n",
              "      <td>19.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Bilal Coulibaly</td>\n",
              "      <td>WAS</td>\n",
              "      <td>@NYK</td>\n",
              "      <td>PTS</td>\n",
              "      <td>11.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>11.604558</td>\n",
              "      <td>1.740684</td>\n",
              "      <td>0.9</td>\n",
              "      <td>WAS</td>\n",
              "      <td>@NYK</td>\n",
              "      <td>11.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Bobby Portis</td>\n",
              "      <td>MIL</td>\n",
              "      <td>@IND</td>\n",
              "      <td>PTS</td>\n",
              "      <td>10.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>15.313920</td>\n",
              "      <td>2.297088</td>\n",
              "      <td>0.9</td>\n",
              "      <td>MIL</td>\n",
              "      <td>@IND</td>\n",
              "      <td>10.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Bradley Beal</td>\n",
              "      <td>LAC</td>\n",
              "      <td>MIA</td>\n",
              "      <td>PTS</td>\n",
              "      <td>10.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>16.090370</td>\n",
              "      <td>2.413556</td>\n",
              "      <td>0.9</td>\n",
              "      <td>LAC</td>\n",
              "      <td>MIA</td>\n",
              "      <td>10.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>caesars</td>\n",
              "      <td>Brook Lopez</td>\n",
              "      <td>LAC</td>\n",
              "      <td>MIA</td>\n",
              "      <td>PTS</td>\n",
              "      <td>6.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>13.562007</td>\n",
              "      <td>2.034301</td>\n",
              "      <td>0.9</td>\n",
              "      <td>LAC</td>\n",
              "      <td>MIA</td>\n",
              "      <td>6.5</td>\n",
              "      <td>caesars</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>mgm</td>\n",
              "      <td>Bruce Brown</td>\n",
              "      <td>DEN</td>\n",
              "      <td>SAC</td>\n",
              "      <td>PTS</td>\n",
              "      <td>5.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>8.134441</td>\n",
              "      <td>1.220166</td>\n",
              "      <td>0.9</td>\n",
              "      <td>DEN</td>\n",
              "      <td>SAC</td>\n",
              "      <td>5.5</td>\n",
              "      <td>mgm</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>caesars</td>\n",
              "      <td>Bub Carrington</td>\n",
              "      <td>WAS</td>\n",
              "      <td>@NYK</td>\n",
              "      <td>PTS</td>\n",
              "      <td>7.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>11.521950</td>\n",
              "      <td>1.728293</td>\n",
              "      <td>0.9</td>\n",
              "      <td>WAS</td>\n",
              "      <td>@NYK</td>\n",
              "      <td>7.5</td>\n",
              "      <td>caesars</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>CJ McCollum</td>\n",
              "      <td>WAS</td>\n",
              "      <td>@NYK</td>\n",
              "      <td>PTS</td>\n",
              "      <td>15.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>25.106386</td>\n",
              "      <td>3.765958</td>\n",
              "      <td>0.9</td>\n",
              "      <td>WAS</td>\n",
              "      <td>@NYK</td>\n",
              "      <td>15.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Cade Cunningham</td>\n",
              "      <td>DET</td>\n",
              "      <td>@MEM</td>\n",
              "      <td>PTS</td>\n",
              "      <td>27.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>29.872528</td>\n",
              "      <td>4.480879</td>\n",
              "      <td>0.9</td>\n",
              "      <td>DET</td>\n",
              "      <td>@MEM</td>\n",
              "      <td>27.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2025-11-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>Cam Thomas</td>\n",
              "      <td>BKN</td>\n",
              "      <td>MIN</td>\n",
              "      <td>PTS</td>\n",
              "      <td>23.5</td>\n",
              "      <td>EXPECTED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>22.159389</td>\n",
              "      <td>3.323909</td>\n",
              "      <td>0.9</td>\n",
              "      <td>BKN</td>\n",
              "      <td>MIN</td>\n",
              "      <td>23.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows Ã— 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     asof_date  game_date       book           player team opponent market  \\\n",
              "0   2025-11-03        NaN  betrivers         AJ Green  MIL     @IND    PTS   \n",
              "1   2025-11-03        NaN  betrivers     Aaron Gordon  DEN      SAC    PTS   \n",
              "2   2025-11-03        NaN  betrivers    Aaron Nesmith  IND      MIL    PTS   \n",
              "3   2025-11-03        NaN  betrivers        Alex Sarr  WAS     @NYK    PTS   \n",
              "4   2025-11-03        NaN  betrivers   Alperen Sengun  HOU      DAL    PTS   \n",
              "5   2025-11-03        NaN  betrivers    Amen Thompson  HOU      DAL    PTS   \n",
              "6   2025-11-03        NaN  betrivers   Andrew Wiggins  MIA     @LAC    PTS   \n",
              "7   2025-11-03        NaN  betrivers  Anfernee Simons  BOS      UTA    PTS   \n",
              "8   2025-11-03        NaN  betrivers   Ausar Thompson  DET     @MEM    PTS   \n",
              "9   2025-11-03        NaN  betrivers    Austin Reaves  LAL     @POR    PTS   \n",
              "10  2025-11-03        NaN  betrivers      Bam Adebayo  MIA     @LAC    PTS   \n",
              "11  2025-11-03        NaN  betrivers  Bilal Coulibaly  WAS     @NYK    PTS   \n",
              "12  2025-11-03        NaN  betrivers     Bobby Portis  MIL     @IND    PTS   \n",
              "13  2025-11-03        NaN  betrivers     Bradley Beal  LAC      MIA    PTS   \n",
              "14  2025-11-03        NaN    caesars      Brook Lopez  LAC      MIA    PTS   \n",
              "15  2025-11-03        NaN        mgm      Bruce Brown  DEN      SAC    PTS   \n",
              "16  2025-11-03        NaN    caesars   Bub Carrington  WAS     @NYK    PTS   \n",
              "17  2025-11-03        NaN  betrivers      CJ McCollum  WAS     @NYK    PTS   \n",
              "18  2025-11-03        NaN  betrivers  Cade Cunningham  DET     @MEM    PTS   \n",
              "19  2025-11-03        NaN  betrivers       Cam Thomas  BKN      MIN    PTS   \n",
              "\n",
              "    line lineup_status  over_odds  ...  projection_mean  projection_sd  \\\n",
              "0    8.5      EXPECTED        NaN  ...         9.090041       1.363506   \n",
              "1   15.5      EXPECTED        NaN  ...        16.117466       2.417620   \n",
              "2   15.5      EXPECTED        NaN  ...        11.377908       1.706686   \n",
              "3   14.5      EXPECTED        NaN  ...        15.788472       2.368271   \n",
              "4   20.5      EXPECTED        NaN  ...        15.609197       2.341380   \n",
              "5   16.5      EXPECTED        NaN  ...        14.023068       2.103460   \n",
              "6   16.5      EXPECTED        NaN  ...        21.194647       3.179197   \n",
              "7   15.5      EXPECTED        NaN  ...        17.163609       2.574541   \n",
              "8   13.5      EXPECTED        NaN  ...        13.508487       2.026273   \n",
              "9   24.5      EXPECTED        NaN  ...        22.698799       3.404820   \n",
              "10  19.5      EXPECTED        NaN  ...        21.228500       3.184275   \n",
              "11  11.5      EXPECTED        NaN  ...        11.604558       1.740684   \n",
              "12  10.5      EXPECTED        NaN  ...        15.313920       2.297088   \n",
              "13  10.5      EXPECTED        NaN  ...        16.090370       2.413556   \n",
              "14   6.5      EXPECTED        NaN  ...        13.562007       2.034301   \n",
              "15   5.5      EXPECTED        NaN  ...         8.134441       1.220166   \n",
              "16   7.5      EXPECTED        NaN  ...        11.521950       1.728293   \n",
              "17  15.5      EXPECTED        NaN  ...        25.106386       3.765958   \n",
              "18  27.5      EXPECTED        NaN  ...        29.872528       4.480879   \n",
              "19  23.5      EXPECTED        NaN  ...        22.159389       3.323909   \n",
              "\n",
              "    start_prob  team_odds  opponent_odds  line_odds  book_odds  \\\n",
              "0          0.9        MIL           @IND        8.5  betrivers   \n",
              "1          0.9        DEN            SAC       15.5  betrivers   \n",
              "2          0.9        IND            MIL       15.5  betrivers   \n",
              "3          0.9        WAS           @NYK       14.5  betrivers   \n",
              "4          0.9        HOU            DAL       20.5  betrivers   \n",
              "5          0.9        HOU            DAL       16.5  betrivers   \n",
              "6          0.9        MIA           @LAC       16.5  betrivers   \n",
              "7          0.9        BOS            UTA       15.5  betrivers   \n",
              "8          0.9        DET           @MEM       13.5  betrivers   \n",
              "9          0.9        LAL           @POR       24.5  betrivers   \n",
              "10         0.9        MIA           @LAC       19.5  betrivers   \n",
              "11         0.9        WAS           @NYK       11.5  betrivers   \n",
              "12         0.9        MIL           @IND       10.5  betrivers   \n",
              "13         0.9        LAC            MIA       10.5  betrivers   \n",
              "14         0.9        LAC            MIA        6.5    caesars   \n",
              "15         0.9        DEN            SAC        5.5        mgm   \n",
              "16         0.9        WAS           @NYK        7.5    caesars   \n",
              "17         0.9        WAS           @NYK       15.5  betrivers   \n",
              "18         0.9        DET           @MEM       27.5  betrivers   \n",
              "19         0.9        BKN            MIN       23.5  betrivers   \n",
              "\n",
              "    game_date_odds  over_odds_odds  under_odds_odds  \n",
              "0              NaN             NaN              NaN  \n",
              "1              NaN             NaN              NaN  \n",
              "2              NaN             NaN              NaN  \n",
              "3              NaN             NaN              NaN  \n",
              "4              NaN             NaN              NaN  \n",
              "5              NaN             NaN              NaN  \n",
              "6              NaN             NaN              NaN  \n",
              "7              NaN             NaN              NaN  \n",
              "8              NaN             NaN              NaN  \n",
              "9              NaN             NaN              NaN  \n",
              "10             NaN             NaN              NaN  \n",
              "11             NaN             NaN              NaN  \n",
              "12             NaN             NaN              NaN  \n",
              "13             NaN             NaN              NaN  \n",
              "14             NaN             NaN              NaN  \n",
              "15             NaN             NaN              NaN  \n",
              "16             NaN             NaN              NaN  \n",
              "17             NaN             NaN              NaN  \n",
              "18             NaN             NaN              NaN  \n",
              "19             NaN             NaN              NaN  \n",
              "\n",
              "[20 rows x 30 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === Cell 17: wide_raw â†’ long (PTS/REB/AST) â†’ join â†’ export ===\n",
        "from datetime import datetime\n",
        "import re, unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "scraper = NBAOddsAndLineupsScraper()\n",
        "\n",
        "# 1) Raw wide odds (no grouping)\n",
        "wide_raw = scraper.get_player_props_odds_wide_raw(book=\"mgm\")\n",
        "if wide_raw.empty:\n",
        "    raise RuntimeError(\"Raw wide odds are empty. The page structure may have changed or was blocked.\")\n",
        "\n",
        "print(\"Total rows in raw wide:\", len(wide_raw))\n",
        "\n",
        "# 2) Detect books present\n",
        "books_seen = sorted({\n",
        "    m.group(1) for c in wide_raw.columns\n",
        "    if (m := re.match(r\"^(draftkings|fanduel|caesars|betrivers|espnbet|hardrock|mgm)_(.+)$\", c))\n",
        "})\n",
        "print(\"Books present in columns:\", books_seen)\n",
        "\n",
        "def _col_exists_nonnull(df, col):\n",
        "    return (col in df.columns) and df[col].notna().any()\n",
        "\n",
        "# Which books have each market today?\n",
        "market_suffix = {\"PTS\":\"pts\",\"REB\":\"reb\",\"AST\":\"ast\"}\n",
        "books_by_market = {\n",
        "    m: [b for b in books_seen if _col_exists_nonnull(wide_raw, f\"{b}_{market_suffix[m]}\")]\n",
        "    for m in [\"PTS\",\"REB\",\"AST\"]\n",
        "}\n",
        "print(\"Books with lines:\", {m: v for m, v in books_by_market.items() if v})\n",
        "\n",
        "# 3) Convert wide â†’ long for markets that actually have any lines\n",
        "target_markets = tuple([m for m, bs in books_by_market.items() if bs])\n",
        "if not target_markets:\n",
        "    raise RuntimeError(\"No books have non-null PTS/REB/AST lines today.\")\n",
        "\n",
        "odds_long = odds_wide_to_long_from_columns(\n",
        "    wide_raw,\n",
        "    books=tuple(sorted({b for bs in books_by_market.values() for b in bs})),\n",
        "    markets=target_markets\n",
        ")\n",
        "if odds_long.empty:\n",
        "    raise RuntimeError(\"odds_long is empty after conversion. Verify your `odds_wide_to_long_from_columns` mapping.\")\n",
        "\n",
        "# Normalize obvious numerics\n",
        "def _num_float(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    m = re.search(r\"[-+]?\\d+(?:\\.\\d+)?\", str(x))\n",
        "    return float(m.group()) if m else np.nan\n",
        "\n",
        "def _num_int(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    m = re.search(r\"[-+]?\\d+\", str(x))\n",
        "    return int(m.group()) if m else np.nan\n",
        "\n",
        "odds_long[\"line\"] = odds_long[\"line\"].apply(_num_float)\n",
        "odds_long[\"over_odds\"] = odds_long[\"over_odds\"].apply(_num_int)\n",
        "odds_long[\"under_odds\"] = odds_long[\"under_odds\"].apply(_num_int)\n",
        "\n",
        "print(\"odds_long rows:\", len(odds_long))\n",
        "print(\"odds_long columns:\", odds_long.columns.tolist())\n",
        "print(odds_long.head(5))\n",
        "\n",
        "# 4) Prepare projections union (must be created earlier, e.g., Cell 16e)\n",
        "if \"df_projections_all\" not in globals() or df_projections_all.empty:\n",
        "    raise RuntimeError(\"df_projections_all not found or empty (run Cell 16 that builds PTS/REB/AST projections).\")\n",
        "\n",
        "# Light name normalizer\n",
        "def _norm_player(name: str) -> str:\n",
        "    if not isinstance(name, str): return \"\"\n",
        "    s = unicodedata.normalize(\"NFKD\", name)\n",
        "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
        "    s = re.sub(r\"[.\\-`'â€™]\", \"\", s).strip().lower()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "odds_long = odds_long.copy()\n",
        "df_projections_all = df_projections_all.copy()\n",
        "\n",
        "odds_long[\"player_key\"] = odds_long[\"player\"].map(_norm_player)\n",
        "df_projections_all[\"player_key\"] = df_projections_all[\"player\"].map(_norm_player)\n",
        "\n",
        "# Ensure projection SD exists per market (fallback to 15% of mean, min 1.0)\n",
        "for m in [\"PTS\",\"REB\",\"AST\"]:\n",
        "    mask = df_projections_all[\"market\"].eq(m)\n",
        "    if \"projection_sd\" not in df_projections_all.columns:\n",
        "        df_projections_all[\"projection_sd\"] = np.nan\n",
        "    missing_sd = df_projections_all.loc[mask, \"projection_sd\"].isna() | (df_projections_all.loc[mask, \"projection_sd\"] <= 0)\n",
        "    if missing_sd.any():\n",
        "        df_projections_all.loc[mask, \"projection_sd\"] = (\n",
        "            df_projections_all.loc[mask, \"projection_mean\"].abs() * 0.15\n",
        "        ).clip(lower=1.0)\n",
        "\n",
        "# 5) Split and join per market, then combine\n",
        "joined_frames = []\n",
        "for mkt in target_markets:\n",
        "    odds_m = odds_long.loc[odds_long[\"market\"].eq(mkt)].copy()\n",
        "    proj_m = df_projections_all.loc[df_projections_all[\"market\"].eq(mkt)].copy()\n",
        "    if odds_m.empty or proj_m.empty:\n",
        "        print(f\"âš ï¸ Skipping {mkt}: odds or projections empty.\")\n",
        "        continue\n",
        "\n",
        "    join_keys = [\"player_key\",\"market\"]\n",
        "    view_cols_odds = join_keys + [\"player\",\"team\",\"opponent\",\"line\",\"book\",\"game_date\",\"over_odds\",\"under_odds\"]\n",
        "    view_cols_odds = [c for c in view_cols_odds if c in odds_m.columns]\n",
        "\n",
        "    view_cols_proj = join_keys + [\n",
        "        \"player\",\"team\",\"opponent\",\"game_date\",\"projection_mean\",\"projection_sd\",\n",
        "        \"projected_minutes\",\"start_prob\",\"lineup_status\"\n",
        "    ]\n",
        "    view_cols_proj = [c for c in view_cols_proj if c in proj_m.columns]\n",
        "\n",
        "    dfj = proj_m[view_cols_proj].merge(\n",
        "        odds_m[view_cols_odds].rename(columns={\"player\":\"player_odds\",\"team\":\"team_odds\",\"opponent\":\"opponent_odds\",\"game_date\":\"game_date_odds\"}),\n",
        "        on=join_keys, how=\"inner\", suffixes=(\"_proj\",\"_odds\")\n",
        "    )\n",
        "\n",
        "    if dfj.empty:\n",
        "        print(f\"âš ï¸ Join produced 0 rows for {mkt}. Check name variants.\")\n",
        "        continue\n",
        "\n",
        "    # Resolve canonical columns\n",
        "    def _pick_first(df_, names, default=np.nan):\n",
        "        for n in names:\n",
        "            if n in df_.columns:\n",
        "                return df_[n]\n",
        "        return default\n",
        "\n",
        "    dfj = dfj.loc[:, ~dfj.columns.duplicated()].copy()\n",
        "    dfj[\"player\"]    = _pick_first(dfj, [\"player_odds\",\"player_proj\",\"player\"])\n",
        "    dfj[\"team\"]      = _pick_first(dfj, [\"team_odds\",\"team_proj\",\"team\"])\n",
        "    dfj[\"opponent\"]  = _pick_first(dfj, [\"opponent_odds\",\"opponent_proj\",\"opponent\"])\n",
        "    dfj[\"game_date\"] = _pick_first(dfj, [\"game_date_odds\",\"game_date_proj\",\"game_date\"])\n",
        "    # line already numeric above, but if any slipped through:\n",
        "    dfj[\"line\"] = dfj[\"line\"].apply(_num_float)\n",
        "\n",
        "    # Compute model probability P(Over)\n",
        "    if \"p_over_from_normal\" not in globals():\n",
        "        from statistics import NormalDist\n",
        "        def p_over_from_normal(mu, sd, line):\n",
        "            if pd.isna(mu) or pd.isna(sd) or pd.isna(line) or float(sd) <= 0: return np.nan\n",
        "            z = (float(line) - float(mu)) / float(sd)\n",
        "            return 1.0 - NormalDist().cdf(z)\n",
        "\n",
        "    dfj[\"p_over_model\"] = dfj.apply(\n",
        "        lambda r: p_over_from_normal(r.get(\"projection_mean\"), r.get(\"projection_sd\"), r.get(\"line\")), axis=1\n",
        "    )\n",
        "\n",
        "    # Implied/fair probabilities + edge (so we can pick best book later)\n",
        "    def implied_prob(american):\n",
        "        o = _num_int(american)\n",
        "        if pd.isna(o): return np.nan\n",
        "        return (-o)/(-o+100.0) if o < 0 else 100.0/(o+100.0)\n",
        "\n",
        "    dfj[\"p_over_imp\"]  = dfj[\"over_odds\"].apply(implied_prob)\n",
        "    dfj[\"p_under_imp\"] = dfj[\"under_odds\"].apply(implied_prob)\n",
        "\n",
        "    def devig_pair(p_over_imp, p_under_imp):\n",
        "        if pd.isna(p_over_imp) or pd.isna(p_under_imp):\n",
        "            return (np.nan, np.nan)\n",
        "        s = p_over_imp + p_under_imp\n",
        "        if s <= 0:\n",
        "            return (np.nan, np.nan)\n",
        "        return (p_over_imp/s, p_under_imp/s)\n",
        "\n",
        "    fair = dfj.apply(\n",
        "        lambda r: pd.Series(devig_pair(r[\"p_over_imp\"], r[\"p_under_imp\"]), index=[\"p_over_fair\",\"p_under_fair\"]),\n",
        "        axis=1\n",
        "    )\n",
        "    dfj = pd.concat([dfj, fair], axis=1)\n",
        "\n",
        "    dfj[\"edge_over\"] = np.where(\n",
        "        dfj[\"p_over_fair\"].notna(),\n",
        "        dfj[\"p_over_model\"] - dfj[\"p_over_fair\"],\n",
        "        dfj[\"p_over_model\"] - dfj[\"p_over_imp\"]\n",
        "    )\n",
        "\n",
        "    # Keep the best book per player/market (highest edge)\n",
        "    dfj = dfj.sort_values([\"player\",\"market\",\"edge_over\"], ascending=[True, True, False])\n",
        "    dfj = dfj.drop_duplicates(subset=[\"player\",\"market\"], keep=\"first\")\n",
        "\n",
        "    joined_frames.append(\n",
        "        dfj[[\n",
        "            \"player\",\"team\",\"opponent\",\"market\",\"line\",\"book\",\"game_date\",\n",
        "            \"over_odds\",\"under_odds\",\"projection_mean\",\"projection_sd\",\n",
        "            \"projected_minutes\",\"start_prob\",\"lineup_status\",\"p_over_model\",\"edge_over\"\n",
        "        ]]\n",
        "    )\n",
        "\n",
        "# Combined joined frame for all markets\n",
        "if not joined_frames:\n",
        "    raise RuntimeError(\"No joined rows produced for any market.\")\n",
        "df_proj_join_all = pd.concat(joined_frames, ignore_index=True)\n",
        "\n",
        "# 6) Prepare odds slice for Excel builder\n",
        "df_odds_for_excel = df_proj_join_all[[\n",
        "    \"player\",\"team\",\"opponent\",\"market\",\"line\",\"book\",\"game_date\",\"over_odds\",\"under_odds\"\n",
        "]].copy()\n",
        "\n",
        "# 7) Export full bets workbook\n",
        "bets, excel_path = build_value_bets_excel(\n",
        "    df_projections=df_proj_join_all,\n",
        "    df_odds=df_odds_for_excel,\n",
        "    outfile_path=f\"nba_value_bets_{datetime.utcnow().strftime('%Y%m%d')}.xlsx\",\n",
        "    join_keys=(\"player\",\"market\")  # permissive merge\n",
        ")\n",
        "\n",
        "print(f\"\\nSaved value bets to: {excel_path}\")\n",
        "print(len(bets), \"value bets found across\", df_proj_join_all['market'].nunique(), \"markets.\")\n",
        "display(bets.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Saved Top-100 value bets to: data/bets\\value_bets_top100_20251103.xlsx\n",
            "Rows â†’ priced: 0 | all: 331\n"
          ]
        }
      ],
      "source": [
        "# === BUILD & SAVE VALUE BETS (Top-100) TO /data/bets ===\n",
        "import os, re\n",
        "import numpy as np, pandas as pd\n",
        "from datetime import datetime\n",
        "from statistics import NormalDist\n",
        "\n",
        "# --- ensure folder structure ---\n",
        "os.makedirs(\"data/bets\", exist_ok=True)\n",
        "\n",
        "# ---- 0) Source table ---------------------------------------------------------\n",
        "if \"df_proj_join_all\" in globals() and isinstance(df_proj_join_all, pd.DataFrame) and not df_proj_join_all.empty:\n",
        "    df = df_proj_join_all.copy()\n",
        "elif \"bets\" in globals() and isinstance(bets, pd.DataFrame) and not bets.empty:\n",
        "    df = bets.copy()\n",
        "elif \"df_proj_join\" in globals() and isinstance(df_proj_join, pd.DataFrame) and not df_proj_join.empty:\n",
        "    df = df_proj_join.copy()\n",
        "else:\n",
        "    raise RuntimeError(\"No joined dataset found (df_proj_join_all/bets/df_proj_join). Run the join cell first.\")\n",
        "\n",
        "# ---- 1) Canonical minimal fields --------------------------------------------\n",
        "def _pick(df_, names):\n",
        "    for n in names:\n",
        "        if n in df_.columns:\n",
        "            return df_[n]\n",
        "    return pd.Series([np.nan]*len(df_))\n",
        "\n",
        "df = df.copy()\n",
        "df[\"player\"]          = _pick(df, [\"player\",\"player_proj\",\"player_odds\"])\n",
        "df[\"team\"]            = _pick(df, [\"team\",\"team_proj\",\"team_odds\"])\n",
        "df[\"opponent\"]        = _pick(df, [\"opponent\",\"opponent_proj\",\"opponent_odds\"])\n",
        "df[\"market\"]          = _pick(df, [\"market\"])\n",
        "df[\"line\"]            = _pick(df, [\"line\",\"posted_line\",\"book_line\"])\n",
        "df[\"book\"]            = _pick(df, [\"book\"])\n",
        "df[\"over_odds\"]       = _pick(df, [\"over_odds\"])\n",
        "df[\"under_odds\"]      = _pick(df, [\"under_odds\"])\n",
        "df[\"projection_mean\"] = _pick(df, [\"projection_mean\",\"expected_line\"])\n",
        "df[\"projection_sd\"]   = _pick(df, [\"projection_sd\"])\n",
        "\n",
        "# ---- 2) Coerce numerics ------------------------------------------------------\n",
        "def _first_float(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    m = re.search(r\"[-+]?\\d+(?:\\.\\d+)?\", str(x))\n",
        "    return float(m.group()) if m else np.nan\n",
        "\n",
        "def _first_int(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    m = re.search(r\"[-+]?\\d+\", str(x))\n",
        "    return float(m.group()) if m else np.nan\n",
        "\n",
        "for col in [\"line\",\"projection_mean\",\"projection_sd\"]:\n",
        "    df[col] = df[col].apply(_first_float)\n",
        "\n",
        "# ---- 3) Model P(Over) with fallback SD --------------------------------------\n",
        "if \"p_over_model\" not in df.columns or df[\"p_over_model\"].isna().all():\n",
        "    sd_missing = (\"projection_sd\" not in df.columns) or df[\"projection_sd\"].fillna(0).eq(0).all()\n",
        "    if sd_missing:\n",
        "        df[\"projection_sd\"] = (df[\"projection_mean\"].abs() * 0.15).clip(lower=1.0)\n",
        "\n",
        "    def p_over_from_normal(mean, sd, line):\n",
        "        if pd.isna(mean) or pd.isna(sd) or pd.isna(line) or float(sd) <= 0:\n",
        "            return np.nan\n",
        "        z = (float(line) - float(mean)) / float(sd)\n",
        "        return 1.0 - NormalDist().cdf(z)\n",
        "\n",
        "    df[\"p_over_model\"] = df.apply(\n",
        "        lambda r: p_over_from_normal(r[\"projection_mean\"], r[\"projection_sd\"], r[\"line\"]),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# ---- 4) Implied / fair probs + edge ------------------------------------------\n",
        "def implied_prob_from_american(american):\n",
        "    o = _first_int(american)\n",
        "    if pd.isna(o): return np.nan\n",
        "    return (-o)/(-o+100.0) if o < 0 else 100.0/(o+100.0)\n",
        "\n",
        "if \"p_over_imp\" not in df.columns:\n",
        "    df[\"p_over_imp\"] = df[\"over_odds\"].apply(implied_prob_from_american)\n",
        "if \"p_under_imp\" not in df.columns:\n",
        "    df[\"p_under_imp\"] = df[\"under_odds\"].apply(implied_prob_from_american)\n",
        "\n",
        "fair = df.apply(\n",
        "    lambda r: pd.Series(\n",
        "        (np.nan, np.nan) if (pd.isna(r[\"p_over_imp\"]) or pd.isna(r[\"p_under_imp\"])) else\n",
        "        (r[\"p_over_imp\"]/(r[\"p_over_imp\"]+r[\"p_under_imp\"]),\n",
        "         r[\"p_under_imp\"]/(r[\"p_over_imp\"]+r[\"p_under_imp\"]))\n",
        "    , index=[\"p_over_fair\",\"p_under_fair\"]), axis=1)\n",
        "df = pd.concat([df, fair], axis=1)\n",
        "\n",
        "df[\"edge_over\"] = np.where(\n",
        "    df[\"p_over_fair\"].notna(),\n",
        "    df[\"p_over_model\"] - df[\"p_over_fair\"],\n",
        "    df[\"p_over_model\"] - df[\"p_over_imp\"]\n",
        ")\n",
        "\n",
        "# Fallback z-score edge\n",
        "nd = NormalDist()\n",
        "df[\"z_score\"] = (df[\"projection_mean\"] - df[\"line\"]) / df[\"projection_sd\"].replace(0, np.nan)\n",
        "df[\"edge_fallback\"] = df[\"z_score\"].map(lambda z: (nd.cdf(z) - 0.5)*2 if pd.notna(z) else np.nan)\n",
        "df[\"edge_rank\"] = np.where(df[\"edge_over\"].notna(), df[\"edge_over\"], df[\"edge_fallback\"])\n",
        "\n",
        "# ---- 5) Split: priced vs all -------------------------------------------------\n",
        "base_cols = [\n",
        "    \"player\",\"team\",\"opponent\",\"market\",\"line\",\"book\",\n",
        "    \"over_odds\",\"under_odds\",\"p_over_imp\",\"p_under_imp\",\n",
        "    \"p_over_fair\",\"p_under_fair\",\n",
        "    \"projection_mean\",\"projection_sd\",\"p_over_model\",\"edge_over\",\"edge_rank\"\n",
        "]\n",
        "lean = df.loc[:, [c for c in base_cols if c in df.columns]].copy()\n",
        "lean = lean.dropna(subset=[\"player\",\"market\",\"line\",\"projection_mean\"], how=\"any\")\n",
        "lean = lean.sort_values([\"player\",\"market\",\"edge_rank\"], ascending=[True, True, False])\n",
        "lean = lean.drop_duplicates(subset=[\"player\",\"market\"], keep=\"first\")\n",
        "\n",
        "priced = lean.dropna(subset=[\"over_odds\",\"under_odds\",\"p_over_imp\",\"p_under_imp\"], how=\"any\").copy()\n",
        "allp   = lean.copy()\n",
        "\n",
        "# Expected value for $100 bet\n",
        "def ev_over_100(p_over, american):\n",
        "    o = _first_int(american)\n",
        "    if pd.isna(p_over) or pd.isna(o): return np.nan\n",
        "    dec = (1 + o/100.0) if o > 0 else (1 + 100.0/abs(o))\n",
        "    return p_over * (dec - 1) * 100 - (1 - p_over) * 100\n",
        "\n",
        "priced[\"EV_over_$100\"] = priced.apply(lambda r: ev_over_100(r[\"p_over_model\"], r[\"over_odds\"]), axis=1)\n",
        "\n",
        "# ---- 6) Write both to Excel --------------------------------------------------\n",
        "out_path = os.path.join(\"data/bets\", f\"value_bets_top100_{datetime.utcnow().strftime('%Y%m%d')}.xlsx\")\n",
        "with pd.ExcelWriter(out_path, engine=\"openpyxl\") as w:\n",
        "    priced.to_excel(w, sheet_name=\"Top100_priced\", index=False)\n",
        "    allp.to_excel(w, sheet_name=\"Top100_all\", index=False)\n",
        "\n",
        "print(f\"âœ… Saved Top-100 value bets to: {out_path}\")\n",
        "print(f\"Rows â†’ priced: {len(priced)} | all: {len(allp)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3662135",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded bets from: data/bets\\value_bets_top100_20251103.xlsx [Top100_all] (331 rows)\n",
            "Using in-memory box_d (182 rows)\n",
            "\n",
            "âœ… Evaluated 85 bets  (WIN=48, LOSS=37, PUSH=0)\n",
            "ðŸŽ¯ Hit rate: 56.5%\n",
            "ðŸ“Š Saved evaluation results to: data/eval\\value_bets_top100_20251103_EVAL.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>player</th>\n",
              "      <th>team</th>\n",
              "      <th>opponent</th>\n",
              "      <th>market</th>\n",
              "      <th>line</th>\n",
              "      <th>book</th>\n",
              "      <th>over_odds</th>\n",
              "      <th>under_odds</th>\n",
              "      <th>p_over_imp</th>\n",
              "      <th>p_under_imp</th>\n",
              "      <th>...</th>\n",
              "      <th>edge_over</th>\n",
              "      <th>edge_rank</th>\n",
              "      <th>player_key</th>\n",
              "      <th>PTS</th>\n",
              "      <th>REB</th>\n",
              "      <th>AST</th>\n",
              "      <th>TEAM_ABBREVIATION</th>\n",
              "      <th>OPPONENT_ABBREVIATION</th>\n",
              "      <th>actual</th>\n",
              "      <th>result_over</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AJ Green</td>\n",
              "      <td>MIL</td>\n",
              "      <td>@IND</td>\n",
              "      <td>AST</td>\n",
              "      <td>1.5</td>\n",
              "      <td>mgm</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.067444</td>\n",
              "      <td>aj green</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AJ Green</td>\n",
              "      <td>MIL</td>\n",
              "      <td>@IND</td>\n",
              "      <td>PTS</td>\n",
              "      <td>8.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.334795</td>\n",
              "      <td>aj green</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AJ Green</td>\n",
              "      <td>MIL</td>\n",
              "      <td>@IND</td>\n",
              "      <td>REB</td>\n",
              "      <td>2.5</td>\n",
              "      <td>caesars</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.511280</td>\n",
              "      <td>aj green</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aaron Gordon</td>\n",
              "      <td>DEN</td>\n",
              "      <td>SAC</td>\n",
              "      <td>AST</td>\n",
              "      <td>2.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.874010</td>\n",
              "      <td>aaron gordon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Aaron Gordon</td>\n",
              "      <td>DEN</td>\n",
              "      <td>SAC</td>\n",
              "      <td>PTS</td>\n",
              "      <td>15.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.201588</td>\n",
              "      <td>aaron gordon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Aaron Gordon</td>\n",
              "      <td>DEN</td>\n",
              "      <td>SAC</td>\n",
              "      <td>REB</td>\n",
              "      <td>4.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.037713</td>\n",
              "      <td>aaron gordon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Aaron Nesmith</td>\n",
              "      <td>IND</td>\n",
              "      <td>MIL</td>\n",
              "      <td>AST</td>\n",
              "      <td>1.5</td>\n",
              "      <td>caesars</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.215278</td>\n",
              "      <td>aaron nesmith</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Aaron Nesmith</td>\n",
              "      <td>IND</td>\n",
              "      <td>MIL</td>\n",
              "      <td>PTS</td>\n",
              "      <td>15.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.984276</td>\n",
              "      <td>aaron nesmith</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Aaron Nesmith</td>\n",
              "      <td>IND</td>\n",
              "      <td>MIL</td>\n",
              "      <td>REB</td>\n",
              "      <td>5.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.616128</td>\n",
              "      <td>aaron nesmith</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Alex Sarr</td>\n",
              "      <td>WAS</td>\n",
              "      <td>@NYK</td>\n",
              "      <td>AST</td>\n",
              "      <td>2.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.824287</td>\n",
              "      <td>alex sarr</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Alex Sarr</td>\n",
              "      <td>WAS</td>\n",
              "      <td>@NYK</td>\n",
              "      <td>PTS</td>\n",
              "      <td>14.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.413597</td>\n",
              "      <td>alex sarr</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Alex Sarr</td>\n",
              "      <td>WAS</td>\n",
              "      <td>@NYK</td>\n",
              "      <td>REB</td>\n",
              "      <td>7.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.868613</td>\n",
              "      <td>alex sarr</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Alperen Sengun</td>\n",
              "      <td>HOU</td>\n",
              "      <td>DAL</td>\n",
              "      <td>AST</td>\n",
              "      <td>6.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.964186</td>\n",
              "      <td>alperen sengun</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Alperen Sengun</td>\n",
              "      <td>HOU</td>\n",
              "      <td>DAL</td>\n",
              "      <td>PTS</td>\n",
              "      <td>20.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.963279</td>\n",
              "      <td>alperen sengun</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Alperen Sengun</td>\n",
              "      <td>HOU</td>\n",
              "      <td>DAL</td>\n",
              "      <td>REB</td>\n",
              "      <td>8.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.657101</td>\n",
              "      <td>alperen sengun</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Amen Thompson</td>\n",
              "      <td>HOU</td>\n",
              "      <td>DAL</td>\n",
              "      <td>AST</td>\n",
              "      <td>5.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.151864</td>\n",
              "      <td>amen thompson</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Amen Thompson</td>\n",
              "      <td>HOU</td>\n",
              "      <td>DAL</td>\n",
              "      <td>PTS</td>\n",
              "      <td>16.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.761024</td>\n",
              "      <td>amen thompson</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Amen Thompson</td>\n",
              "      <td>HOU</td>\n",
              "      <td>DAL</td>\n",
              "      <td>REB</td>\n",
              "      <td>6.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.611039</td>\n",
              "      <td>amen thompson</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Andrew Wiggins</td>\n",
              "      <td>MIA</td>\n",
              "      <td>@LAC</td>\n",
              "      <td>AST</td>\n",
              "      <td>2.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.906959</td>\n",
              "      <td>andrew wiggins</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>MIA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>LOSS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Andrew Wiggins</td>\n",
              "      <td>MIA</td>\n",
              "      <td>@LAC</td>\n",
              "      <td>PTS</td>\n",
              "      <td>16.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.860238</td>\n",
              "      <td>andrew wiggins</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>MIA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>LOSS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Andrew Wiggins</td>\n",
              "      <td>MIA</td>\n",
              "      <td>@LAC</td>\n",
              "      <td>REB</td>\n",
              "      <td>4.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.361446</td>\n",
              "      <td>andrew wiggins</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>MIA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.0</td>\n",
              "      <td>WIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Anfernee Simons</td>\n",
              "      <td>BOS</td>\n",
              "      <td>UTA</td>\n",
              "      <td>AST</td>\n",
              "      <td>2.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.693362</td>\n",
              "      <td>anfernee simons</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Anfernee Simons</td>\n",
              "      <td>BOS</td>\n",
              "      <td>UTA</td>\n",
              "      <td>PTS</td>\n",
              "      <td>15.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.481835</td>\n",
              "      <td>anfernee simons</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Anfernee Simons</td>\n",
              "      <td>BOS</td>\n",
              "      <td>UTA</td>\n",
              "      <td>REB</td>\n",
              "      <td>2.5</td>\n",
              "      <td>hardrock</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.386654</td>\n",
              "      <td>anfernee simons</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Ausar Thompson</td>\n",
              "      <td>DET</td>\n",
              "      <td>@MEM</td>\n",
              "      <td>AST</td>\n",
              "      <td>3.5</td>\n",
              "      <td>betrivers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.258779</td>\n",
              "      <td>ausar thompson</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25 rows Ã— 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             player team opponent market  line       book  over_odds  \\\n",
              "0          AJ Green  MIL     @IND    AST   1.5        mgm        NaN   \n",
              "1          AJ Green  MIL     @IND    PTS   8.5  betrivers        NaN   \n",
              "2          AJ Green  MIL     @IND    REB   2.5    caesars        NaN   \n",
              "3      Aaron Gordon  DEN      SAC    AST   2.5  betrivers        NaN   \n",
              "4      Aaron Gordon  DEN      SAC    PTS  15.5  betrivers        NaN   \n",
              "5      Aaron Gordon  DEN      SAC    REB   4.5  betrivers        NaN   \n",
              "6     Aaron Nesmith  IND      MIL    AST   1.5    caesars        NaN   \n",
              "7     Aaron Nesmith  IND      MIL    PTS  15.5  betrivers        NaN   \n",
              "8     Aaron Nesmith  IND      MIL    REB   5.5  betrivers        NaN   \n",
              "9         Alex Sarr  WAS     @NYK    AST   2.5  betrivers        NaN   \n",
              "10        Alex Sarr  WAS     @NYK    PTS  14.5  betrivers        NaN   \n",
              "11        Alex Sarr  WAS     @NYK    REB   7.5  betrivers        NaN   \n",
              "12   Alperen Sengun  HOU      DAL    AST   6.5  betrivers        NaN   \n",
              "13   Alperen Sengun  HOU      DAL    PTS  20.5  betrivers        NaN   \n",
              "14   Alperen Sengun  HOU      DAL    REB   8.5  betrivers        NaN   \n",
              "15    Amen Thompson  HOU      DAL    AST   5.5  betrivers        NaN   \n",
              "16    Amen Thompson  HOU      DAL    PTS  16.5  betrivers        NaN   \n",
              "17    Amen Thompson  HOU      DAL    REB   6.5  betrivers        NaN   \n",
              "18   Andrew Wiggins  MIA     @LAC    AST   2.5  betrivers        NaN   \n",
              "19   Andrew Wiggins  MIA     @LAC    PTS  16.5  betrivers        NaN   \n",
              "20   Andrew Wiggins  MIA     @LAC    REB   4.5  betrivers        NaN   \n",
              "21  Anfernee Simons  BOS      UTA    AST   2.5  betrivers        NaN   \n",
              "22  Anfernee Simons  BOS      UTA    PTS  15.5  betrivers        NaN   \n",
              "23  Anfernee Simons  BOS      UTA    REB   2.5   hardrock        NaN   \n",
              "24   Ausar Thompson  DET     @MEM    AST   3.5  betrivers        NaN   \n",
              "\n",
              "    under_odds  p_over_imp  p_under_imp  ...  edge_over  edge_rank  \\\n",
              "0          NaN         NaN          NaN  ...        NaN   0.067444   \n",
              "1          NaN         NaN          NaN  ...        NaN   0.334795   \n",
              "2          NaN         NaN          NaN  ...        NaN   0.511280   \n",
              "3          NaN         NaN          NaN  ...        NaN   0.874010   \n",
              "4          NaN         NaN          NaN  ...        NaN   0.201588   \n",
              "5          NaN         NaN          NaN  ...        NaN   0.037713   \n",
              "6          NaN         NaN          NaN  ...        NaN  -0.215278   \n",
              "7          NaN         NaN          NaN  ...        NaN  -0.984276   \n",
              "8          NaN         NaN          NaN  ...        NaN  -0.616128   \n",
              "9          NaN         NaN          NaN  ...        NaN   0.824287   \n",
              "10         NaN         NaN          NaN  ...        NaN   0.413597   \n",
              "11         NaN         NaN          NaN  ...        NaN  -0.868613   \n",
              "12         NaN         NaN          NaN  ...        NaN  -0.964186   \n",
              "13         NaN         NaN          NaN  ...        NaN  -0.963279   \n",
              "14         NaN         NaN          NaN  ...        NaN  -0.657101   \n",
              "15         NaN         NaN          NaN  ...        NaN  -0.151864   \n",
              "16         NaN         NaN          NaN  ...        NaN  -0.761024   \n",
              "17         NaN         NaN          NaN  ...        NaN   0.611039   \n",
              "18         NaN         NaN          NaN  ...        NaN   0.906959   \n",
              "19         NaN         NaN          NaN  ...        NaN   0.860238   \n",
              "20         NaN         NaN          NaN  ...        NaN  -0.361446   \n",
              "21         NaN         NaN          NaN  ...        NaN   0.693362   \n",
              "22         NaN         NaN          NaN  ...        NaN   0.481835   \n",
              "23         NaN         NaN          NaN  ...        NaN   0.386654   \n",
              "24         NaN         NaN          NaN  ...        NaN  -0.258779   \n",
              "\n",
              "         player_key   PTS  REB  AST  TEAM_ABBREVIATION OPPONENT_ABBREVIATION  \\\n",
              "0          aj green   NaN  NaN  NaN                NaN                   NaN   \n",
              "1          aj green   NaN  NaN  NaN                NaN                   NaN   \n",
              "2          aj green   NaN  NaN  NaN                NaN                   NaN   \n",
              "3      aaron gordon   NaN  NaN  NaN                NaN                   NaN   \n",
              "4      aaron gordon   NaN  NaN  NaN                NaN                   NaN   \n",
              "5      aaron gordon   NaN  NaN  NaN                NaN                   NaN   \n",
              "6     aaron nesmith   NaN  NaN  NaN                NaN                   NaN   \n",
              "7     aaron nesmith   NaN  NaN  NaN                NaN                   NaN   \n",
              "8     aaron nesmith   NaN  NaN  NaN                NaN                   NaN   \n",
              "9         alex sarr   NaN  NaN  NaN                NaN                   NaN   \n",
              "10        alex sarr   NaN  NaN  NaN                NaN                   NaN   \n",
              "11        alex sarr   NaN  NaN  NaN                NaN                   NaN   \n",
              "12   alperen sengun   NaN  NaN  NaN                NaN                   NaN   \n",
              "13   alperen sengun   NaN  NaN  NaN                NaN                   NaN   \n",
              "14   alperen sengun   NaN  NaN  NaN                NaN                   NaN   \n",
              "15    amen thompson   NaN  NaN  NaN                NaN                   NaN   \n",
              "16    amen thompson   NaN  NaN  NaN                NaN                   NaN   \n",
              "17    amen thompson   NaN  NaN  NaN                NaN                   NaN   \n",
              "18   andrew wiggins  15.0  9.0  1.0                MIA                   NaN   \n",
              "19   andrew wiggins  15.0  9.0  1.0                MIA                   NaN   \n",
              "20   andrew wiggins  15.0  9.0  1.0                MIA                   NaN   \n",
              "21  anfernee simons   NaN  NaN  NaN                NaN                   NaN   \n",
              "22  anfernee simons   NaN  NaN  NaN                NaN                   NaN   \n",
              "23  anfernee simons   NaN  NaN  NaN                NaN                   NaN   \n",
              "24   ausar thompson   NaN  NaN  NaN                NaN                   NaN   \n",
              "\n",
              "    actual  result_over  \n",
              "0      NaN           NA  \n",
              "1      NaN           NA  \n",
              "2      NaN           NA  \n",
              "3      NaN           NA  \n",
              "4      NaN           NA  \n",
              "5      NaN           NA  \n",
              "6      NaN           NA  \n",
              "7      NaN           NA  \n",
              "8      NaN           NA  \n",
              "9      NaN           NA  \n",
              "10     NaN           NA  \n",
              "11     NaN           NA  \n",
              "12     NaN           NA  \n",
              "13     NaN           NA  \n",
              "14     NaN           NA  \n",
              "15     NaN           NA  \n",
              "16     NaN           NA  \n",
              "17     NaN           NA  \n",
              "18     1.0         LOSS  \n",
              "19    15.0         LOSS  \n",
              "20     9.0          WIN  \n",
              "21     NaN           NA  \n",
              "22     NaN           NA  \n",
              "23     NaN           NA  \n",
              "24     NaN           NA  \n",
              "\n",
              "[25 rows x 25 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === EVALUATE YESTERDAY'S BETS (Europe/Athens) â†’ saves to /data/eval ===\n",
        "import os, re, numpy as np, pandas as pd\n",
        "from glob import glob\n",
        "from datetime import datetime, timedelta\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "# ------------------ settings: always \"day before\" in Europe/Athens ------------------\n",
        "TZ = ZoneInfo(\"Europe/Athens\")\n",
        "today_local = datetime.now(TZ).date()\n",
        "ydate = today_local - timedelta(days=1)                 # <-- Yesterday (local)\n",
        "ystr = ydate.strftime(\"%Y%m%d\")\n",
        "print(f\"Evaluating bets for YESTERDAY (Europe/Athens): {ydate} ({ystr})\")\n",
        "\n",
        "os.makedirs(\"data/eval\", exist_ok=True)\n",
        "\n",
        "# ------------------ helpers ------------------\n",
        "def _norm_player(s):\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    return re.sub(r\"[.`'â€™\\-]\", \"\", s.strip()).lower()\n",
        "\n",
        "def pick_col(df, candidates, default=np.nan):\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return df[c]\n",
        "    return pd.Series([default]*len(df))\n",
        "\n",
        "def _first_float(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    m = re.search(r\"[-+]?\\d+(?:\\.\\d+)?\", str(x))\n",
        "    return float(m.group()) if m else np.nan\n",
        "\n",
        "def infer_opponent(df):\n",
        "    if \"OPPONENT_ABBREVIATION\" in df.columns:\n",
        "        return df[\"OPPONENT_ABBREVIATION\"]\n",
        "    matchup = pick_col(df, [\"MATCHUP\",\"Matchup\"])\n",
        "    team = pick_col(df, [\"TEAM_ABBREVIATION\",\"TEAM\"])\n",
        "    out = []\n",
        "    for t, m in zip(team.fillna(\"\"), matchup.fillna(\"\")):\n",
        "        opp = np.nan\n",
        "        if isinstance(m, str) and m:\n",
        "            parts = re.split(r\"[@vVsS]+\\.*\", m)\n",
        "            if len(parts) >= 2:\n",
        "                cand = parts[-1].strip().upper()\n",
        "                if cand == str(t).upper() and len(parts) >= 2:\n",
        "                    cand = parts[0].strip().upper()\n",
        "                opp = cand\n",
        "        out.append(opp)\n",
        "    return pd.Series(out, index=df.index)\n",
        "\n",
        "def _parse_date_from_filename(path, pattern):\n",
        "    m = re.search(pattern, os.path.basename(path))\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "# ------------------ 1) Pick yesterday's bets workbook ------------------\n",
        "bet_files = sorted(glob(\"data/bets/value_bets_top100_*.xlsx\"))\n",
        "if not bet_files:\n",
        "    raise FileNotFoundError(\"No bets files found in data/bets/. Run the Top-100 export first.\")\n",
        "\n",
        "# Prefer exact yesterday; if missing, use nearest earlier\n",
        "dated = []\n",
        "for p in bet_files:\n",
        "    ds = _parse_date_from_filename(p, r\"value_bets_top100_(\\d{8})\\.xlsx\")\n",
        "    if ds:\n",
        "        dated.append((ds, p))\n",
        "dated = sorted(dated, key=lambda x: x[0])\n",
        "\n",
        "bets_path = None\n",
        "for ds, p in reversed(dated):\n",
        "    if ds <= ystr:\n",
        "        bets_path = p\n",
        "        break\n",
        "if bets_path is None:\n",
        "    # fall back to earliest (shouldn't usually happen)\n",
        "    bets_path = dated[0][1]\n",
        "    print(\"âš ï¸ No bets file on/before yesterday; using earliest available:\", os.path.basename(bets_path))\n",
        "else:\n",
        "    print(\"Using bets workbook:\", os.path.basename(bets_path))\n",
        "\n",
        "# ------------------ 2) Load a non-empty Top100 sheet ------------------\n",
        "with pd.ExcelFile(bets_path) as xf:\n",
        "    sheet = None\n",
        "    for s in [\"Top100_priced\",\"Top100_all\",\"Top100\"]:\n",
        "        if s in xf.sheet_names:\n",
        "            tmp = pd.read_excel(bets_path, sheet_name=s)\n",
        "            if not tmp.empty:\n",
        "                sheet, bets = s, tmp\n",
        "                break\n",
        "if sheet is None:\n",
        "    raise RuntimeError(\"All Top100 sheets empty in bets workbook.\")\n",
        "\n",
        "print(f\"Loaded bets: sheet [{sheet}], rows={len(bets)}\")\n",
        "\n",
        "# ------------------ 3) Load boxscores for YESTERDAY ------------------\n",
        "# Priority: in-memory `box_d` filtered to ydate â†’ file nba_boxscores_YYYYMMDD.csv â†’ latest fallback\n",
        "def _as_date(s):\n",
        "    try:\n",
        "        return pd.to_datetime(s).date()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "box = None\n",
        "if \"box_d\" in globals() and isinstance(box_d, pd.DataFrame) and not box_d.empty:\n",
        "    bd = box_d.copy()\n",
        "    # Try to locate a date column and filter to yesterday\n",
        "    date_col = None\n",
        "    for c in [\"GAME_DATE\", \"GAME_DATE_EST\", \"GAME_DATE_LCL\", \"Date\", \"date\"]:\n",
        "        if c in bd.columns:\n",
        "            date_col = c\n",
        "            break\n",
        "    if date_col is not None:\n",
        "        bd[\"_gdate\"] = bd[date_col].apply(_as_date)\n",
        "        box = bd.loc[bd[\"_gdate\"].eq(ydate)].copy()\n",
        "        print(f\"box_d in memory â†’ filtered rows for {ydate}: {len(box)}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ box_d found but no date column to filter; using all rows.\")\n",
        "        box = bd.copy()\n",
        "\n",
        "if box is None or box.empty:\n",
        "    # Try file with exact date\n",
        "    exact_csv = f\"nba_boxscores_{ystr}.csv\"\n",
        "    if os.path.exists(exact_csv):\n",
        "        box = pd.read_csv(exact_csv)\n",
        "        print(f\"Loaded boxscores from file: {exact_csv} ({len(box)} rows)\")\n",
        "    else:\n",
        "        # pick latest available matching pattern\n",
        "        csv_files = sorted(glob(\"nba_boxscores_*.csv\"))\n",
        "        if csv_files:\n",
        "            box = pd.read_csv(csv_files[-1])\n",
        "            print(f\"âš ï¸ No boxscore file for {ystr}; using latest: {os.path.basename(csv_files[-1])} ({len(box)} rows)\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"No boxscore CSV found (nba_boxscores_YYYYMMDD.csv).\")\n",
        "\n",
        "# ------------------ 4) Normalize & (optionally) filter box by yesterday ------------------\n",
        "# Try filter again if a date column exists (helps when we loaded a combined CSV)\n",
        "for c in [\"GAME_DATE\", \"GAME_DATE_EST\", \"GAME_DATE_LCL\", \"Date\", \"date\"]:\n",
        "    if c in box.columns:\n",
        "        _dates = pd.to_datetime(box[c], errors=\"coerce\").dt.date\n",
        "        if _dates.notna().any():\n",
        "            box = box.loc[_dates.eq(ydate)].copy()\n",
        "            print(f\"Filtered boxscores to {ydate} by column '{c}': {len(box)} rows\")\n",
        "        break\n",
        "\n",
        "# ------------------ 5) Prepare bets & box for join ------------------\n",
        "bets = bets.copy()\n",
        "bets[\"player\"] = pick_col(bets, [\"player\",\"Player\"])\n",
        "bets[\"market\"] = pick_col(bets, [\"market\",\"Market\"])\n",
        "bets[\"line\"]   = pd.to_numeric(pick_col(bets, [\"posted_line\",\"line\"]), errors=\"coerce\")\n",
        "bets[\"player_key\"] = bets[\"player\"].map(_norm_player)\n",
        "\n",
        "box = box.copy()\n",
        "box[\"player\"] = pick_col(box, [\"PLAYER_NAME\",\"Player\"])\n",
        "box[\"player_key\"] = box[\"player\"].map(_norm_player)\n",
        "box[\"PTS\"] = pd.to_numeric(pick_col(box, [\"PTS\",\"Points\"]), errors=\"coerce\")\n",
        "box[\"REB\"] = pd.to_numeric(pick_col(box, [\"REB\",\"Rebounds\"]), errors=\"coerce\")\n",
        "box[\"AST\"] = pd.to_numeric(pick_col(box, [\"AST\",\"Assists\"]), errors=\"coerce\")\n",
        "box[\"TEAM_ABBREVIATION\"] = pick_col(box, [\"TEAM_ABBREVIATION\",\"TEAM\"])\n",
        "box[\"OPPONENT_ABBREVIATION\"] = infer_opponent(box)\n",
        "\n",
        "# ------------------ 6) Join & grade ------------------\n",
        "joined = bets.merge(\n",
        "    box[[\"player_key\",\"PTS\",\"REB\",\"AST\",\"TEAM_ABBREVIATION\",\"OPPONENT_ABBREVIATION\"]],\n",
        "    on=\"player_key\", how=\"left\", suffixes=(\"\",\"_box\")\n",
        ")\n",
        "\n",
        "def pick_actual(row):\n",
        "    m = str(row.get(\"market\",\"\")).upper()\n",
        "    return row.get(m, np.nan) if m in [\"PTS\",\"REB\",\"AST\"] else np.nan\n",
        "\n",
        "joined[\"actual\"] = joined.apply(pick_actual, axis=1)\n",
        "joined[\"result_over\"] = np.where(\n",
        "    joined[\"actual\"].notna() & joined[\"line\"].notna(),\n",
        "    np.where(joined[\"actual\"] > joined[\"line\"], \"WIN\",\n",
        "    np.where(joined[\"actual\"] == joined[\"line\"], \"PUSH\", \"LOSS\")),\n",
        "    \"NA\"\n",
        ")\n",
        "\n",
        "# ------------------ 7) Summary ------------------\n",
        "graded = joined[\"result_over\"].isin([\"WIN\",\"LOSS\",\"PUSH\"]).sum()\n",
        "wins = (joined[\"result_over\"]==\"WIN\").sum()\n",
        "losses = (joined[\"result_over\"]==\"LOSS\").sum()\n",
        "pushes = (joined[\"result_over\"]==\"PUSH\").sum()\n",
        "hitrate = wins / max(wins+losses, 1)\n",
        "\n",
        "print(f\"\\nâœ… Evaluated {graded} bets for {ydate}  (WIN={wins}, LOSS={losses}, PUSH={pushes})\")\n",
        "print(f\"ðŸŽ¯ Hit rate: {hitrate:.1%}\")\n",
        "\n",
        "# ------------------ 8) Save evaluation ------------------\n",
        "eval_out = os.path.join(\"data/eval\", f\"value_bets_top100_{ystr}_EVAL.csv\")\n",
        "joined.to_csv(eval_out, index=False)\n",
        "print(f\"ðŸ“Š Saved evaluation to: {eval_out}\")\n",
        "\n",
        "display(joined.head(25))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c68841f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
