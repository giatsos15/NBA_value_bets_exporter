{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b09a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--cell 1--#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For data analysis\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e17b0961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching NBA betting data and lineups...\n",
      "Successfully fetched RAW odds rows: 1850 | columns: 263\n",
      "Successfully fetched lineups for 0 games\n",
      "Data successfully saved to nba_betting_data_20251104_002811.xlsx\n",
      "\n",
      "==================================================\n",
      "NBA BETTING DATA SUMMARY\n",
      "==================================================\n",
      "\n",
      "Betting Lines: 0 player-stat combinations\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#--cell 2--#\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class NBAOddsAndLineupsScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.setup_headers()\n",
    "    \n",
    "    def setup_headers(self):\n",
    "        \"\"\"Setup common headers for requests\"\"\"\n",
    "        self.headers = {\n",
    "            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "            'accept-language': 'en-US,en;q=0.9',\n",
    "            'cache-control': 'max-age=0',\n",
    "            'priority': 'u=0, i',\n",
    "            'referer': 'https://www.rotowire.com/',\n",
    "            'sec-ch-ua': '\"Chromium\";v=\"130\", \"Google Chrome\";v=\"130\", \"Not?A_Brand\";v=\"99\"',\n",
    "            'sec-ch-ua-mobile': '?0',\n",
    "            'sec-ch-ua-platform': '\"Windows\"',\n",
    "            'sec-fetch-dest': 'document',\n",
    "            'sec-fetch-mode': 'navigate',\n",
    "            'sec-fetch-site': 'same-origin',\n",
    "            'sec-fetch-user': '?1',\n",
    "            'upgrade-insecure-requests': '1',\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36'\n",
    "        }\n",
    "\n",
    "    # --------- RAW WIDE ODDS (no aggregation) ---------------------------------\n",
    "    def get_player_props_odds_wide_raw(self, book='mgm'):\n",
    "        \"\"\"\n",
    "        Return the raw 'wide' odds table by scraping Rotowire's player-props page.\n",
    "        This preserves columns like:\n",
    "          name, team, opp, <book>_pts, <book>_ptsUnder, <book>_ptsOver, ...\n",
    "        Works across many books present in the page JSON blocks.\n",
    "        \"\"\"\n",
    "        url = f\"https://www.rotowire.com/betting/nba/player-props.php?book={book}\"\n",
    "        try:\n",
    "            r = self.session.get(url, headers=self.headers)\n",
    "            r.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to GET odds page: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Extract ALL JSON lists assigned to \"data:\" in the page\n",
    "        matches = re.findall(r\"data:\\s*(\\[\\{.*?\\}\\])\", r.text, flags=re.DOTALL)\n",
    "        frames = []\n",
    "        for m in matches:\n",
    "            try:\n",
    "                rows = json.loads(m)\n",
    "                if isinstance(rows, list) and rows:\n",
    "                    frames.append(pd.DataFrame(rows))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not frames:\n",
    "            print(\"No odds JSON blocks found.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.concat(frames, ignore_index=True)\n",
    "        # keep the most useful id/basic columns if present\n",
    "        base_cols = [c for c in [\"name\",\"gameID\",\"playerID\",\"firstName\",\"lastName\",\"team\",\"opp\",\"logo\",\"playerLink\"] if c in df.columns]\n",
    "        other_cols = [c for c in df.columns if c not in base_cols]\n",
    "        df = df[base_cols + other_cols]\n",
    "        # normalize team/opponent field names\n",
    "        if \"opp\" in df.columns and \"opponent\" not in df.columns:\n",
    "            df = df.rename(columns={\"opp\": \"opponent\"})\n",
    "        # add as-of date and (best-guess) game_date if not present\n",
    "        df[\"asof_date\"] = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "        if \"game_date\" not in df.columns:\n",
    "            df[\"game_date\"] = df[\"asof_date\"]\n",
    "        print(f\"Successfully fetched RAW odds rows: {len(df)} | columns: {len(df.columns)}\")\n",
    "        return df\n",
    "\n",
    "    # --------- Legacy aggregated method (kept in case you still call it) ------\n",
    "    def get_player_props_odds(self, book='mgm'):\n",
    "        \"\"\"\n",
    "        Old helper that aggregated rows by 'name'.\n",
    "        Prefer get_player_props_odds_wide_raw() for modeling/joins.\n",
    "        \"\"\"\n",
    "        wide = self.get_player_props_odds_wide_raw(book=book)\n",
    "        if wide.empty:\n",
    "            return None\n",
    "        aggregated_df = wide.groupby('name', as_index=False, sort=False).agg(\n",
    "            lambda x: ', '.join(pd.Series(x).dropna().astype(str).unique())\n",
    "        )\n",
    "        aggregated_df = aggregated_df.dropna(axis=1, how='all')\n",
    "        print(f\"Successfully aggregated odds for {len(aggregated_df)} players\")\n",
    "        return aggregated_df\n",
    "\n",
    "    # --------- Lineups scraping (unchanged logic, made a bit sturdier) --------\n",
    "    def get_expected_lineups(self):\n",
    "        \"\"\"Get expected lineups from Rotowire\"\"\"\n",
    "        url = \"https://www.rotowire.com/basketball/nba-lineups.php\"\n",
    "        try:\n",
    "            r = self.session.get(url, headers=self.headers)\n",
    "            r.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to retrieve lineup page: {e}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        game_containers = soup.find_all('div', class_='lineup__main')\n",
    "        if not game_containers:\n",
    "            print(\"No lineup data found\")\n",
    "            return None\n",
    "\n",
    "        lineups_data = []\n",
    "        for game in game_containers:\n",
    "            game_info = self._parse_game_info(game)\n",
    "            if game_info:\n",
    "                lineups_data.append(game_info)\n",
    "\n",
    "        print(f\"Successfully fetched lineups for {len(lineups_data)} games\")\n",
    "        return lineups_data\n",
    "\n",
    "    def _parse_game_info(self, game_container):\n",
    "        \"\"\"Parse individual game information and lineups\"\"\"\n",
    "        try:\n",
    "            game_data = {}\n",
    "            header = game_container.find('div', class_='lineup__hdr')\n",
    "            if header:\n",
    "                teams = header.find_all('div', class_='lineup__team')\n",
    "                if len(teams) >= 2:\n",
    "                    game_data['away_team'] = teams[0].get_text(strip=True)\n",
    "                    game_data['home_team'] = teams[1].get_text(strip=True)\n",
    "            time_info = header.find('div', class_='lineup__time') if header else None\n",
    "            if time_info:\n",
    "                game_data['game_time'] = time_info.get_text(strip=True)\n",
    "            lineup_containers = game_container.find_all('div', class_='lineup__box')\n",
    "            if len(lineup_containers) >= 2:\n",
    "                game_data['away_starters'] = self._parse_team_lineup(lineup_containers[0])\n",
    "                game_data['home_starters'] = self._parse_team_lineup(lineup_containers[1])\n",
    "            return game_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing game info: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _parse_team_lineup(self, team_container):\n",
    "        \"\"\"Parse individual team lineup\"\"\"\n",
    "        starters = []\n",
    "        try:\n",
    "            starters_section = team_container.find('div', class_='lineup__list')\n",
    "            if starters_section:\n",
    "                player_elements = starters_section.find_all('div', class_='lineup__player')\n",
    "                for player_elem in player_elements:\n",
    "                    player_info = self._parse_player_info(player_elem)\n",
    "                    if player_info:\n",
    "                        starters.append(player_info)\n",
    "            return starters\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing team lineup: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _parse_player_info(self, player_elem):\n",
    "        \"\"\"Parse individual player information\"\"\"\n",
    "        try:\n",
    "            player_data = {}\n",
    "            name_elem = player_elem.find('a', class_='lineup__player-link')\n",
    "            if name_elem:\n",
    "                player_data['name'] = name_elem.get_text(strip=True)\n",
    "                player_data['player_link'] = name_elem.get('href', '')\n",
    "            pos_elem = player_elem.find('span', class_='lineup__pos')\n",
    "            if pos_elem:\n",
    "                player_data['position'] = pos_elem.get_text(strip=True)\n",
    "            injury_elem = player_elem.find('span', class_='lineup__inj')\n",
    "            player_data['injury_status'] = injury_elem.get_text(strip=True) if injury_elem else 'Active'\n",
    "            confirmed_elem = player_elem.find('span', class_='lineup__confirm')\n",
    "            player_data['confirmed_starter'] = confirmed_elem is not None\n",
    "            return player_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing player info: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_comprehensive_data(self):\n",
    "        \"\"\"Get both odds and lineups data\"\"\"\n",
    "        print(\"Fetching NBA betting data and lineups...\")\n",
    "        odds_data = self.get_player_props_odds_wide_raw()  # <-- use RAW wide\n",
    "        lineups_data = self.get_expected_lineups()\n",
    "        combined_data = {\n",
    "            'odds': odds_data,\n",
    "            'lineups': lineups_data,\n",
    "            'last_updated': datetime.now().isoformat()\n",
    "        }\n",
    "        return combined_data\n",
    "    \n",
    "    def save_to_excel(self, data, filename=None):\n",
    "        \"\"\"Save the scraped data to Excel files\"\"\"\n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f'nba_betting_data_{timestamp}.xlsx'\n",
    "        try:\n",
    "            with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "                if isinstance(data.get('odds'), pd.DataFrame) and not data['odds'].empty:\n",
    "                    data['odds'].to_excel(writer, sheet_name='Player_Odds', index=False)\n",
    "                if data.get('lineups') is not None:\n",
    "                    lineups_list = []\n",
    "                    for game in data['lineups']:\n",
    "                        for starter_type in ['away_starters', 'home_starters']:\n",
    "                            team = game.get('away_team' if starter_type == 'away_starters' else 'home_team', 'Unknown')\n",
    "                            starters = game.get(starter_type, [])\n",
    "                            for starter in starters:\n",
    "                                lineups_list.append({\n",
    "                                    'game_time': game.get('game_time', ''),\n",
    "                                    'team': team,\n",
    "                                    'player_name': starter.get('name', ''),\n",
    "                                    'position': starter.get('position', ''),\n",
    "                                    'injury_status': starter.get('injury_status', ''),\n",
    "                                    'confirmed_starter': starter.get('confirmed_starter', False),\n",
    "                                    'player_link': starter.get('player_link', '')\n",
    "                                })\n",
    "                    if lineups_list:\n",
    "                        lineups_df = pd.DataFrame(lineups_list)\n",
    "                        lineups_df.to_excel(writer, sheet_name='Expected_Lineups', index=False)\n",
    "                metadata = pd.DataFrame([{\n",
    "                    'last_updated': data.get('last_updated', ''),\n",
    "                    'total_games': len(data.get('lineups', [])) if isinstance(data.get('lineups'), list) else 0,\n",
    "                    'total_players_odds': len(data.get('odds', [])) if isinstance(data.get('odds'), pd.DataFrame) else 0\n",
    "                }])\n",
    "                metadata.to_excel(writer, sheet_name='Metadata', index=False)\n",
    "            print(f\"Data successfully saved to {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to Excel: {e}\")\n",
    "            return False\n",
    "\n",
    "# Usage example and integration with your existing analytics\n",
    "def integrate_with_analytics():\n",
    "    \"\"\"Integrate the scraper with your existing analytics\"\"\"\n",
    "    scraper = NBAOddsAndLineupsScraper()\n",
    "    nba_data = scraper.get_comprehensive_data()\n",
    "    scraper.save_to_excel(nba_data)\n",
    "    processed_data = process_for_analytics(nba_data)\n",
    "    return processed_data\n",
    "\n",
    "def process_for_analytics(nba_data):\n",
    "    \"\"\"Process the scraped data for use in analytics\"\"\"\n",
    "    processed = {}\n",
    "    # Odds data ‚Üí extract basic lines for PTS/REB/AST if present\n",
    "    if isinstance(nba_data.get('odds'), pd.DataFrame) and not nba_data['odds'].empty:\n",
    "        odds_df = nba_data['odds']\n",
    "        def pick_line(row, market):\n",
    "            # Look for any <book>_<marketLower> columns (line, Under, Over)\n",
    "            m = market.lower()\n",
    "            line = None\n",
    "            over = None\n",
    "            under = None\n",
    "            for col in row.index:\n",
    "                c = col.lower()\n",
    "                if c.endswith(f\"_{m}\"):\n",
    "                    line = row[col]\n",
    "                elif c.endswith(f\"_{m}over\"):\n",
    "                    over = row[col]\n",
    "                elif c.endswith(f\"_{m}under\"):\n",
    "                    under = row[col]\n",
    "            try:\n",
    "                line = float(line) if line is not None and str(line).strip() not in (\"\", \"None\", \"nan\") else None\n",
    "            except Exception:\n",
    "                line = None\n",
    "            return line, over, under\n",
    "\n",
    "        betting_lines = []\n",
    "        for _, r in odds_df.iterrows():\n",
    "            player_name = r.get('name', '')\n",
    "            for mk in [\"pts\", \"reb\", \"ast\"]:\n",
    "                line, over, under = pick_line(r, mk)\n",
    "                if line is not None:\n",
    "                    betting_lines.append({\n",
    "                        \"player\": player_name,\n",
    "                        \"stat\": {\"pts\":\"points\",\"reb\":\"rebounds\",\"ast\":\"assists\"}[mk],\n",
    "                        \"line\": line,\n",
    "                        \"over_odds\": over,\n",
    "                        \"under_odds\": under\n",
    "                    })\n",
    "        processed['betting_lines'] = pd.DataFrame(betting_lines)\n",
    "\n",
    "    # Lineups\n",
    "    if nba_data.get('lineups') is not None:\n",
    "        lineups = nba_data['lineups']\n",
    "        team_players = {}\n",
    "        for game in lineups:\n",
    "            away_team = game.get('away_team')\n",
    "            if away_team and away_team not in team_players:\n",
    "                team_players[away_team] = []\n",
    "            for starter in game.get('away_starters', []):\n",
    "                if away_team and starter.get('name'):\n",
    "                    team_players[away_team].append({\n",
    "                        'name': starter['name'],\n",
    "                        'position': starter.get('position', ''),\n",
    "                        'status': starter.get('injury_status', 'Active'),\n",
    "                        'confirmed': starter.get('confirmed_starter', False)\n",
    "                    })\n",
    "            home_team = game.get('home_team')\n",
    "            if home_team and home_team not in team_players:\n",
    "                team_players[home_team] = []\n",
    "            for starter in game.get('home_starters', []):\n",
    "                if home_team and starter.get('name'):\n",
    "                    team_players[home_team].append({\n",
    "                        'name': starter['name'],\n",
    "                        'position': starter.get('position', ''),\n",
    "                        'status': starter.get('injury_status', 'Active'),\n",
    "                        'confirmed': starter.get('confirmed_starter', False)\n",
    "                    })\n",
    "        processed['team_lineups'] = team_players\n",
    "        processed['games_today'] = lineups\n",
    "    return processed\n",
    "\n",
    "def extract_betting_line(player_row, stat_type):\n",
    "    \"\"\"Extract betting line for specific stat type (legacy helper)\"\"\"\n",
    "    line_col = over_odds_col = under_odds_col = None\n",
    "    for col in player_row.index:\n",
    "        col_lower = col.lower()\n",
    "        if stat_type in col_lower and 'line' in col_lower:\n",
    "            line_col = col\n",
    "        elif stat_type in col_lower and 'over' in col_lower and 'odds' in col_lower:\n",
    "            over_odds_col = col\n",
    "        elif stat_type in col_lower and 'under' in col_lower and 'odds' in col_lower:\n",
    "            under_odds_col = col\n",
    "    line_value = player_row.get(line_col) if line_col else None\n",
    "    if line_value and str(line_value).replace('.', '').isdigit():\n",
    "        return {\n",
    "            'line': float(line_value),\n",
    "            'over_odds': player_row.get(over_odds_col) if over_odds_col else None,\n",
    "            'under_odds': player_row.get(under_odds_col) if under_odds_col else None\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    data = integrate_with_analytics()\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"NBA BETTING DATA SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    if data.get('betting_lines') is not None:\n",
    "        print(f\"\\nBetting Lines: {len(data['betting_lines'])} player-stat combinations\")\n",
    "        print(data['betting_lines'].head(10))\n",
    "    if data.get('team_lineups'):\n",
    "        print(f\"\\nTeams with Lineups: {len(data['team_lineups'])}\")\n",
    "        for team, players in list(data['team_lineups'].items())[:3]:\n",
    "            print(f\"{team}: {len(players)} players\")\n",
    "            for player in players[:3]:\n",
    "                print(f\"  - {player['name']} ({player['position']}) - {player['status']}\")\n",
    "    if data.get('games_today'):\n",
    "        print(f\"\\nGames Today: {len(data['games_today'])}\")\n",
    "        for game in data['games_today'][:3]:\n",
    "            print(f\"{game.get('away_team', 'TBD')} @ {game.get('home_team', 'TBD')} - {game.get('game_time', 'Time TBD')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60652ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--cell 3--#\n",
    "def get_daily_matchups(date=None):\n",
    "    \"\"\"Get NBA games for a specific date\"\"\"\n",
    "    if date is None:\n",
    "        date = datetime.now().strftime('%Y-%m-%d')\n",
    "    # Placeholder demo; replace with a real schedule API if desired\n",
    "    sample_matchups = [\n",
    "        {'home_team': 'GSW', 'away_team': 'LAL', 'time': '7:30 PM ET'},\n",
    "        {'home_team': 'BOS', 'away_team': 'MIA', 'time': '8:00 PM ET'},\n",
    "        {'home_team': 'DEN', 'away_team': 'DAL', 'time': '9:00 PM ET'},\n",
    "    ]\n",
    "    return sample_matchups\n",
    "\n",
    "def calculate_player_correlations(player_a_logs, player_b_logs):\n",
    "    \"\"\"Calculate correlation between two players' performances\"\"\"\n",
    "    merged = pd.merge(player_a_logs, player_b_logs, on='GAME_DATE', suffixes=('_a', '_b'))\n",
    "    correlations = {}\n",
    "    for stat in ['PTS', 'REB', 'AST']:\n",
    "        if f'{stat}_a' in merged.columns and f'{stat}_b' in merged.columns:\n",
    "            corr = merged[f'{stat}_a'].corr(merged[f'{stat}_b'])\n",
    "            correlations[stat] = corr\n",
    "    return correlations\n",
    "\n",
    "# Export results to Excel\n",
    "def export_analysis(results, filename='nba_betting_analysis.xlsx'):\n",
    "    \"\"\"Export analysis results to Excel\"\"\"\n",
    "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "        if 'value_bets' in results:\n",
    "            pd.DataFrame(results['value_bets']).to_excel(writer, sheet_name='Value_Bets', index=False)\n",
    "        if 'predictions' in results:\n",
    "            predictions_df = pd.DataFrame.from_dict(results['predictions'], orient='index')\n",
    "            predictions_df.to_excel(writer, sheet_name='Player_Predictions')\n",
    "    print(f\"Analysis exported to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÄ Fetching NBA stats for 2023-24...\n",
      "‚Üí Attempt 1 fetching 2023-24 data...\n",
      "‚úÖ 2023-24: saved 572 player records to 'nba_player_stats_2023_24.csv'\n",
      "\n",
      "üèÄ Fetching NBA stats for 2024-25...\n",
      "‚Üí Attempt 1 fetching 2024-25 data...\n",
      "‚úÖ 2024-25: saved 569 player records to 'nba_player_stats_2024_25.csv'\n",
      "\n",
      "üéâ Done! Both 2023-24 and 2024-25 seasons downloaded.\n"
     ]
    }
   ],
   "source": [
    "#--cell 4--#\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "url = \"https://stats.nba.com/stats/leaguedashplayerstats\"\n",
    "\n",
    "base_params = {\n",
    "    \"College\": \"\",\n",
    "    \"Conference\": \"\",\n",
    "    \"Country\": \"\",\n",
    "    \"DateFrom\": \"\",\n",
    "    \"DateTo\": \"\",\n",
    "    \"Division\": \"\",\n",
    "    \"DraftPick\": \"\",\n",
    "    \"DraftYear\": \"\",\n",
    "    \"GameScope\": \"\",\n",
    "    \"GameSegment\": \"\",\n",
    "    \"Height\": \"\",\n",
    "    \"ISTRound\": \"\",\n",
    "    \"LastNGames\": \"0\",\n",
    "    \"LeagueID\": \"00\",\n",
    "    \"Location\": \"\",\n",
    "    \"MeasureType\": \"Base\",\n",
    "    \"Month\": \"0\",\n",
    "    \"OpponentTeamID\": \"0\",\n",
    "    \"Outcome\": \"\",\n",
    "    \"PORound\": \"0\",\n",
    "    \"PaceAdjust\": \"N\",\n",
    "    \"PerMode\": \"PerGame\",\n",
    "    \"Period\": \"0\",\n",
    "    \"PlayerExperience\": \"\",\n",
    "    \"PlayerPosition\": \"\",\n",
    "    \"PlusMinus\": \"N\",\n",
    "    \"Rank\": \"N\",\n",
    "    \"SeasonSegment\": \"\",\n",
    "    \"SeasonType\": \"Regular Season\",\n",
    "    \"ShotClockRange\": \"\",\n",
    "    \"StarterBench\": \"\",\n",
    "    \"TeamID\": \"0\",\n",
    "    \"VsConference\": \"\",\n",
    "    \"VsDivision\": \"\",\n",
    "    \"Weight\": \"\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"application/json, text/plain, */*\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Origin\": \"https://www.nba.com\",\n",
    "    \"Referer\": \"https://www.nba.com/\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/141.0.0.0 Safari/537.36\",\n",
    "    \"x-nba-stats-origin\": \"stats\",\n",
    "    \"x-nba-stats-token\": \"true\"\n",
    "}\n",
    "\n",
    "seasons = [\"2023-24\", \"2024-25\"]\n",
    "\n",
    "def fetch_season_data(season, retries=3):\n",
    "    \"\"\"Fetch one season‚Äôs player stats, retrying if timeout or network error.\"\"\"\n",
    "    params = base_params.copy()\n",
    "    params[\"Season\"] = season\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            print(f\"‚Üí Attempt {attempt} fetching {season} data...\")\n",
    "            response = requests.get(url, headers=headers, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"‚ö†Ô∏è Timeout on attempt {attempt}/{retries} for {season}. Retrying...\")\n",
    "            time.sleep(3 * attempt)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Error on attempt {attempt}/{retries}: {e}\")\n",
    "            time.sleep(3 * attempt)\n",
    "    raise RuntimeError(f\"Failed to fetch {season} data after {retries} attempts.\")\n",
    "\n",
    "# Main loop\n",
    "for season in seasons:\n",
    "    print(f\"\\nüèÄ Fetching NBA stats for {season}...\")\n",
    "    data = fetch_season_data(season)\n",
    "\n",
    "    headers_list = data[\"resultSets\"][0][\"headers\"]\n",
    "    rows = data[\"resultSets\"][0][\"rowSet\"]\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=headers_list)\n",
    "    filename = f\"nba_player_stats_{season.replace('-', '_')}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    print(f\"‚úÖ {season}: saved {len(df)} player records to '{filename}'\")\n",
    "\n",
    "    # Wait 3‚Äì6 seconds before next season to avoid throttling\n",
    "    time.sleep(random.uniform(3, 6))\n",
    "\n",
    "print(\"\\nüéâ Done! Both 2023-24 and 2024-25 seasons downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce98caeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 2023-24...\n",
      "‚úÖ Saved 26401 records for 2023-24\n",
      "Fetching 2024-25...\n",
      "‚úÖ Saved 26306 records for 2024-25\n",
      "Fetching 2025-26...\n",
      "‚úÖ Saved 2115 records for 2025-26\n"
     ]
    }
   ],
   "source": [
    "#--cell 5--#\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_box_scores(season, season_type=\"Regular Season\"):\n",
    "    url = \"https://stats.nba.com/stats/leaguegamelog\"\n",
    "    params = {\n",
    "        \"Counter\": 1000,\n",
    "        \"DateFrom\": \"\",\n",
    "        \"DateTo\": \"\",\n",
    "        \"Direction\": \"DESC\",\n",
    "        \"ISTRound\": \"\",\n",
    "        \"LeagueID\": \"00\",\n",
    "        \"PlayerOrTeam\": \"P\",\n",
    "        \"Season\": season,\n",
    "        \"SeasonType\": season_type,\n",
    "        \"Sorter\": \"DATE\"\n",
    "    }\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36\",\n",
    "        \"Referer\": \"https://www.nba.com/\",\n",
    "        \"Origin\": \"https://www.nba.com\",\n",
    "        \"Accept\": \"application/json, text/plain, */*\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()[\"resultSets\"][0]\n",
    "    df = pd.DataFrame(data[\"rowSet\"], columns=data[\"headers\"])\n",
    "    return df\n",
    "\n",
    "# Get all three seasons\n",
    "seasons = [\"2023-24\", \"2024-25\", \"2025-26\"]\n",
    "for season in seasons:\n",
    "    print(f\"Fetching {season}...\")\n",
    "    df = get_box_scores(season)\n",
    "    df.to_csv(f\"nba_boxscores_{season}.csv\", index=False)\n",
    "    print(f\"‚úÖ Saved {len(df)} records for {season}\")\n",
    "    time.sleep(2)  # polite delay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eefaa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: nba_player_stats_2023_24_enriched.csv\n",
      "‚úÖ Saved: nba_player_stats_2024_25_enriched.csv\n",
      "üèÄ Combined: nba_player_stats_2023_25_combined.csv\n"
     ]
    }
   ],
   "source": [
    "#--cell 6--#\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import unicodedata\n",
    "\n",
    "# ---- Keep/Map settings -------------------------------------------------------\n",
    "\n",
    "ADV_COLS_KEEP = [\n",
    "    \"Player\", \"Pos\", \"Age\", \"Tm\", \"G\", \"MP\",\n",
    "    \"PER\", \"TS%\", \"3PAr\", \"FTr\",\n",
    "    \"ORB%\", \"DRB%\", \"TRB%\",\n",
    "    \"AST%\", \"STL%\", \"BLK%\",\n",
    "    \"TOV%\", \"USG%\",\n",
    "    \"ORtg\", \"DRtg\",\n",
    "    \"OWS\", \"DWS\", \"WS\", \"WS/48\",\n",
    "    \"OBPM\", \"DBPM\", \"BPM\", \"VORP\"\n",
    "]\n",
    "\n",
    "# Basketball-Reference -> NBA/your dataset codes\n",
    "TEAM_ABBR_MAP = {\n",
    "    \"BRK\": \"BKN\",\n",
    "    \"PHO\": \"PHX\",\n",
    "    \"CHO\": \"CHA\",\n",
    "    \"UTH\": \"UTA\",   # rare alias safety\n",
    "    \"NJN\": \"BKN\",   # historical\n",
    "    \"SEA\": \"OKC\",   # historical\n",
    "    \"VAN\": \"MEM\",   # historical\n",
    "}\n",
    "\n",
    "# ---- Helpers -----------------------------------------------------------------\n",
    "\n",
    "def normalize_name(s):\n",
    "    \"\"\"Normalize player names for consistent joining (lowercase, no accents/punct).\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return s\n",
    "    s = s.strip().lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    for ch in [\".\", \"'\", \"`\", \"‚Äô\", \"‚Äú\", \"‚Äù\", \",\"]:\n",
    "        s = s.replace(ch, \"\")\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "# ---- Fetch advanced table from Basketball-Reference --------------------------\n",
    "\n",
    "def fetch_advanced_table(season=2026):\n",
    "    \"\"\"\n",
    "    Fetch and clean Basketball-Reference advanced stats table for a given season.\n",
    "    Example: season=2025 -> https://www.basketball-reference.com/leagues/NBA_2025_advanced.html\n",
    "    \"\"\"\n",
    "    url = f\"https://www.basketball-reference.com/leagues/NBA_{season}_advanced.html\"\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    tables = pd.read_html(io.StringIO(resp.text), header=0)\n",
    "    if not tables:\n",
    "        raise RuntimeError(\"No tables found on Basketball-Reference page.\")\n",
    "\n",
    "    df = tables[0].copy()\n",
    "\n",
    "    # Remove duplicate header rows\n",
    "    if \"Rk\" in df.columns:\n",
    "        df = df[df[\"Rk\"] != \"Rk\"].copy()\n",
    "        df.drop(columns=[\"Rk\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Normalize column names (strip and upper-case for easy access)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # Basketball Reference sometimes labels the team column differently ‚Äî make sure it exists\n",
    "    team_col = None\n",
    "    for c in df.columns:\n",
    "        if c.lower() in [\"tm\", \"team\", \"team_name\"]:\n",
    "            team_col = c\n",
    "            break\n",
    "    if not team_col:\n",
    "        raise KeyError(f\"Could not find a team column in advanced table. Found: {df.columns.tolist()}\")\n",
    "    df.rename(columns={team_col: \"Tm\"}, inplace=True)\n",
    "\n",
    "    # Keep relevant columns if present\n",
    "    keep = [c for c in ADV_COLS_KEEP if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "\n",
    "    # Convert numeric columns\n",
    "    non_numeric = {\"Player\", \"Pos\", \"Tm\"}\n",
    "    for c in [c for c in df.columns if c not in non_numeric]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Map team abbreviations to match your dataset\n",
    "    df[\"Tm\"] = df[\"Tm\"].replace(TEAM_ABBR_MAP)\n",
    "\n",
    "    # Add join keys\n",
    "    df[\"player_key\"] = df[\"Player\"].map(normalize_name)\n",
    "    df[\"team_key\"] = df[\"Tm\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---- Load your averages CSV and align columns --------------------------------\n",
    "\n",
    "def load_averages_csv(path):\n",
    "    \"\"\"\n",
    "    Load your NBA averages CSV (with headers like PLAYER_NAME, TEAM_ABBREVIATION).\n",
    "    Renames to canonical 'Player' and 'Team' and adds join keys.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Auto-map your headers to canonical names\n",
    "    col_map = {}\n",
    "    for c in df.columns:\n",
    "        cl = c.strip().lower()\n",
    "        if cl == \"player_name\":\n",
    "            col_map[c] = \"Player\"\n",
    "        elif cl in (\"team_abbreviation\", \"tm\", \"team\"):\n",
    "            col_map[c] = \"Team\"\n",
    "        # keep other columns as-is\n",
    "\n",
    "    df = df.rename(columns=col_map)\n",
    "\n",
    "    if \"Player\" not in df.columns or \"Team\" not in df.columns:\n",
    "        raise ValueError(\n",
    "            \"Couldn't find columns for 'Player' and 'Team'. \"\n",
    "            f\"Available columns: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Join keys\n",
    "    df[\"player_key\"] = df[\"Player\"].map(normalize_name)\n",
    "    df[\"team_key\"] = df[\"Team\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---- Merge logic (with TOT fallback for traded players) ----------------------\n",
    "\n",
    "def merge_advanced_into_averages(df_avg, df_adv):\n",
    "    \"\"\"\n",
    "    Merge advanced metrics into averages.\n",
    "    1) Exact Player+Team match (ignore TOT).\n",
    "    2) For remaining NaNs, fill from TOT row by Player.\n",
    "    \"\"\"\n",
    "    adv_team = df_adv[df_adv[\"Tm\"] != \"TOT\"].copy()\n",
    "    adv_tot  = df_adv[df_adv[\"Tm\"] == \"TOT\"].copy()\n",
    "\n",
    "    adv_cols_to_add = [c for c in df_adv.columns if c not in {\"Player\", \"Pos\", \"Age\", \"Tm\", \"player_key\", \"team_key\"}]\n",
    "    meta_cols = [c for c in [\"Pos\", \"Age\"] if c in df_adv.columns]\n",
    "    join_cols_full = meta_cols + adv_cols_to_add\n",
    "\n",
    "    merged = df_avg.merge(\n",
    "        adv_team[[\"player_key\", \"team_key\"] + join_cols_full],\n",
    "        on=[\"player_key\", \"team_key\"],\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_adv\"),\n",
    "    )\n",
    "\n",
    "    # Determine \"missing\" based on a representative advanced column\n",
    "    probe_col = \"PER\" if \"PER\" in merged.columns else (\"WS/48\" if \"WS/48\" in merged.columns else None)\n",
    "    missing_mask = merged[probe_col].isna() if probe_col else merged.isna().any(axis=1)\n",
    "\n",
    "    if missing_mask.any() and not adv_tot.empty:\n",
    "        fallback = merged[missing_mask].merge(\n",
    "            adv_tot[[\"player_key\"] + join_cols_full],\n",
    "            on=\"player_key\",\n",
    "            how=\"left\",\n",
    "            suffixes=(\"\", \"_tot\"),\n",
    "        )\n",
    "        for col in join_cols_full:\n",
    "            if col in merged.columns and col in fallback.columns:\n",
    "                merged.loc[missing_mask, col] = merged.loc[missing_mask, col].fillna(fallback[col])\n",
    "\n",
    "    return merged\n",
    "\n",
    "# ==============================================================================\n",
    "# Example usage for your two files\n",
    "# ==============================================================================\n",
    "\n",
    "# ---- 2023‚Äì24 (Basketball-Reference season code = 2024) -----------------------\n",
    "df_avg_2024 = load_averages_csv(\"nba_player_stats_2023_24.csv\")\n",
    "df_adv_2024 = fetch_advanced_table(season=2024)\n",
    "df_enriched_2024 = merge_advanced_into_averages(df_avg_2024, df_adv_2024)\n",
    "df_enriched_2024.to_csv(\"nba_player_stats_2023_24_enriched.csv\", index=False)\n",
    "print(\"‚úÖ Saved: nba_player_stats_2023_24_enriched.csv\")\n",
    "\n",
    "# ---- 2024‚Äì25 (Basketball-Reference season code = 2025) -----------------------\n",
    "df_avg_2025 = load_averages_csv(\"nba_player_stats_2024_25.csv\")\n",
    "df_adv_2025 = fetch_advanced_table(season=2025)\n",
    "df_enriched_2025 = merge_advanced_into_averages(df_avg_2025, df_adv_2025)\n",
    "df_enriched_2025.to_csv(\"nba_player_stats_2024_25_enriched.csv\", index=False)\n",
    "print(\"‚úÖ Saved: nba_player_stats_2024_25_enriched.csv\")\n",
    "\n",
    "# ---- (Optional) Combine both seasons into one file ---------------------------\n",
    "df_combined = pd.concat([df_enriched_2024, df_enriched_2025], ignore_index=True)\n",
    "df_combined.to_csv(\"nba_player_stats_2023_25_combined.csv\", index=False)\n",
    "print(\"üèÄ Combined: nba_player_stats_2023_25_combined.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68cb75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CELL 7a: assemble_player_game_features helper ----------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def assemble_player_game_features(logs_df: pd.DataFrame, enriched_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine raw boxscore logs + per-player enriched season stats into a\n",
    "    training-ready feature table.\n",
    "\n",
    "    Output columns include:\n",
    "      ‚Ä¢ GAME_DATE, PLAYER_ID, PLAYER_NAME, TEAM_ABBREVIATION, OPPONENT_ABBREVIATION\n",
    "      ‚Ä¢ rolling stats (PTS_roll5, REB_roll5, etc.)\n",
    "      ‚Ä¢ efficiency metrics from enriched_df (PER, TS%, USG%, BPM, VORP, ORtg, DRtg, WS/48)\n",
    "      ‚Ä¢ rest / HOME indicators and next-game targets (PTS_next, REB_next, AST_next)\n",
    "    \"\"\"\n",
    "\n",
    "    df = logs_df.copy()\n",
    "    df[\"GAME_DATE\"] = pd.to_datetime(df[\"GAME_DATE\"], errors=\"coerce\")\n",
    "\n",
    "    # sort for proper rolling windows\n",
    "    df = df.sort_values([\"PLAYER_ID\", \"GAME_DATE\"]).reset_index(drop=True)\n",
    "\n",
    "    # numeric ensure\n",
    "    for col in [\"PTS\",\"REB\",\"AST\",\"MIN\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # rolling means (momentum features)\n",
    "    for stat in [\"PTS\",\"REB\",\"AST\",\"MIN\"]:\n",
    "        df[f\"{stat}_roll5\"]  = (df.groupby(\"PLAYER_ID\")[stat]\n",
    "                                  .rolling(5, min_periods=1).mean().reset_index(level=0, drop=True))\n",
    "        df[f\"{stat}_roll10\"] = (df.groupby(\"PLAYER_ID\")[stat]\n",
    "                                  .rolling(10, min_periods=1).mean().reset_index(level=0, drop=True))\n",
    "\n",
    "    # next-game ‚Äútarget‚Äù columns\n",
    "    for stat in [\"PTS\",\"REB\",\"AST\"]:\n",
    "        df[f\"{stat}_next\"] = df.groupby(\"PLAYER_ID\")[stat].shift(-1)\n",
    "\n",
    "    # days_rest\n",
    "    if \"GAME_DATE\" in df.columns:\n",
    "        df[\"days_rest\"] = (df.groupby(\"PLAYER_ID\")[\"GAME_DATE\"]\n",
    "                             .diff().dt.days.clip(lower=0).fillna(3))\n",
    "\n",
    "    # home/away flag (if not already encoded)\n",
    "    if \"MATCHUP\" in df.columns:\n",
    "        df[\"HOME\"] = (~df[\"MATCHUP\"].str.contains(\"@\")).astype(int)\n",
    "\n",
    "    # merge in season-level efficiency stats\n",
    "    common_key = \"PLAYER_ID\" if \"PLAYER_ID\" in enriched_df.columns else \"PLAYER_NAME\"\n",
    "    eff_cols = [\"PER\",\"TS%\",\"USG%\",\"BPM\",\"VORP\",\"ORtg\",\"DRtg\",\"WS/48\"]\n",
    "    eff_cols = [c for c in eff_cols if c in enriched_df.columns]\n",
    "    df = df.merge(enriched_df[[common_key]+eff_cols].drop_duplicates(common_key),\n",
    "                  on=common_key, how=\"left\")\n",
    "\n",
    "    # clip out extreme or empty records\n",
    "    df = df.dropna(subset=[\"PTS\",\"REB\",\"AST\",\"MIN\"])\n",
    "    df = df[df[\"MIN\"] > 0]\n",
    "\n",
    "    print(f\"‚úÖ Built features for {df['PLAYER_ID'].nunique()} players ‚Äî {len(df):,} rows.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d31974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Built features for 571 players ‚Äî 26,283 rows.\n",
      "‚úÖ Built features for 569 players ‚Äî 26,206 rows.\n",
      "‚ö†Ô∏è Skipping team context: missing required columns.\n",
      "üîÅ PTS MAE: 4.55 ¬± 0.13\n",
      "‚ö†Ô∏è Quantile model failed for PTS: xgboost.sklearn.XGBRegressor() got multiple values for keyword argument 'objective'\n",
      "üîÅ REB MAE: 1.93 ¬± 0.05\n",
      "‚ö†Ô∏è Quantile model failed for REB: xgboost.sklearn.XGBRegressor() got multiple values for keyword argument 'objective'\n",
      "üîÅ AST MAE: 1.38 ¬± 0.05\n",
      "‚ö†Ô∏è Quantile model failed for AST: xgboost.sklearn.XGBRegressor() got multiple values for keyword argument 'objective'\n",
      "\n",
      "Top PTS importances:\n",
      "usage_minutes_interact    17617.062500\n",
      "PTS_roll10                 5413.078125\n",
      "ts_usage_interact          1549.593506\n",
      "TS%                         434.394745\n",
      "WS/48                       368.271759\n",
      "PER                         335.622955\n",
      "MIN_roll10                  330.547943\n",
      "PTS_roll5                   314.593933\n",
      "VORP                        294.220703\n",
      "USG%                        273.860565\n",
      "BPM                         260.296143\n",
      "MIN_roll5                   250.700424\n",
      "PTS_roll5_std               242.570236\n",
      "AST_roll5_std               239.030594\n",
      "REB_roll5_std               237.973419\n",
      "dtype: float64\n",
      "\n",
      "Top REB importances:\n",
      "REB_roll10                2100.805176\n",
      "REB_roll5                  162.213974\n",
      "WS/48                       74.934334\n",
      "PER                         73.951691\n",
      "usage_minutes_interact      58.997601\n",
      "VORP                        56.854694\n",
      "BPM                         55.037693\n",
      "MIN_roll5                   53.304329\n",
      "ts_usage_interact           53.269489\n",
      "TS%                         51.231003\n",
      "MIN_roll10                  49.189793\n",
      "REB_roll5_std               47.540466\n",
      "AST_roll5_std               46.572861\n",
      "REB_trend                   45.823708\n",
      "AST_trend                   45.647415\n",
      "dtype: float64\n",
      "\n",
      "Top AST importances:\n",
      "AST_roll10                1441.683960\n",
      "AST_roll5                   99.659943\n",
      "ts_usage_interact           41.756992\n",
      "VORP                        40.178501\n",
      "USG%                        37.897484\n",
      "usage_minutes_interact      36.344166\n",
      "WS/48                       33.392971\n",
      "TS%                         32.336269\n",
      "BPM                         30.172745\n",
      "REB_roll5_std               29.129976\n",
      "PER                         28.261156\n",
      "PTS_roll5_std               27.963045\n",
      "MIN_roll5                   27.801201\n",
      "AST_roll5_std               26.746092\n",
      "AST_trend                   26.380806\n",
      "dtype: float64\n",
      "\n",
      "‚úÖ Training complete. Use `project_players(features_today)` to get mean & sd for PTS/REB/AST.\n",
      "Cross-validated MAE by stat: {'PTS': (4.545437261078701, 0.12719152825455315), 'REB': (1.9296686361741162, 0.0459578112542017), 'AST': (1.380224899262885, 0.05081482574124309)}\n"
     ]
    }
   ],
   "source": [
    "# -- CELL 8 (Final Robust Version) --------------------------------------------\n",
    "# XGBoost player models with: player-wise CV, feature engineering, opponent context,\n",
    "# optional hyperparameter tuning, and quantile models for uncertainty.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Load season data\n",
    "# ----------------------------\n",
    "logs_2324      = pd.read_csv(\"nba_boxscores_2023-24.csv\")\n",
    "logs_2425      = pd.read_csv(\"nba_boxscores_2024-25.csv\")\n",
    "enriched_2324  = pd.read_csv(\"nba_player_stats_2023_24_enriched.csv\")\n",
    "enriched_2425  = pd.read_csv(\"nba_player_stats_2024_25_enriched.csv\")\n",
    "\n",
    "feat_2324 = assemble_player_game_features(logs_2324, enriched_2324)\n",
    "feat_2425 = assemble_player_game_features(logs_2425, enriched_2425)\n",
    "features_all = pd.concat([feat_2324, feat_2425], ignore_index=True)\n",
    "\n",
    "if \"GAME_DATE\" in features_all.columns:\n",
    "    features_all[\"GAME_DATE\"] = pd.to_datetime(features_all[\"GAME_DATE\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Feature Engineering\n",
    "# ----------------------------\n",
    "def safe_has(cols): \n",
    "    return all(c in features_all.columns for c in cols)\n",
    "\n",
    "# Trend (momentum) features\n",
    "for stat in [\"PTS\", \"REB\", \"AST\"]:\n",
    "    c5, c10 = f\"{stat}_roll5\", f\"{stat}_roll10\"\n",
    "    if safe_has([c5, c10]):\n",
    "        features_all[f\"{stat}_trend\"] = features_all[c5] - features_all[c10]\n",
    "\n",
    "# Rolling volatility\n",
    "def ensure_roll5_std(stat):\n",
    "    col_std = f\"{stat}_roll5_std\"\n",
    "    if col_std not in features_all.columns and stat in features_all.columns:\n",
    "        features_all[col_std] = (\n",
    "            features_all.sort_values([\"PLAYER_ID\",\"GAME_DATE\"])\n",
    "                        .groupby(\"PLAYER_ID\")[stat]\n",
    "                        .rolling(5, min_periods=1).std()\n",
    "                        .reset_index(level=0, drop=True)\n",
    "        )\n",
    "for stat in [\"PTS\",\"REB\",\"AST\"]:\n",
    "    ensure_roll5_std(stat)\n",
    "\n",
    "# Interactions\n",
    "if safe_has([\"USG%\",\"MIN_roll5\"]):\n",
    "    features_all[\"usage_minutes_interact\"] = features_all[\"USG%\"] * features_all[\"MIN_roll5\"]\n",
    "if safe_has([\"TS%\",\"USG%\"]):\n",
    "    features_all[\"ts_usage_interact\"] = features_all[\"TS%\"] * features_all[\"USG%\"]\n",
    "\n",
    "# Back-to-back indicator\n",
    "if \"days_rest\" in features_all.columns and \"is_b2b\" not in features_all.columns:\n",
    "    features_all[\"is_b2b\"] = (features_all[\"days_rest\"] == 0).astype(int)\n",
    "\n",
    "# ensure boolean-like columns are numeric\n",
    "for col in [\"HOME\", \"is_b2b\"]:\n",
    "    if col in features_all.columns and features_all[col].dtype == bool:\n",
    "        features_all[col] = features_all[col].astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Opponent / Matchup Context\n",
    "# ----------------------------\n",
    "def build_team_context(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def _dedupe_by_name(d: pd.DataFrame, names=(\"TEAM_ABBREVIATION\",\"OPPONENT_ABBREVIATION\",\"GAME_DATE\")) -> pd.DataFrame:\n",
    "        d = d.copy()\n",
    "        for nm in names:\n",
    "            idxs = [i for i, c in enumerate(d.columns) if c == nm]\n",
    "            if len(idxs) > 1:\n",
    "                to_drop = [d.columns[i] for i in idxs[1:]]\n",
    "                d.drop(columns=to_drop, inplace=True, errors=\"ignore\")\n",
    "                print(f\"üßπ Deduped duplicate column '{nm}' (dropped {len(to_drop)} duplicates).\")\n",
    "        if d.columns.duplicated().any():\n",
    "            d = d.loc[:, ~d.columns.duplicated()].copy()\n",
    "        return d\n",
    "\n",
    "    req = [\"GAME_DATE\", \"TEAM_ABBREVIATION\", \"OPPONENT_ABBREVIATION\"]\n",
    "    ctx_cols = [c for c in [\"ORtg_g_roll5\", \"DRtg_g_roll5\", \"Pace_g_roll5\"] if c in df.columns]\n",
    "    if not all(c in df.columns for c in req) or not ctx_cols:\n",
    "        print(\"‚ö†Ô∏è Skipping team context: missing required columns.\")\n",
    "        return df\n",
    "\n",
    "    df_clean = _dedupe_by_name(df)\n",
    "    team_day = (\n",
    "        df_clean[req + ctx_cols]\n",
    "        .drop_duplicates(subset=[\"GAME_DATE\", \"TEAM_ABBREVIATION\"])\n",
    "        .groupby([\"GAME_DATE\", \"TEAM_ABBREVIATION\"], as_index=False)\n",
    "        .mean(numeric_only=True)\n",
    "    )\n",
    "    opp_day = team_day.rename(\n",
    "        columns={\"TEAM_ABBREVIATION\": \"OPPONENT_ABBREVIATION\", **{c: f\"opp_{c}\" for c in ctx_cols}}\n",
    "    )\n",
    "    opp_day = _dedupe_by_name(opp_day, names=(\"GAME_DATE\",\"OPPONENT_ABBREVIATION\"))\n",
    "    right_cols = [\"GAME_DATE\",\"OPPONENT_ABBREVIATION\"] + [f\"opp_{c}\" for c in ctx_cols]\n",
    "    opp_day = opp_day[right_cols].drop_duplicates(subset=[\"GAME_DATE\",\"OPPONENT_ABBREVIATION\"])\n",
    "    out = df_clean.merge(opp_day, on=[\"GAME_DATE\",\"OPPONENT_ABBREVIATION\"], how=\"left\", validate=\"m:1\")\n",
    "    if \"Pace_g_roll5\" in out.columns and \"opp_Pace_g_roll5\" in out.columns:\n",
    "        out[\"pace_diff5\"] = out[\"Pace_g_roll5\"] - out[\"opp_Pace_g_roll5\"]\n",
    "    print(f\"‚úÖ Team context added. Rows: {len(out):,}\")\n",
    "    return out\n",
    "\n",
    "features_all = build_team_context(features_all)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Training setup\n",
    "# ----------------------------\n",
    "BASE_FEATURES = [\n",
    "    \"MIN_roll5\",\"MIN_roll10\",\"TS_game_roll5\",\"TS_game_roll10\",\n",
    "    \"usage_share_roll5\",\"ORtg_g_roll5\",\"DRtg_g_roll5\",\"Pace_g_roll5\",\n",
    "    \"PER\",\"TS%\",\"USG%\",\"ORtg\",\"DRtg\",\"WS/48\",\"BPM\",\"VORP\",\n",
    "    \"days_rest\",\"is_b2b\",\"HOME\",\n",
    "    \"PTS_trend\",\"REB_trend\",\"AST_trend\",\n",
    "    \"PTS_roll5_std\",\"REB_roll5_std\",\"AST_roll5_std\",\n",
    "    \"usage_minutes_interact\",\"ts_usage_interact\",\n",
    "    \"opp_ORtg_g_roll5\",\"opp_DRtg_g_roll5\",\"opp_Pace_g_roll5\",\"pace_diff5\"\n",
    "]\n",
    "STAT_ROLL = {\"PTS\":[\"PTS_roll5\",\"PTS_roll10\"],\"REB\":[\"REB_roll5\",\"REB_roll10\"],\"AST\":[\"AST_roll5\",\"AST_roll10\"]}\n",
    "TARGETS = {\"PTS\":\"PTS_next\",\"REB\":\"REB_next\",\"AST\":\"AST_next\"}\n",
    "\n",
    "USE_OPTUNA = False\n",
    "DEFAULT_SPLITS = 5\n",
    "\n",
    "models_mean, models_q_lo, models_q_hi = {}, {}, {}\n",
    "cv_scores, feature_bags = {}, {}\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Training loop\n",
    "# ----------------------------\n",
    "for stat, target_col in TARGETS.items():\n",
    "    feat_cols = [c for c in (BASE_FEATURES + STAT_ROLL[stat]) if c in features_all.columns]\n",
    "    feature_bags[stat] = feat_cols\n",
    "    needed = feat_cols + [target_col,\"PLAYER_ID\",\"GAME_DATE\"]\n",
    "    data = features_all.dropna(subset=[c for c in needed if c in features_all.columns]).copy()\n",
    "    if data.empty:\n",
    "        print(f\"‚ö†Ô∏è No data for {stat}.\")\n",
    "        continue\n",
    "\n",
    "    data = data.sort_values([\"PLAYER_ID\",\"GAME_DATE\"])\n",
    "    X, y, groups = data[feat_cols], data[target_col], data[\"PLAYER_ID\"]\n",
    "\n",
    "    unique_players = groups.nunique()\n",
    "    if unique_players < 2:\n",
    "        print(f\"‚ö†Ô∏è Only one player for {stat}; fallback to simple split.\")\n",
    "        cut = int(len(X)*0.8)\n",
    "        Xtr, Xte, ytr, yte = X.iloc[:cut], X.iloc[cut:], y.iloc[:cut], y.iloc[cut:]\n",
    "        model = XGBRegressor(n_estimators=400,learning_rate=0.05,max_depth=5,\n",
    "                             subsample=0.85,colsample_bytree=0.9,\n",
    "                             reg_lambda=1.0,reg_alpha=0.0,\n",
    "                             random_state=42,n_jobs=-1,verbosity=0)\n",
    "        model.fit(Xtr,ytr)\n",
    "        pred = model.predict(Xte)\n",
    "        mae = mean_absolute_error(yte,pred)\n",
    "        fold_mae=[mae]\n",
    "    else:\n",
    "        splits = min(DEFAULT_SPLITS, max(2, unique_players))\n",
    "        if splits < DEFAULT_SPLITS:\n",
    "            print(f\"‚ÑπÔ∏è Reduced splits for {stat} to {splits} (players={unique_players}).\")\n",
    "        group_kfold = GroupKFold(n_splits=splits)\n",
    "        fold_mae=[]\n",
    "        for tr,te in group_kfold.split(X,y,groups):\n",
    "            model=XGBRegressor(n_estimators=400,learning_rate=0.05,max_depth=5,\n",
    "                               subsample=0.85,colsample_bytree=0.9,\n",
    "                               reg_lambda=1.0,reg_alpha=0.0,\n",
    "                               random_state=42,n_jobs=-1,verbosity=0)\n",
    "            model.fit(X.iloc[tr],y.iloc[tr])\n",
    "            pred=model.predict(X.iloc[te])\n",
    "            fold_mae.append(mean_absolute_error(y.iloc[te],pred))\n",
    "\n",
    "    cv_scores[stat]=(float(np.mean(fold_mae)),float(np.std(fold_mae)))\n",
    "    print(f\"üîÅ {stat} MAE: {np.mean(fold_mae):.2f} ¬± {np.std(fold_mae):.2f}\")\n",
    "\n",
    "    model_mean=XGBRegressor(n_estimators=400,learning_rate=0.05,max_depth=5,\n",
    "                             subsample=0.85,colsample_bytree=0.9,\n",
    "                             reg_lambda=1.0,reg_alpha=0.0,\n",
    "                             random_state=42,n_jobs=-1,verbosity=0)\n",
    "    model_mean.fit(X,y)\n",
    "    models_mean[stat]=model_mean\n",
    "\n",
    "    # Quantile fits (if supported)\n",
    "    try:\n",
    "        q_lo=XGBRegressor(objective=\"reg:pquantile\",alpha=0.2,**model_mean.get_xgb_params())\n",
    "        q_hi=XGBRegressor(objective=\"reg:pquantile\",alpha=0.8,**model_mean.get_xgb_params())\n",
    "        q_lo.fit(X,y); q_hi.fit(X,y)\n",
    "        models_q_lo[stat],models_q_hi[stat]=q_lo,q_hi\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Quantile model failed for {stat}: {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Importances\n",
    "# ----------------------------\n",
    "def top_importances(model, cols, k=15):\n",
    "    try: gain=model.get_booster().get_score(importance_type=\"gain\")\n",
    "    except Exception: gain=model.get_booster().get_fscore()\n",
    "    if not gain: return pd.Series(dtype=float)\n",
    "    name_map={f\"f{i}\":col for i,col in enumerate(cols)}\n",
    "    s=pd.Series(gain).rename(index=lambda x:name_map.get(x,x))\n",
    "    s=pd.to_numeric(s,errors=\"coerce\")\n",
    "    return s.sort_values(ascending=False).head(k)\n",
    "\n",
    "for stat in models_mean:\n",
    "    print(f\"\\nTop {stat} importances:\")\n",
    "    imp=top_importances(models_mean[stat],feature_bags[stat])\n",
    "    print(imp if not imp.empty else \"(none)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Game-day projection helper\n",
    "# ----------------------------\n",
    "def project_players(df_today: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_today is None or df_today.empty:\n",
    "        return pd.DataFrame()\n",
    "    frames=[]\n",
    "    for stat in [\"PTS\",\"REB\",\"AST\"]:\n",
    "        if stat not in models_mean: continue\n",
    "        cols=[c for c in feature_bags[stat] if c in df_today.columns]\n",
    "        if not cols: continue\n",
    "        X=df_today[cols]\n",
    "        mu=models_mean[stat].predict(X)\n",
    "        if stat in models_q_lo and stat in models_q_hi:\n",
    "            try:\n",
    "                ql=models_q_lo[stat].predict(X)\n",
    "                qh=models_q_hi[stat].predict(X)\n",
    "                sd=(qh-ql)/2.0\n",
    "            except Exception: sd=np.full_like(mu,np.nan)\n",
    "        else: sd=np.full_like(mu,np.nan)\n",
    "        frames.append(pd.DataFrame({\n",
    "            \"PLAYER_ID\":df_today.get(\"PLAYER_ID\"),\n",
    "            \"PLAYER_NAME\":df_today.get(\"PLAYER_NAME\"),\n",
    "            \"TEAM_ABBREVIATION\":df_today.get(\"TEAM_ABBREVIATION\"),\n",
    "            \"OPPONENT_ABBREVIATION\":df_today.get(\"OPPONENT_ABBREVIATION\"),\n",
    "            \"market\":stat,\"projection_mean\":mu,\"projection_sd\":sd\n",
    "        }))\n",
    "    return pd.concat(frames,ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete. Use `project_players(features_today)` to get mean & sd for PTS/REB/AST.\")\n",
    "print(\"Cross-validated MAE by stat:\", cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5d8812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 importances ‚Äî PTS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "usage_minutes_interact    0.602913\n",
       "PTS_roll10                0.185253\n",
       "ts_usage_interact         0.053032\n",
       "TS%                       0.014866\n",
       "WS/48                     0.012603\n",
       "PER                       0.011486\n",
       "MIN_roll10                0.011312\n",
       "PTS_roll5                 0.010766\n",
       "VORP                      0.010069\n",
       "USG%                      0.009372\n",
       "BPM                       0.008908\n",
       "MIN_roll5                 0.008580\n",
       "PTS_roll5_std             0.008302\n",
       "AST_roll5_std             0.008180\n",
       "REB_roll5_std             0.008144\n",
       "dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 importances ‚Äî REB:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "REB_roll10                0.658034\n",
       "REB_roll5                 0.050810\n",
       "WS/48                     0.023472\n",
       "PER                       0.023164\n",
       "usage_minutes_interact    0.018480\n",
       "VORP                      0.017809\n",
       "BPM                       0.017239\n",
       "MIN_roll5                 0.016696\n",
       "ts_usage_interact         0.016686\n",
       "TS%                       0.016047\n",
       "MIN_roll10                0.015408\n",
       "REB_roll5_std             0.014891\n",
       "AST_roll5_std             0.014588\n",
       "REB_trend                 0.014353\n",
       "AST_trend                 0.014298\n",
       "dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 importances ‚Äî AST:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AST_roll10                0.692874\n",
       "AST_roll5                 0.047897\n",
       "ts_usage_interact         0.020068\n",
       "VORP                      0.019310\n",
       "USG%                      0.018214\n",
       "usage_minutes_interact    0.017467\n",
       "WS/48                     0.016049\n",
       "TS%                       0.015541\n",
       "BPM                       0.014501\n",
       "REB_roll5_std             0.014000\n",
       "PER                       0.013582\n",
       "PTS_roll5_std             0.013439\n",
       "MIN_roll5                 0.013361\n",
       "AST_roll5_std             0.012854\n",
       "AST_trend                 0.012679\n",
       "dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Feature importance summaries computed for: ['PTS', 'REB', 'AST']\n"
     ]
    }
   ],
   "source": [
    "# -- CELL 9 (updated for new model structure) ---------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Detect which model dictionary is available\n",
    "if 'models_mean' in globals() and models_mean:\n",
    "    active_models = models_mean\n",
    "elif 'models' in globals() and models:\n",
    "    active_models = models\n",
    "else:\n",
    "    raise RuntimeError(\"No trained models found. Run Cell 8 first.\")\n",
    "\n",
    "all_imps = {}\n",
    "\n",
    "for stat, mdl in active_models.items():\n",
    "    # Try to find associated feature columns\n",
    "    if 'feature_bags' in globals():\n",
    "        feat_cols = feature_bags.get(stat, [])\n",
    "    elif 'feature_cols_by_stat' in globals():\n",
    "        feat_cols = feature_cols_by_stat.get(stat, [])\n",
    "    else:\n",
    "        feat_cols = []\n",
    "\n",
    "    # Importance extraction\n",
    "    if hasattr(mdl, \"feature_importances_\"):\n",
    "        imp_series = pd.Series(mdl.feature_importances_, index=feat_cols).sort_values(ascending=False)\n",
    "    else:\n",
    "        try:\n",
    "            gain = mdl.get_booster().get_score(importance_type=\"gain\")\n",
    "            name_map = {f\"f{i}\": col for i, col in enumerate(feat_cols)}\n",
    "            imp_series = pd.Series(gain).rename(index=lambda x: name_map.get(x, x)).astype(float)\n",
    "            imp_series = imp_series.sort_values(ascending=False)\n",
    "        except Exception:\n",
    "            imp_series = pd.Series(dtype=float)\n",
    "\n",
    "    all_imps[stat] = imp_series\n",
    "\n",
    "    print(f\"\\nTop 15 importances ‚Äî {stat}:\")\n",
    "    if not imp_series.empty:\n",
    "        display(imp_series.head(15))\n",
    "    else:\n",
    "        print(\"(no importances available)\")\n",
    "\n",
    "# For backward compatibility (PTS importances stored as imp)\n",
    "if \"PTS\" in all_imps:\n",
    "    imp = all_imps[\"PTS\"]\n",
    "\n",
    "print(\"\\n‚úÖ Feature importance summaries computed for:\", list(all_imps.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c03eeccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No ORtg/DRtg/Pace in data ‚Äî using PTS_roll5/REB_roll5/AST_roll5 as proxies.\n",
      "‚úÖ Team PTS baseline R¬≤: 0.197  |  n=87,504\n"
     ]
    }
   ],
   "source": [
    "# -- CELL 10 (final robust version) ------------------------------------------\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# 1Ô∏è‚É£ Try to ensure team-level rolling stats exist, or fall back to simpler proxies\n",
    "context_bases = []\n",
    "for base in [\"ORtg\", \"DRtg\", \"Pace\"]:\n",
    "    if base in features_all.columns:\n",
    "        roll_col = f\"{base}_g_roll5\"\n",
    "        if roll_col not in features_all.columns:\n",
    "            features_all[roll_col] = (\n",
    "                features_all.sort_values([\"TEAM_ABBREVIATION\",\"GAME_DATE\"])\n",
    "                            .groupby(\"TEAM_ABBREVIATION\")[base]\n",
    "                            .rolling(5, min_periods=1)\n",
    "                            .mean()\n",
    "                            .reset_index(level=0, drop=True)\n",
    "            )\n",
    "            print(f\"‚úÖ Created {roll_col}\")\n",
    "        context_bases.append(roll_col)\n",
    "\n",
    "# If none of those exist, use generic team performance metrics instead\n",
    "if not context_bases:\n",
    "    print(\"‚ö†Ô∏è No ORtg/DRtg/Pace in data ‚Äî using PTS_roll5/REB_roll5/AST_roll5 as proxies.\")\n",
    "    proxy_cols = [c for c in [\"PTS_roll5\",\"REB_roll5\",\"AST_roll5\"] if c in features_all.columns]\n",
    "    for c in proxy_cols:\n",
    "        features_all[f\"team_{c}\"] = (\n",
    "            features_all.sort_values([\"TEAM_ABBREVIATION\",\"GAME_DATE\"])\n",
    "                        .groupby(\"TEAM_ABBREVIATION\")[c]\n",
    "                        .transform(\"mean\")\n",
    "        )\n",
    "    context_bases = [f\"team_{c}\" for c in proxy_cols]\n",
    "\n",
    "# 2Ô∏è‚É£ Build team-level table\n",
    "agg_dict = {\n",
    "    \"team_pts\": (\"PTS\",\"sum\"),\n",
    "    \"team_pts_next\": (\"PTS_next\",\"sum\"),\n",
    "}\n",
    "for c in context_bases:\n",
    "    agg_dict[c.replace(\"_g_roll5\",\"\").replace(\"team_\",\"\")] = (c,\"mean\")\n",
    "\n",
    "team_games = (\n",
    "    features_all.groupby([\"GAME_DATE\",\"TEAM_ABBREVIATION\"], as_index=False)\n",
    "    .agg(**agg_dict)\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Build opponent-matched pairs (same date)\n",
    "opp = team_games.rename(columns={\n",
    "    \"TEAM_ABBREVIATION\": \"OPPONENT_ABBREVIATION\",\n",
    "    \"team_pts\": \"opp_pts\",\n",
    "    \"team_pts_next\": \"opp_pts_next\",\n",
    "    **{k: f\"opp_{k}\" for k in [x for x in team_games.columns if x not in [\"GAME_DATE\",\"TEAM_ABBREVIATION\",\"team_pts\",\"team_pts_next\"]]}\n",
    "})\n",
    "team_matchups = team_games.merge(opp, on=[\"GAME_DATE\"], how=\"inner\")\n",
    "\n",
    "# 4Ô∏è‚É£ Train baseline Ridge model\n",
    "team_feature_cols = [c for c in team_matchups.columns if c.startswith((\"or\",\"dr\",\"pace\",\"team_PTS\",\"opp_\")) and c != \"team_pts_next\"]\n",
    "\n",
    "tm = team_matchups.dropna(subset=team_feature_cols + [\"team_pts_next\"]).copy()\n",
    "\n",
    "if not tm.empty:\n",
    "    X_tm = tm[team_feature_cols]\n",
    "    y_tm = tm[\"team_pts_next\"]\n",
    "    ridge = Ridge(alpha=5.0).fit(X_tm, y_tm)\n",
    "    print(f\"‚úÖ Team PTS baseline R¬≤: {ridge.score(X_tm, y_tm):.3f}  |  n={len(X_tm):,}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough data to train team baseline model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "565341f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagnostics: lineup blocks=12, player nodes=146\n",
      "‚úÖ Shape: (18, 11)\n",
      "game_time team side lineup_status                                                                starters     starter_1     starter_2     starter_3        starter_4     starter_5  lineup_confirmed\n",
      "               AWAY      EXPECTED    [Ryan Rollins, AJ Green, Gary Trent, G. Antetokounmpo, Myles Turner]  Ryan Rollins      AJ Green    Gary Trent G. Antetokounmpo  Myles Turner                 0\n",
      "               AWAY      EXPECTED  [D. DiVincenzo, Mike Conley, J. McDaniels, Julius Randle, Rudy Gobert] D. DiVincenzo   Mike Conley  J. McDaniels    Julius Randle   Rudy Gobert                 0\n",
      "               AWAY      EXPECTED   [K. George, S. Mykhailiuk, L. Markkanen, K. Filipowski, Jusuf Nurkic]     K. George S. Mykhailiuk  L. Markkanen    K. Filipowski  Jusuf Nurkic                 0\n",
      "               AWAY      EXPECTED         [CJ McCollum, K. George, K. Middleton, B. Coulibaly, Alex Sarr]   CJ McCollum     K. George  K. Middleton     B. Coulibaly     Alex Sarr                 0\n",
      "               AWAY      EXPECTED  [Cooper Flagg, Max Christie, Klay Thompson, P. Washington, D. Gafford]  Cooper Flagg  Max Christie Klay Thompson    P. Washington    D. Gafford                 0\n",
      "               AWAY      EXPECTED       [C. Cunningham, D. Robinson, A. Thompson, T. Harris, Jalen Duren] C. Cunningham   D. Robinson   A. Thompson        T. Harris   Jalen Duren                 0\n",
      "               AWAY      EXPECTED     [D. Schroder, R. Westbrook, Zach LaVine, DeMar DeRozan, D. Sabonis]   D. Schroder  R. Westbrook   Zach LaVine    DeMar DeRozan    D. Sabonis                 0\n",
      "               AWAY      EXPECTED    [Marcus Smart, Jake LaRavia, Rui Hachimura, J. Vanderbilt, D. Ayton]  Marcus Smart  Jake LaRavia Rui Hachimura    J. Vanderbilt      D. Ayton                 0\n",
      "               AWAY      EXPECTED          [D. Mitchell, N. Powell, A. Wiggins, Bam Adebayo, Kel'el Ware]   D. Mitchell     N. Powell    A. Wiggins      Bam Adebayo   Kel'el Ware                 0\n",
      "               HOME      EXPECTED   [Q. Jackson, Aaron Nesmith, Jarace Walker, Pascal Siakam, I. Jackson]    Q. Jackson Aaron Nesmith Jarace Walker    Pascal Siakam    I. Jackson                 0\n",
      "               HOME      EXPECTED    [Tyrese Martin, Cam Thomas, Terance Mann, Noah Clowney, Nic Claxton] Tyrese Martin    Cam Thomas  Terance Mann     Noah Clowney   Nic Claxton                 0\n",
      "               HOME      EXPECTED [Derrick White, P. Pritchard, Jaylen Brown, Josh Minott, Neemias Queta] Derrick White  P. Pritchard  Jaylen Brown      Josh Minott Neemias Queta                 0\n"
     ]
    }
   ],
   "source": [
    "# pip install selenium webdriver-manager bs4 pandas lxml\n",
    "\n",
    "import os, re, time, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "\n",
    "def _clean_list(xs):\n",
    "    return [re.sub(r\"\\s+\\(.*?\\)\\s*$\", \"\", x) for x in xs]\n",
    "\n",
    "def _try_click_consent(driver, timeout=6):\n",
    "    XPATHS = [\n",
    "        \"//button[contains(.,'Accept')]\",\n",
    "        \"//button[contains(.,'I Agree')]\",\n",
    "        \"//button[contains(.,'Agree')]\",\n",
    "        \"//button[contains(.,'ŒëœÄŒøŒ¥ŒøœáŒÆ')]\",\n",
    "        \"//button[contains(.,'Œ£œÖŒºœÜœâŒΩœé')]\",\n",
    "    ]\n",
    "    end = time.time() + timeout\n",
    "    for xp in XPATHS:\n",
    "        try:\n",
    "            btn = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.XPATH, xp)))\n",
    "            btn.click()\n",
    "            return True\n",
    "        except Exception:\n",
    "            if time.time() > end: break\n",
    "    return False\n",
    "\n",
    "def _progress_scroll(driver, steps=10, pause=0.8):\n",
    "    h = driver.execute_script(\"return document.body.scrollHeight || document.documentElement.scrollHeight;\")\n",
    "    for i in range(1, steps + 1):\n",
    "        y = int(h * i / steps)\n",
    "        driver.execute_script(f\"window.scrollTo(0, {y});\")\n",
    "        time.sleep(pause)\n",
    "\n",
    "def _extract_team(side):\n",
    "    team_el = side.select_one(\".lineup__abbr, .lineup__team-name, .lineup__name\")\n",
    "    if team_el:\n",
    "        return team_el.get_text(strip=True)\n",
    "    logo = side.select_one(\"img[alt]\")\n",
    "    return (logo.get(\"alt\") or \"\").strip() if logo else \"\"\n",
    "\n",
    "def _extract_status(side):\n",
    "    status_el = side.select_one(\".lineup__status\")\n",
    "    txt = (status_el.get_text(\" \", strip=True) if status_el else \"\").upper()\n",
    "    if \"CONFIRM\" in txt:  return \"CONFIRMED\"\n",
    "    if \"EXPECT\" in txt or \"PROBABLE\" in txt: return \"EXPECTED\"\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def _extract_starters(side):\n",
    "    # Try several variants for starters content\n",
    "    containers = side.select(\".lineup__list--starters, .lineup__list, .lineup__players\")\n",
    "    if not containers:\n",
    "        containers = [side]\n",
    "\n",
    "    names = []\n",
    "    for blk in containers:\n",
    "        for a in blk.select(\"a.lineup__player-link, .lineup__player a\"):\n",
    "            t = a.get_text(\" \", strip=True)\n",
    "            if t: names.append(t)\n",
    "        if not names:\n",
    "            for row in blk.select(\".lineup__player\"):\n",
    "                t = row.get_text(\" \", strip=True)\n",
    "                if re.match(r\"^(PG|SG|SF|PF|C)\\b\", t): names.append(t)\n",
    "        if not names:\n",
    "            for li in blk.select(\"li\"):\n",
    "                t = li.get_text(\" \", strip=True)\n",
    "                if re.match(r\"^(PG|SG|SF|PF|C)\\b\", t): names.append(t)\n",
    "\n",
    "    if not names:\n",
    "        txt = side.get_text(\"\\n\", strip=True)\n",
    "        names = re.findall(r\"(?:^|\\n)(?:PG|SG|SF|PF|C)\\s+[^\\n]+\", txt)\n",
    "\n",
    "    return _clean_list(names)[:5]\n",
    "\n",
    "# ---------------- main ----------------\n",
    "\n",
    "def fetch_rotowire_lineups_selenium(date: str | None = None,\n",
    "                                    wait_sec: float = 14.0,\n",
    "                                    headless: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Render Rotowire lineups & parse BOTH sides per game (global side selectors).\n",
    "    Returns:\n",
    "      game_time, team, side (AWAY/HOME), lineup_status, starters,\n",
    "      starter_1..starter_5, lineup_confirmed (0/1)\n",
    "    \"\"\"\n",
    "    base = \"https://www.rotowire.com/basketball/nba-lineups.php\"\n",
    "    url = base if not date else f\"{base}?date={date}\"\n",
    "\n",
    "    opts = Options()\n",
    "    if headless: opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--window-size=1400,1000\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_argument(\"--lang=en-US,en;q=0.9\")\n",
    "    opts.add_argument(\n",
    "        \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    driver.get(url)\n",
    "\n",
    "    _try_click_consent(driver, timeout=6)\n",
    "    time.sleep(1.2)\n",
    "    try:\n",
    "        WebDriverWait(driver, int(wait_sec)).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".lineup, .lineup.is-nba\"))\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    _progress_scroll(driver, steps=10, pause=0.8)\n",
    "    time.sleep(1.0)\n",
    "\n",
    "    # quick diagnostics\n",
    "    blocks = driver.find_elements(By.CSS_SELECTOR, \".lineup.is-nba, .lineup\")\n",
    "    players = driver.find_elements(By.CSS_SELECTOR, \".lineup__player, a.lineup__player-link\")\n",
    "    print(f\"diagnostics: lineup blocks={len(blocks)}, player nodes={len(players)}\")\n",
    "\n",
    "    html = driver.page_source\n",
    "    os.makedirs(\"_rotowire_debug\", exist_ok=True)\n",
    "    with open(\"_rotowire_debug/last_lineups.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    try:\n",
    "        driver.save_screenshot(\"_rotowire_debug/last_lineups.png\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    driver.quit()\n",
    "\n",
    "    # -------- parse globally by side classes ----------\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # game time map: find each game container time\n",
    "    game_time_map = {}\n",
    "    for gi, g in enumerate(soup.select(\".lineup__main, .lineup.is-nba, .lineup\")):\n",
    "        t = g.select_one(\".lineup__time, .game-time\")\n",
    "        game_time_map[id(g)] = t.get_text(strip=True) if t else \"\"\n",
    "\n",
    "    # Select **visit/away** & **home** side boxes explicitly\n",
    "    visit_sel = (\n",
    "        '[class*=\"lineup__box\"][class*=\"is-visit\"], '\n",
    "        '[class*=\"lineup__team\"][class*=\"is-visit\"], '\n",
    "        '[class*=\"lineup__side\"][class*=\"is-visit\"], '\n",
    "        '[class*=\"visit\"]'\n",
    "    )\n",
    "    home_sel = (\n",
    "        '[class*=\"lineup__box\"][class*=\"is-home\"], '\n",
    "        '[class*=\"lineup__team\"][class*=\"is-home\"], '\n",
    "        '[class*=\"lineup__side\"][class*=\"is-home\"], '\n",
    "        '[class*=\"home\"]'\n",
    "    )\n",
    "\n",
    "    visit_boxes = soup.select(visit_sel)\n",
    "    home_boxes  = soup.select(home_sel)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    def add_rows(boxes, side_label):\n",
    "        for box in boxes:\n",
    "            # nearest parent game container for time\n",
    "            parent = box.find_parent(lambda tag: tag.has_attr(\"class\") and any(\n",
    "                c in {\"lineup__main\",\"lineup\",\"lineup is-nba\"} for c in tag.get(\"class\", [])\n",
    "            ))\n",
    "            game_time = game_time_map.get(id(parent), \"\") if parent else \"\"\n",
    "            team = _extract_team(box)\n",
    "            starters = _extract_starters(box)\n",
    "            status = _extract_status(box)\n",
    "            if starters or team:\n",
    "                rows.append({\n",
    "                    \"game_time\": game_time,\n",
    "                    \"team\": team,\n",
    "                    \"side\": side_label,\n",
    "                    \"lineup_status\": status,\n",
    "                    \"starters\": starters,\n",
    "                    \"starter_1\": starters[0] if len(starters)>0 else None,\n",
    "                    \"starter_2\": starters[1] if len(starters)>1 else None,\n",
    "                    \"starter_3\": starters[2] if len(starters)>2 else None,\n",
    "                    \"starter_4\": starters[3] if len(starters)>3 else None,\n",
    "                    \"starter_5\": starters[4] if len(starters)>4 else None,\n",
    "                    \"lineup_confirmed\": int(status == \"CONFIRMED\"),\n",
    "                })\n",
    "\n",
    "    add_rows(visit_boxes, \"AWAY\")\n",
    "    add_rows(home_boxes,  \"HOME\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates(\n",
    "            subset=[\"game_time\",\"team\",\"side\",\"starter_1\",\"starter_2\",\"starter_3\",\"starter_4\",\"starter_5\"]\n",
    "        )\n",
    "        all_na = df[[\"starter_1\",\"starter_2\",\"starter_3\",\"starter_4\",\"starter_5\"]].isna().all(axis=1)\n",
    "        df = df[~all_na].reset_index(drop=True)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Parsed zero rows. Check _rotowire_debug/last_lineups.html & .png\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------- run it ----------\n",
    "df_lineups = fetch_rotowire_lineups_selenium(wait_sec=14.0, headless=False)\n",
    "print(\"‚úÖ Shape:\", df_lineups.shape)\n",
    "print(df_lineups.sort_values([\"game_time\",\"side\"]).head(12).to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89cbaedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOM diagnostics: {'lineup__teams': 9, 'ul.lineup__list': 18, 'ul.is-visit': 9, 'ul.is-home': 9, 'see-proj-minutes buttons': 18, 'header abbr': 0, 'header team': 0, 'player anchors': 146, 'MNP titles': 18}\n",
      "Fallback B: scanning all ul.lineup__list globally...\n",
      "‚Üí Parsed rows: 18\n",
      "\n",
      "‚úÖ Preview:\n",
      "game_time team side lineup_status  may_not_play_count     starter_1        starter_2     starter_3        starter_4     starter_5\n",
      "           DAL AWAY      EXPECTED                   9  Cooper Flagg     Max Christie Klay Thompson    P. Washington    D. Gafford\n",
      "           DET AWAY      EXPECTED                   8 C. Cunningham      D. Robinson   A. Thompson      Jalen Duren C. Cunningham\n",
      "           LAL AWAY      EXPECTED                  12  Marcus Smart     Jake LaRavia Rui Hachimura    J. Vanderbilt  Marcus Smart\n",
      "           MIA AWAY      EXPECTED                   9   D. Mitchell       A. Wiggins   Bam Adebayo      Kel'el Ware   D. Mitchell\n",
      "           MIL AWAY      EXPECTED                   6  Ryan Rollins         AJ Green    Gary Trent G. Antetokounmpo  Myles Turner\n",
      "           MIN AWAY      EXPECTED                   6 D. DiVincenzo      Mike Conley  J. McDaniels    Julius Randle   Rudy Gobert\n",
      "           SAC AWAY      EXPECTED                   7   D. Schroder     R. Westbrook   Zach LaVine    DeMar DeRozan    D. Sabonis\n",
      "           UTA AWAY      EXPECTED                   7     K. George    S. Mykhailiuk  L. Markkanen    K. Filipowski  Jusuf Nurkic\n",
      "           WAS AWAY      EXPECTED                   6   CJ McCollum        K. George  B. Coulibaly        Alex Sarr   CJ McCollum\n",
      "           BKN HOME      EXPECTED                   8 Tyrese Martin       Cam Thomas  Terance Mann     Noah Clowney   Nic Claxton\n",
      "           BOS HOME      EXPECTED                   6 Derrick White     P. Pritchard  Jaylen Brown      Josh Minott Neemias Queta\n",
      "           DEN HOME      EXPECTED                   7      C. Braun     Aaron Gordon  Nikola Jokic        J. Murray      C. Braun\n",
      "           HOU HOME      EXPECTED                   9 Amen Thompson      Josh Okogie  Kevin Durant        A. Sengun Amen Thompson\n",
      "           IND HOME      EXPECTED                  13    Q. Jackson    Aaron Nesmith Jarace Walker    Pascal Siakam    I. Jackson\n",
      "           LAC HOME      EXPECTED                   2  James Harden     Bradley Beal Kawhi Leonard    Derrick Jones   Ivica Zubac\n",
      "           MEM HOME      EXPECTED                   9     Ja Morant K. Caldwell-Pope  Jaylen Wells    Jaren Jackson  Jock Landale\n",
      "           NYK HOME      EXPECTED                   7 Jalen Brunson    Mikal Bridges    OG Anunoby      G. Yabusele      K. Towns\n",
      "           POR HOME      EXPECTED                  10  Jrue Holiday        T. Camara   Deni Avdija       D. Clingan  Jrue Holiday\n"
     ]
    }
   ],
   "source": [
    "# pip install bs4 lxml pandas\n",
    "import re, os, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def _txt(x):\n",
    "    return re.sub(r\"\\s+\", \" \", x.get_text(\" \", strip=True)) if x else \"\"\n",
    "\n",
    "def _clean_player(n):\n",
    "    if not n: return n\n",
    "    n = re.sub(r\"\\s+\\(.*?\\)\\s*$\", \"\", n).strip()\n",
    "    n = re.sub(r\"^(PG|SG|SF|PF|C)\\s+\", \"\", n, flags=re.I)\n",
    "    return n\n",
    "\n",
    "def _get_mnp_from_ul(ul):\n",
    "    \"\"\"Extract 'May Not Play' entries from a team UL.\"\"\"\n",
    "    mnp = []\n",
    "    # Strategy 1: find the title li inside this UL, then collect following player lis until next title\n",
    "    title = ul.find(\"li\", class_=lambda c: c and \"lineup__title\" in c and re.search(r\"may\\s+not\\s+play\", _txt(ul.find(\"li\", class_=c)) if ul.find(\"li\", class_=c) else \"\", re.I))\n",
    "    if title:\n",
    "        for li in title.find_all_next(\"li\"):\n",
    "            # stop if next section title\n",
    "            if \"lineup__title\" in (li.get(\"class\") or []):\n",
    "                break\n",
    "            if \"lineup__player\" in (li.get(\"class\") or []):\n",
    "                a = li.select_one(\"a\")\n",
    "                tag = li.select_one(\".lineup__inj\")\n",
    "                nm = _txt(a) if a else \"\"\n",
    "                if nm:\n",
    "                    mnp.append(f\"{nm} ({_txt(tag)})\" if tag else nm)\n",
    "        # normalize\n",
    "        return [_clean_player(x) for x in mnp if x and x.lower() != \"none\"]\n",
    "\n",
    "    # Strategy 2: common MNP containers inside UL\n",
    "    for li in ul.select(\".lineup__notplay li, .lineup__status--out, .lineup__inj-list li\"):\n",
    "        nm = _txt(li)\n",
    "        if nm: mnp.append(_clean_player(nm))\n",
    "    return [x for x in mnp if x and x.lower() != \"none\"]\n",
    "\n",
    "def _extract_starters_from_ul(ul):\n",
    "    \"\"\"Try multiple ways to get five starters out of a team UL.\"\"\"\n",
    "    names = []\n",
    "    # Most reliable: 100% rows\n",
    "    for li in ul.select(\"li.lineup__player.is-pct-play-100 a\"):\n",
    "        nm = _txt(li)\n",
    "        if nm: names.append(nm)\n",
    "    # Fallback: any lineup__player anchors in first list group\n",
    "    if len(names) < 5:\n",
    "        for li in ul.select(\"li.lineup__player a\"):\n",
    "            nm = _txt(li)\n",
    "            if nm: names.append(nm)\n",
    "            if len(names) >= 5: break\n",
    "    # Final cleanup + trim\n",
    "    names = [_clean_player(n) for n in names]\n",
    "    return names[:5]\n",
    "\n",
    "def _lineup_status(ul):\n",
    "    st = _txt(ul.select_one(\".lineup__status\"))\n",
    "    stU = st.upper()\n",
    "    if \"CONFIRM\" in stU: return \"CONFIRMED\"\n",
    "    if \"EXPECT\" in stU or \"PROBABLE\" in stU: return \"EXPECTED\"\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def parse_rotowire_lineups_flexible(html_path: str) -> pd.DataFrame:\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # --- Diagnostics to understand the DOM you have ---\n",
    "    diag = {\n",
    "        \"lineup__teams\": len(soup.select(\"div.lineup__teams\")),\n",
    "        \"ul.lineup__list\": len(soup.select(\"ul.lineup__list\")),\n",
    "        \"ul.is-visit\": len(soup.select(\"ul.lineup__list.is-visit\")),\n",
    "        \"ul.is-home\": len(soup.select(\"ul.lineup__list.is-home\")),\n",
    "        \"see-proj-minutes buttons\": len(soup.select(\"button.see-proj-minutes\")),\n",
    "        \"header abbr\": len(soup.select(\".lineup__hdr .lineup__abbr\")),\n",
    "        \"header team\": len(soup.select(\".lineup__hdr .lineup__team\")),\n",
    "        \"player anchors\": len(soup.select(\"a.lineup__player-link, .lineup__player a\")),\n",
    "        \"MNP titles\": len(soup.find_all(string=re.compile(r\"^\\s*may\\s+not\\s+play\\s*$\", re.I))),\n",
    "    }\n",
    "    print(\"DOM diagnostics:\", diag)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # ========== STRATEGY A: by matchup blocks ==========\n",
    "    for teams_div in soup.select(\"div.lineup__teams\"):\n",
    "        # game time near this block (looks upward for a sibling header)\n",
    "        time_el = teams_div.find_previous(\"div\", class_=\"lineup__time\")\n",
    "        game_time = _txt(time_el)\n",
    "\n",
    "        # find both team ULs inside this matchup\n",
    "        uls = teams_div.select(\"ul.lineup__list\")\n",
    "        if len(uls) < 1:\n",
    "            continue\n",
    "\n",
    "        # Try to pair AWAY then HOME by class flags; else preserve order\n",
    "        away_ul = None\n",
    "        home_ul = None\n",
    "        for ul in uls:\n",
    "            classes = \" \".join(ul.get(\"class\", [])).lower()\n",
    "            if \"is-visit\" in classes or \"visit\" in classes or \"away\" in classes:\n",
    "                away_ul = ul\n",
    "            if \"is-home\" in classes or \"home\" in classes:\n",
    "                home_ul = home_ul or ul  # keep the first\n",
    "\n",
    "        if away_ul is None and home_ul is None and len(uls) >= 2:\n",
    "            away_ul, home_ul = uls[0], uls[1]\n",
    "        elif away_ul is None and len(uls) >= 1:\n",
    "            away_ul = uls[0]\n",
    "        elif home_ul is None and len(uls) >= 2:\n",
    "            # pick the other UL as home\n",
    "            home_ul = next((u for u in uls if u is not away_ul), None)\n",
    "\n",
    "        pairs = [(\"AWAY\", away_ul), (\"HOME\", home_ul)]\n",
    "        # Extract team code (prefer button data-team; else header abbrs in the same matchup)\n",
    "        header_abbrs = [ _txt(el) for el in teams_div.select(\".lineup__abbr\") if _txt(el) ]\n",
    "        # If header not inside teams_div, try its parent block\n",
    "        if not header_abbrs:\n",
    "            parent_main = teams_div.find_parent([\"div\",\"section\"])\n",
    "            if parent_main:\n",
    "                header_abbrs = [ _txt(el) for el in parent_main.select(\".lineup__abbr\") if _txt(el) ]\n",
    "\n",
    "        for idx, (side, ul) in enumerate(pairs):\n",
    "            if not ul: continue\n",
    "            btn = ul.select_one(\"button.see-proj-minutes\")\n",
    "            team = btn[\"data-team\"].strip().upper() if btn and btn.has_attr(\"data-team\") else None\n",
    "            if not team and header_abbrs and idx < len(header_abbrs):\n",
    "                team = header_abbrs[idx].upper()\n",
    "\n",
    "            starters = _extract_starters_from_ul(ul)\n",
    "            mnp = _get_mnp_from_ul(ul)\n",
    "            status = _lineup_status(ul)\n",
    "\n",
    "            # Only add if we have at least a team or any player info\n",
    "            if team or starters or mnp:\n",
    "                rows.append({\n",
    "                    \"game_time\": game_time,\n",
    "                    \"team\": team,\n",
    "                    \"side\": side,\n",
    "                    \"lineup_status\": status,\n",
    "                    \"starters\": starters,\n",
    "                    \"may_not_play\": mnp,\n",
    "                    \"may_not_play_count\": len(mnp),\n",
    "                    \"lineup_confirmed\": int(status == \"CONFIRMED\"),\n",
    "                })\n",
    "\n",
    "    # ========== STRATEGY B: fall back to any lineup ULs globally ==========\n",
    "    if not rows:\n",
    "        print(\"Fallback B: scanning all ul.lineup__list globally...\")\n",
    "        for ul in soup.select(\"ul.lineup__list\"):\n",
    "            # Guess side by class or position among siblings\n",
    "            side = \"AWAY\" if \"is-visit\" in (ul.get(\"class\") or []) else (\"HOME\" if \"is-home\" in (ul.get(\"class\") or []) else None)\n",
    "            # Team from button\n",
    "            btn = ul.select_one(\"button.see-proj-minutes\")\n",
    "            team = btn[\"data-team\"].strip().upper() if btn and btn.has_attr(\"data-team\") else None\n",
    "            starters = _extract_starters_from_ul(ul)\n",
    "            mnp = _get_mnp_from_ul(ul)\n",
    "            status = _lineup_status(ul)\n",
    "\n",
    "            if side and (team or starters or mnp):\n",
    "                rows.append({\n",
    "                    \"game_time\": \"\",  # unknown at this scope\n",
    "                    \"team\": team,\n",
    "                    \"side\": side,\n",
    "                    \"lineup_status\": status,\n",
    "                    \"starters\": starters,\n",
    "                    \"may_not_play\": mnp,\n",
    "                    \"may_not_play_count\": len(mnp),\n",
    "                    \"lineup_confirmed\": int(status == \"CONFIRMED\"),\n",
    "                })\n",
    "\n",
    "    # ========== STRATEGY C: header-driven pairing (very defensive) ==========\n",
    "    if not rows:\n",
    "        print(\"Fallback C: pairing by header labels and nearest lists...\")\n",
    "        for block in soup.select(\".lineup, .lineup__main\"):\n",
    "            hdr = block.select(\".lineup__hdr .lineup__abbr, .lineup__hdr .lineup__team\")\n",
    "            labels = [ _txt(x) for x in hdr if _txt(x) ]\n",
    "            if len(labels) < 2:\n",
    "                continue\n",
    "            away_label, home_label = labels[:2]\n",
    "            lists = block.select(\"ul.lineup__list\")\n",
    "            if len(lists) < 2:\n",
    "                continue\n",
    "            for side, lab, ul in [(\"AWAY\", away_label, lists[0]), (\"HOME\", home_label, lists[1])]:\n",
    "                starters = _extract_starters_from_ul(ul)\n",
    "                mnp = _get_mnp_from_ul(ul)\n",
    "                status = _lineup_status(ul)\n",
    "                rows.append({\n",
    "                    \"game_time\": _txt(block.select_one(\".lineup__time, .game-time\")),\n",
    "                    \"team\": lab.upper(),\n",
    "                    \"side\": side,\n",
    "                    \"lineup_status\": status,\n",
    "                    \"starters\": starters,\n",
    "                    \"may_not_play\": mnp,\n",
    "                    \"may_not_play_count\": len(mnp),\n",
    "                    \"lineup_confirmed\": int(status == \"CONFIRMED\"),\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Expand starters to columns for easier merging\n",
    "    for i in range(5):\n",
    "        col = f\"starter_{i+1}\"\n",
    "        df[col] = df[\"starters\"].apply(lambda xs: xs[i] if isinstance(xs, list) and len(xs) > i else None)\n",
    "\n",
    "    print(f\"‚Üí Parsed rows: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "# ---- RUN IT (point to your saved file) ----\n",
    "HTML_PATH = \"_rotowire_debug/last_lineups.html\"  # change if needed\n",
    "if not os.path.exists(HTML_PATH):\n",
    "    # if you uploaded as 'last_lineups.html' in current directory\n",
    "    if os.path.exists(\"last_lineups.html\"):\n",
    "        HTML_PATH = \"last_lineups.html\"\n",
    "\n",
    "df_lineups = parse_rotowire_lineups_flexible(HTML_PATH)\n",
    "\n",
    "# Safe display\n",
    "if df_lineups.empty:\n",
    "    print(\"\\n‚ö†Ô∏è Still empty. Please share the values printed in 'DOM diagnostics' (above).\")\n",
    "else:\n",
    "    cols = [\"game_time\",\"team\",\"side\",\"lineup_status\",\"may_not_play_count\",\n",
    "            \"starter_1\",\"starter_2\",\"starter_3\",\"starter_4\",\"starter_5\"]\n",
    "    print(\"\\n‚úÖ Preview:\")\n",
    "    print(df_lineups[cols].sort_values([\"game_time\",\"side\",\"team\"], na_position=\"last\").to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 games in HTML.\n",
      "‚úÖ Parsed 56 'May Not Play' players across 18 teams.\n",
      "  game_time team side position        player status            title_text  likelihood_pct\n",
      "10:00 PM ET  LAL AWAY        G     A. Reaves    Out Very Unlikely To Play               0\n",
      "10:00 PM ET  LAL AWAY        F     A. Thiero    Out Very Unlikely To Play               0\n",
      "10:00 PM ET  LAL AWAY        C      D. Ayton   Ques       Toss Up To Play              50\n",
      "10:00 PM ET  LAL AWAY        G    G. Vincent    Out Very Unlikely To Play               0\n",
      "10:00 PM ET  LAL AWAY        G     L. Doncic    Out Very Unlikely To Play               0\n",
      "10:00 PM ET  LAL AWAY        F      L. James    Out Very Unlikely To Play               0\n",
      "10:00 PM ET  LAL AWAY        C     M. Kleber   Ques       Toss Up To Play              50\n",
      "10:00 PM ET  POR HOME        G     B. Wesley    Out Very Unlikely To Play               0\n",
      "10:00 PM ET  POR HOME        G    D. Lillard    OFS   Very Likely To Play               0\n",
      "10:00 PM ET  POR HOME        F   M. Thybulle    Out Very Unlikely To Play               0\n",
      "10:00 PM ET  POR HOME        G  S. Henderson    Out Very Unlikely To Play               0\n",
      "10:00 PM ET  POR HOME        G     S. Sharpe   Prob        Likely To Play              75\n",
      "10:30 PM ET  MIA AWAY        G K. Jakucionis    Out Very Unlikely To Play               0\n",
      "10:30 PM ET  MIA AWAY        G     N. Powell   Ques       Toss Up To Play              50\n",
      "10:30 PM ET  MIA AWAY        G      T. Herro    Out Very Unlikely To Play               0\n",
      "10:30 PM ET  MIA AWAY        G     T. Rozier    Out Very Unlikely To Play               0\n",
      "10:30 PM ET  LAC HOME        G     J. Miller    Out Very Unlikely To Play               0\n",
      "10:30 PM ET  LAC HOME        F    K. Sanders    Out Very Unlikely To Play               0\n",
      " 7:00 PM ET  MIL AWAY        G     K. Porter    Out Very Unlikely To Play               0\n",
      " 7:00 PM ET  MIN AWAY        G    A. Edwards    Out Very Unlikely To Play               0\n",
      " 7:00 PM ET  BKN HOME        F     D. Powell    Out Very Unlikely To Play               0\n",
      " 7:00 PM ET  BKN HOME        F  H. Highsmith    Out Very Unlikely To Play               0\n",
      " 7:00 PM ET  BKN HOME        F     M. Porter    Out Very Unlikely To Play               0\n",
      " 7:00 PM ET  IND HOME        G   A. Nembhard    Out Very Unlikely To Play               0\n",
      " 7:00 PM ET  IND HOME        F   B. Mathurin    Out Very Unlikely To Play               0\n",
      " 7:00 PM ET  IND HOME        F     J. Furphy   Prob        Likely To Play              75\n",
      " 7:00 PM ET  IND HOME        G     Kam Jones    Out Very Unlikely To Play               0\n",
      " 7:00 PM ET  IND HOME        F    Obi Toppin    Out Very Unlikely To Play               0\n",
      " 7:00 PM ET  IND HOME        G     R. Dennis   Prob        Likely To Play              75\n",
      " 7:00 PM ET  IND HOME        G T. Haliburton    OFS   Very Likely To Play               0\n",
      "\n",
      "Saved: may_not_play_players.csv\n"
     ]
    }
   ],
   "source": [
    "# pip install bs4 lxml pandas\n",
    "import os, re, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "HTML_PATH = \"_rotowire_debug/last_lineups.html\" if os.path.exists(\"_rotowire_debug/last_lineups.html\") else \"last_lineups.html\"\n",
    "\n",
    "LIKELIHOOD_MAP = {\n",
    "    \"is-pct-play-100\": 100, \"is-pct-play-90\": 90, \"is-pct-play-75\": 75,\n",
    "    \"is-pct-play-60\": 60, \"is-pct-play-50\": 50, \"is-pct-play-40\": 40,\n",
    "    \"is-pct-play-25\": 25, \"is-pct-play-10\": 10, \"is-pct-play-0\": 0\n",
    "}\n",
    "\n",
    "def _txt(node): return re.sub(r\"\\s+\", \" \", node.get_text(\" \", strip=True)) if node else \"\"\n",
    "def _likelihood(classes): \n",
    "    for c in classes: \n",
    "        if c in LIKELIHOOD_MAP: \n",
    "            return LIKELIHOOD_MAP[c]\n",
    "    return None\n",
    "\n",
    "def parse_rotowire_mnp_final(html_path: str) -> pd.DataFrame:\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "\n",
    "    rows = []\n",
    "    games = soup.select(\"div.lineup.is-nba[data-lnum]\")\n",
    "    print(f\"Found {len(games)} games in HTML.\")\n",
    "\n",
    "    for game in games:\n",
    "        game_time = _txt(game.select_one(\".lineup__time\"))\n",
    "        team_blocks = game.select(\".lineup__team\")\n",
    "        teams = []\n",
    "        for tb in team_blocks:\n",
    "            abbr = _txt(tb.select_one(\".lineup__abbr\"))\n",
    "            side = \"AWAY\" if \"is-visit\" in tb.get(\"class\", []) else \"HOME\" if \"is-home\" in tb.get(\"class\", []) else None\n",
    "            teams.append((abbr, side))\n",
    "\n",
    "        ul_lists = game.select(\"ul.lineup__list\")\n",
    "        for idx, ul in enumerate(ul_lists):\n",
    "            if idx >= len(teams):  # mismatch safety\n",
    "                continue\n",
    "            team, side = teams[idx]\n",
    "            mnp_title = ul.find(\"li\", class_=\"lineup__title\", string=lambda s: s and \"MAY NOT PLAY\" in s.upper())\n",
    "            if not mnp_title:\n",
    "                continue\n",
    "\n",
    "            for li in mnp_title.find_next_siblings(\"li\"):\n",
    "                classes = li.get(\"class\") or []\n",
    "                if \"lineup__title\" in classes:\n",
    "                    break\n",
    "                if \"lineup__player\" not in classes:\n",
    "                    continue\n",
    "\n",
    "                pos = _txt(li.select_one(\".lineup__pos\"))\n",
    "                a = li.select_one(\"a\")\n",
    "                player = _txt(a)\n",
    "                if not player:\n",
    "                    continue\n",
    "\n",
    "                status = _txt(li.select_one(\".lineup__inj\"))\n",
    "                title_text = (li.get(\"title\") or \"\").strip()\n",
    "                likelihood_pct = _likelihood(classes)\n",
    "\n",
    "                rows.append({\n",
    "                    \"game_time\": game_time,\n",
    "                    \"team\": team,\n",
    "                    \"side\": side,\n",
    "                    \"position\": pos,\n",
    "                    \"player\": player,\n",
    "                    \"status\": status,\n",
    "                    \"title_text\": title_text,\n",
    "                    \"likelihood_pct\": likelihood_pct\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No 'May Not Play' players found. Check if Rotowire changed markup.\")\n",
    "    else:\n",
    "        df = df.sort_values([\"game_time\",\"side\",\"team\",\"player\"]).reset_index(drop=True)\n",
    "        print(f\"‚úÖ Parsed {len(df)} 'May Not Play' players across {df['team'].nunique()} teams.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---- RUN ----\n",
    "mnp_df = parse_rotowire_mnp_final(HTML_PATH)\n",
    "if not mnp_df.empty:\n",
    "    print(mnp_df.head(30).to_string(index=False))\n",
    "    mnp_df.to_csv(os.path.join(FOLDER_EVALS, f\"value_bets_top100_{TODAY}_EVAL.csv\"), index=False)\n",
    "    print(\"\\nSaved: may_not_play_players.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfd44494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def american_to_prob(odds):\n",
    "    if pd.isna(odds): return np.nan\n",
    "    o = float(odds)\n",
    "    return 100.0/(o+100.0) if o>0 else (-o)/(-o+100.0)\n",
    "\n",
    "def devig_pair(p_over, p_under):\n",
    "    if pd.isna(p_over) or pd.isna(p_under): return (np.nan, np.nan)\n",
    "    s = p_over + p_under\n",
    "    if s <= 0: return (np.nan, np.nan)\n",
    "    return (p_over/s, p_under/s)\n",
    "\n",
    "def kelly_fraction(p, american_odds, cap=0.25):\n",
    "    if pd.isna(p) or pd.isna(american_odds): return 0.0\n",
    "    o = float(american_odds)\n",
    "    b = o/100.0 if o>0 else 100.0/(-o)\n",
    "    f = (p*(b+1)-1)/b\n",
    "    return float(max(0.0, min(f, cap)))\n",
    "\n",
    "def ev_flat_over(p, american_odds):\n",
    "    if pd.isna(p) or pd.isna(american_odds): return np.nan\n",
    "    o = float(american_odds)\n",
    "    win = o/100.0 if o>0 else 100.0/(-o)\n",
    "    lose = 1.0\n",
    "    return p*win - (1-p)*lose\n",
    "\n",
    "# Normal CDF helper (if SciPy available) to turn mean/sd into p_over\n",
    "try:\n",
    "    from scipy.stats import norm\n",
    "    def p_over_from_normal(mu, sd, line):\n",
    "        if pd.isna(mu) or pd.isna(sd) or pd.isna(line) or sd <= 0: return np.nan\n",
    "        return 1.0 - norm.cdf((line - mu)/sd)\n",
    "except Exception:\n",
    "    def p_over_from_normal(mu, sd, line): return np.nan\n",
    "\n",
    "def build_value_bets_excel(\n",
    "    df_projections, df_odds, outfile_path=None,\n",
    "    join_keys=(\"player\",\"team\",\"opponent\",\"market\",\"line\",\"book\",\"game_date\"),\n",
    "    cap_kelly=0.25\n",
    "):\n",
    "    def _norm(x): return None if pd.isna(x) else str(x).strip()\n",
    "    proj, odds = df_projections.copy(), df_odds.copy()\n",
    "    for k in join_keys:\n",
    "        if k in proj: proj[k] = proj[k].map(_norm)\n",
    "        if k in odds: odds[k] = odds[k].map(_norm)\n",
    "\n",
    "    merged = proj.merge(odds, on=list(join_keys), how=\"inner\", suffixes=(\"\", \"_odds\"))\n",
    "\n",
    "    if \"p_over_model\" not in merged.columns or merged[\"p_over_model\"].isna().all():\n",
    "        merged[\"p_over_model\"] = merged.apply(\n",
    "            lambda r: p_over_from_normal(r.get(\"projection_mean\"), r.get(\"projection_sd\"), r.get(\"line\")), axis=1\n",
    "        )\n",
    "\n",
    "    merged[\"p_over_imp\"]  = merged[\"over_odds\"].map(american_to_prob)\n",
    "    merged[\"p_under_imp\"] = merged[\"under_odds\"].map(american_to_prob)\n",
    "    merged[[\"p_over_fair\",\"p_under_fair\"]] = merged.apply(\n",
    "        lambda r: pd.Series(devig_pair(r[\"p_over_imp\"], r[\"p_under_imp\"])), axis=1\n",
    "    )\n",
    "\n",
    "    merged[\"edge_over\"]       = merged[\"p_over_model\"] - merged[\"p_over_fair\"]\n",
    "    merged[\"kelly_frac_over\"] = merged.apply(lambda r: kelly_fraction(r[\"p_over_model\"], r[\"over_odds\"], cap=cap_kelly), axis=1)\n",
    "    merged[\"EV_over_1u\"]      = merged.apply(lambda r: ev_flat_over(r[\"p_over_model\"], r[\"over_odds\"]), axis=1)\n",
    "    merged[\"asof_date\"]       = merged.get(\"asof_date\") if \"asof_date\" in merged else datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    preferred = [\n",
    "        \"asof_date\",\"game_date\",\"book\",\"player\",\"team\",\"opponent\",\"market\",\"line\",\"lineup_status\",\n",
    "        \"over_odds\",\"under_odds\",\"p_over_imp\",\"p_under_imp\",\"p_over_fair\",\"p_under_fair\",\"p_over_model\",\n",
    "        \"edge_over\",\"kelly_frac_over\",\"EV_over_1u\",\n",
    "        \"projected_minutes\",\"projection_mean\",\"projection_sd\",\"start_prob\",\n",
    "        \"opponent_allowance_idx\",\"team_orating\",\"opp_drating\",\n",
    "    ]\n",
    "    cols = [c for c in preferred if c in merged.columns] + [c for c in merged.columns if c not in preferred]\n",
    "    bets = merged[cols].sort_values([\"edge_over\",\"EV_over_1u\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        \"n_bets\":[len(bets)],\n",
    "        \"avg_edge_pp\":[bets[\"edge_over\"].mean()*100.0 if len(bets) else np.nan],\n",
    "        \"avg_kelly_pct\":[bets[\"kelly_frac_over\"].mean()*100.0 if len(bets) else np.nan],\n",
    "        \"avg_ev_1u\":[bets[\"EV_over_1u\"].mean() if len(bets) else np.nan],\n",
    "    })\n",
    "    by_market = bets.groupby(\"market\", dropna=False).agg(\n",
    "        n=(\"player\",\"count\"),\n",
    "        avg_edge_pp=(\"edge_over\", lambda x: 100.0*x.mean()),\n",
    "        avg_kelly_pct=(\"kelly_frac_over\", lambda x: 100.0*x.mean()),\n",
    "        avg_ev_1u=(\"EV_over_1u\",\"mean\")\n",
    "    ).reset_index()\n",
    "    by_book = bets.groupby(\"book\", dropna=False).agg(\n",
    "        n=(\"player\",\"count\"),\n",
    "        avg_edge_pp=(\"edge_over\", lambda x: 100.0*x.mean()),\n",
    "        avg_ev_1u=(\"EV_over_1u\",\"mean\")\n",
    "    ).reset_index()\n",
    "\n",
    "    if outfile_path is None:\n",
    "        outfile_path = f\"nba_value_bets_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "    with pd.ExcelWriter(outfile_path, engine=\"openpyxl\") as w:\n",
    "        bets.to_excel(w, sheet_name=\"Bets\", index=False)\n",
    "        summary.to_excel(w, sheet_name=\"Summary\", index=False, startrow=0)\n",
    "        by_market.to_excel(w, sheet_name=\"Summary\", index=False, startrow=5)\n",
    "        by_book.to_excel(w, sheet_name=\"Summary\", index=False, startrow=5+len(by_market)+3)\n",
    "\n",
    "        dd = pd.DataFrame([\n",
    "            (\"asof_date\",\"UTC run date\"), (\"game_date\",\"Game date\"),\n",
    "            (\"player\",\"Player\"), (\"team\",\"Team abbr\"), (\"opponent\",\"Opponent abbr\"),\n",
    "            (\"market\",\"PTS/REB/AST/3PM/PRA etc.\"), (\"line\",\"Book line\"), (\"book\",\"Sportsbook id\"),\n",
    "            (\"lineup_status\",\"EXPECTED/CONFIRMED/UNKNOWN\"),\n",
    "            (\"over_odds\",\"American odds Over\"), (\"under_odds\",\"American odds Under\"),\n",
    "            (\"p_over_imp\",\"Implied prob Over (pre-vig)\"), (\"p_under_imp\",\"Implied prob Under (pre-vig)\"),\n",
    "            (\"p_over_fair\",\"De-vigged prob Over\"), (\"p_under_fair\",\"De-vigged prob Under\"),\n",
    "            (\"p_over_model\",\"Model prob Over\"), (\"edge_over\",\"p_model ‚àí p_fair\"),\n",
    "            (\"kelly_frac_over\",\"Kelly fraction (cap)\"), (\"EV_over_1u\",\"EV if staking 1u\"),\n",
    "            (\"projected_minutes\",\"Projected minutes\"), (\"projection_mean\",\"Projected mean\"),\n",
    "            (\"projection_sd\",\"Projected stdev\"), (\"start_prob\",\"Start probability\"),\n",
    "            (\"opponent_allowance_idx\",\"Opponent allowance index\"),\n",
    "            (\"team_orating\",\"Team ORtg\"), (\"opp_drating\",\"Opponent DRtg\"),\n",
    "        ], columns=[\"column\",\"description\"])\n",
    "        dd.to_excel(w, sheet_name=\"Data_Dictionary\", index=False)\n",
    "\n",
    "    return bets, outfile_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a65c8c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 16: raw wide odds + resilient numeric parsing ===\n",
    "import re, json, pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def _first_numeric_float(x):\n",
    "    \"\"\"Return the first decimal number in x (e.g., '23.5, 24.5' -> 23.5).\"\"\"\n",
    "    if x is None: return None\n",
    "    s = str(x)\n",
    "    m = re.search(r\"[-+]?\\d+(?:\\.\\d+)?\", s)\n",
    "    return float(m.group()) if m else None\n",
    "\n",
    "def _first_numeric_int(x):\n",
    "    \"\"\"Return the first integer in x (e.g., '+110, +105' -> 110).\"\"\"\n",
    "    if x is None: return None\n",
    "    s = str(x)\n",
    "    m = re.search(r\"[-+]?\\d+\", s)\n",
    "    return int(m.group()) if m else None\n",
    "\n",
    "# override the helpers used by 16d converter (if defined)\n",
    "def _to_float_or_none(x):  # noqa: F811\n",
    "    return _first_numeric_float(x)\n",
    "\n",
    "def _to_int_or_none(x):    # noqa: F811\n",
    "    return _first_numeric_int(x)\n",
    "\n",
    "def get_player_props_odds_wide_raw(self, book=\"mgm\"):\n",
    "    \"\"\"\n",
    "    Returns the raw 'wide' odds table from Rotowire (no grouping, no aggregation).\n",
    "    Contains columns like mgm_pts, mgm_ptsOver, mgm_ptsUnder, etc.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.rotowire.com/betting/nba/player-props.php?book={book}\"\n",
    "    r = self.session.get(url, headers=self.headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    matches = re.findall(r'data:\\s*(\\[\\{.*?\\}\\])', r.text, flags=re.DOTALL)\n",
    "    frames = []\n",
    "    for blob in matches:\n",
    "        try:\n",
    "            frames.append(pd.DataFrame(json.loads(blob)))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "    # concat all blocks without grouping to preserve raw book columns\n",
    "    wide_raw = pd.concat(frames, ignore_index=True)\n",
    "    return wide_raw\n",
    "\n",
    "# attach to your scraper class\n",
    "NBAOddsAndLineupsScraper.get_player_props_odds_wide_raw = get_player_props_odds_wide_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70c82f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Projection rows by market: {'PTS': 693, 'REB': 693, 'AST': 693}\n"
     ]
    }
   ],
   "source": [
    "# === Cell 16 (Upgraded): projections for PTS/REB/AST using trained XGB models ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Detect model dictionary\n",
    "if \"models_mean\" in globals() and models_mean:\n",
    "    active_models = models_mean\n",
    "    feature_dict = feature_bags\n",
    "elif \"models\" in globals() and models:\n",
    "    active_models = models\n",
    "    feature_dict = feature_cols_by_stat\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå No trained models found. Run Cell 8 first to populate models_mean and feature_bags.\")\n",
    "\n",
    "MARKETS = [\"PTS\", \"REB\", \"AST\"]\n",
    "\n",
    "# 1Ô∏è‚É£ Latest row per player = baseline for next game prediction\n",
    "latest = (\n",
    "    features_all.sort_values([\"PLAYER_NAME\", \"GAME_DATE\"])\n",
    "    .groupby(\"PLAYER_NAME\")\n",
    "    .tail(1)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Estimate per-player variability (standard deviation of recent games)\n",
    "def _player_sd_map(stat: str, n=10):\n",
    "    def _sd(g):\n",
    "        s = g[stat].tail(n)\n",
    "        if s.notna().sum() >= 4:\n",
    "            return float(s.std(ddof=1))\n",
    "        return float(features_all[stat].std(ddof=1))\n",
    "    return features_all.groupby(\"PLAYER_NAME\").apply(_sd)\n",
    "\n",
    "# 3Ô∏è‚É£ Common export keys\n",
    "base_out = pd.DataFrame({\n",
    "    \"player\": latest[\"PLAYER_NAME\"],\n",
    "    \"team\": latest[\"TEAM_ABBREVIATION\"],\n",
    "    \"opponent\": latest.get(\"OPPONENT_ABBREVIATION\", \"TBD\"),\n",
    "    \"game_date\": datetime.utcnow().strftime(\"%Y-%m-%d\"),\n",
    "    \"projected_minutes\": latest.get(\"MIN_roll5\", 30).fillna(30).clip(lower=10, upper=40).values,\n",
    "    \"start_prob\": 0.90,\n",
    "    \"lineup_status\": \"EXPECTED\"\n",
    "})\n",
    "\n",
    "proj_frames = {}\n",
    "\n",
    "# 4Ô∏è‚É£ Generate predictions per market\n",
    "for stat in MARKETS:\n",
    "    if stat not in active_models:\n",
    "        print(f\"‚ö†Ô∏è Skipping {stat}: model not found.\")\n",
    "        continue\n",
    "\n",
    "    feat_cols = feature_dict.get(stat, [])\n",
    "    if not feat_cols:\n",
    "        print(f\"‚ö†Ô∏è Skipping {stat}: no feature list recorded.\")\n",
    "        continue\n",
    "\n",
    "    X_pred = latest[feat_cols].fillna(0)\n",
    "    pred_mean = active_models[stat].predict(X_pred)\n",
    "\n",
    "    # Per-player SD (fallback if not enough data)\n",
    "    sd_map = _player_sd_map(stat)\n",
    "    pred_sd = latest[\"PLAYER_NAME\"].map(sd_map)\n",
    "    sd_fallback = np.maximum(np.abs(pred_mean) * 0.15, 1.0)\n",
    "    pred_sd = np.where(np.isnan(pred_sd), sd_fallback, pred_sd)\n",
    "\n",
    "    dfp = base_out.copy()\n",
    "    dfp[\"projection_mean\"] = pred_mean\n",
    "    dfp[\"projection_sd\"] = pred_sd\n",
    "    dfp[\"market\"] = stat\n",
    "\n",
    "    proj_frames[stat] = dfp[\n",
    "        [\"player\",\"team\",\"opponent\",\"game_date\",\"market\",\n",
    "         \"projection_mean\",\"projection_sd\",\"projected_minutes\",\n",
    "         \"start_prob\",\"lineup_status\"]\n",
    "    ].copy()\n",
    "\n",
    "# 5Ô∏è‚É£ Combined projections\n",
    "df_projections_pts = proj_frames.get(\"PTS\", pd.DataFrame())\n",
    "df_projections_reb = proj_frames.get(\"REB\", pd.DataFrame())\n",
    "df_projections_ast = proj_frames.get(\"AST\", pd.DataFrame())\n",
    "df_projections_all = (\n",
    "    pd.concat(list(proj_frames.values()), ignore_index=True)\n",
    "    if proj_frames else pd.DataFrame()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Projection rows by market:\",\n",
    "      {k: len(v) for k, v in proj_frames.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ac17011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: turn wide props (per-book columns) into a long, tidy table ---\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def odds_wide_to_long_from_columns(\n",
    "    wide_df: pd.DataFrame,\n",
    "    *,\n",
    "    books: tuple[str, ...] = (\"mgm\",\"draftkings\",\"fanduel\",\"caesars\",\"betrivers\",\"espnbet\",\"hardrock\"),\n",
    "    markets: tuple[str, ...] = (\"PTS\",\"REB\",\"AST\"),\n",
    "    player_cols=(\"name\",\"player\",\"PLAYER_NAME\"),\n",
    "    team_cols=(\"team\",\"TEAM\",\"team_name\",\"TEAM_ABBREVIATION\"),\n",
    "    opp_cols=(\"opponent\",\"opp\",\"OPPONENT\",\"OPPONENT_ABBREVIATION\"),\n",
    "    date_cols=(\"game_date\",\"GAME_DATE\",\"date\")\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a 'wide' props frame into a tidy long format:\n",
    "    one row per (player, market, book), with numeric line and American odds.\n",
    "\n",
    "    Expected column patterns (flexible by regex):\n",
    "      <book>_<suffix>                 -> the line (e.g., mgm_pts, fanduel_ast)\n",
    "      <book>_<suffix>_over_odds       -> over odds (American)\n",
    "      <book>_<suffix>_under_odds      -> under odds (American)\n",
    "\n",
    "    Suffixes recognized per market:\n",
    "      PTS:  'pts','points'\n",
    "      REB:  'reb','rebounds'\n",
    "      AST:  'ast','assists'\n",
    "    \"\"\"\n",
    "    df = wide_df.copy()\n",
    "\n",
    "    # Identify reference columns\n",
    "    def _first_col(cands):\n",
    "        for c in cands:\n",
    "            if c in df.columns: return c\n",
    "        return None\n",
    "\n",
    "    player_col = _first_col(player_cols)\n",
    "    team_col   = _first_col(team_cols)\n",
    "    opp_col    = _first_col(opp_cols)\n",
    "    date_col   = _first_col(date_cols)\n",
    "\n",
    "    # Fallbacks if totally missing\n",
    "    if player_col is None:\n",
    "        raise ValueError(\"Could not find a player name column in wide_df. \"\n",
    "                         f\"Tried {player_cols}. Got columns: {list(df.columns)[:20]}...\")\n",
    "\n",
    "    # Normalize helpers\n",
    "    def _num_float(x):\n",
    "        if pd.isna(x): return np.nan\n",
    "        m = re.search(r\"[-+]?\\d+(?:\\.\\d+)?\", str(x))\n",
    "        return float(m.group()) if m else np.nan\n",
    "\n",
    "    def _num_int(x):\n",
    "        if pd.isna(x): return np.nan\n",
    "        m = re.search(r\"[-+]?\\d+\", str(x))\n",
    "        return int(m.group()) if m else np.nan\n",
    "\n",
    "    # Market suffix map (flex)\n",
    "    market_suffixes = {\n",
    "        \"PTS\": (\"pts\",\"points\"),\n",
    "        \"REB\": (\"reb\",\"rebounds\"),\n",
    "        \"AST\": (\"ast\",\"assists\"),\n",
    "    }\n",
    "\n",
    "    # Build long rows\n",
    "    long_rows = []\n",
    "    # Iterate rows once; pull columns per book/market dynamically\n",
    "    for _, row in df.iterrows():\n",
    "        base = {\n",
    "            \"player\": row[player_col],\n",
    "            \"team\": row[team_col] if team_col else np.nan,\n",
    "            \"opponent\": row[opp_col] if opp_col else np.nan,\n",
    "            \"game_date\": row[date_col] if date_col else np.nan,\n",
    "        }\n",
    "        for mkt in markets:\n",
    "            suffixes = market_suffixes.get(mkt, ())\n",
    "            for b in books:\n",
    "                # Find the *line* column by trying allowed suffixes\n",
    "                line_val = np.nan\n",
    "                over_val = np.nan\n",
    "                under_val = np.nan\n",
    "                line_col_used = None\n",
    "\n",
    "                for suf in suffixes:\n",
    "                    # exact line column (most common)\n",
    "                    c_line = f\"{b}_{suf}\"\n",
    "                    if c_line in df.columns and pd.notna(row[c_line]):\n",
    "                        line_val = row[c_line]\n",
    "                        line_col_used = c_line\n",
    "                        # odds columns (several sites use these names)\n",
    "                        for over_name in (f\"{b}_{suf}_over_odds\", f\"{b}_{suf}_o_odds\", f\"{b}_{suf}_over\"):\n",
    "                            if over_name in df.columns:\n",
    "                                over_val = row[over_name]\n",
    "                                break\n",
    "                        for under_name in (f\"{b}_{suf}_under_odds\", f\"{b}_{suf}_u_odds\", f\"{b}_{suf}_under\"):\n",
    "                            if under_name in df.columns:\n",
    "                                under_val = row[under_name]\n",
    "                                break\n",
    "                        break  # found a suffix match\n",
    "\n",
    "                # If not found, try a looser search (e.g., 'mgm_pts_line')\n",
    "                if (isinstance(line_val, float) and np.isnan(line_val)) or line_col_used is None:\n",
    "                    pat = re.compile(rf\"^{re.escape(b)}_({ '|'.join(map(re.escape, suffixes)) })(_line)?$\", re.I)\n",
    "                    for c in df.columns:\n",
    "                        if pat.match(str(c)) and pd.notna(row[c]):\n",
    "                            line_val = row[c]\n",
    "                            line_col_used = c\n",
    "                            # odds columns with same base\n",
    "                            base_prefix = re.sub(r\"(_line)?$\", \"\", c)\n",
    "                            for over_name in (f\"{base_prefix}_over_odds\", f\"{base_prefix}_o_odds\", f\"{base_prefix}_over\"):\n",
    "                                if over_name in df.columns:\n",
    "                                    over_val = row[over_name]\n",
    "                                    break\n",
    "                            for under_name in (f\"{base_prefix}_under_odds\", f\"{base_prefix}_u_odds\", f\"{base_prefix}_under\"):\n",
    "                                if under_name in df.columns:\n",
    "                                    under_val = row[under_name]\n",
    "                                    break\n",
    "                            break\n",
    "\n",
    "                # Only emit a row if we actually found a line\n",
    "                if pd.notna(line_val):\n",
    "                    long_rows.append({\n",
    "                        **base,\n",
    "                        \"market\": mkt,\n",
    "                        \"book\": b,\n",
    "                        \"line\": _num_float(line_val),\n",
    "                        \"over_odds\": _num_int(over_val),\n",
    "                        \"under_odds\": _num_int(under_val),\n",
    "                    })\n",
    "\n",
    "    out = pd.DataFrame(long_rows)\n",
    "\n",
    "    # Clean up: drop obviously invalid lines\n",
    "    if not out.empty:\n",
    "        out = out[pd.notna(out[\"line\"])]\n",
    "        # remove zero/negative lines that can't be real for these markets (optional)\n",
    "        out = out[out[\"line\"] > 0]\n",
    "\n",
    "        # De-duplicate best-effort (sometimes the page contains duplicates per book)\n",
    "        out = (out.sort_values([\"player\",\"market\",\"book\",\"line\"])\n",
    "                  .drop_duplicates(subset=[\"player\",\"market\",\"book\"], keep=\"last\")\n",
    "                  .reset_index(drop=True))\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a960bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Output directories ready under: xgb_outputs\n"
     ]
    }
   ],
   "source": [
    "# === Cell: Define output folders for new XGBoost model ===\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "BASE_DIR = \"xgb_outputs\"\n",
    "TODAY = datetime.utcnow().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Folders for different result types\n",
    "FOLDER_PROJECTIONS = os.path.join(BASE_DIR, \"projections\")\n",
    "FOLDER_BETS        = os.path.join(BASE_DIR, \"value_bets\")\n",
    "FOLDER_EVALS       = os.path.join(BASE_DIR, \"evaluations\")\n",
    "\n",
    "for folder in [BASE_DIR, FOLDER_PROJECTIONS, FOLDER_BETS, FOLDER_EVALS]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Output directories ready under: {BASE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae1b96fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in raw wide: 1872\n",
      "Books present in columns: ['betrivers', 'caesars', 'draftkings', 'espnbet', 'fanduel', 'hardrock', 'mgm']\n",
      "Books with lines: {'PTS': ['betrivers', 'caesars', 'draftkings', 'fanduel', 'hardrock', 'mgm'], 'REB': ['betrivers', 'caesars', 'draftkings', 'fanduel', 'hardrock', 'mgm'], 'AST': ['betrivers', 'caesars', 'draftkings', 'fanduel', 'hardrock', 'mgm']}\n",
      "odds_long rows: 1868\n",
      "odds_long columns: ['player', 'team', 'opponent', 'game_date', 'market', 'book', 'line', 'over_odds', 'under_odds']\n",
      "     player team opponent  game_date market        book  line  over_odds  \\\n",
      "0  AJ Green  MIL     @IND        NaN    AST         mgm   0.5        NaN   \n",
      "1  AJ Green  MIL     @IND        NaN    PTS   betrivers   9.5        NaN   \n",
      "2  AJ Green  MIL     @IND        NaN    PTS     caesars   8.5        NaN   \n",
      "3  AJ Green  MIL     @IND        NaN    PTS  draftkings   8.5        NaN   \n",
      "4  AJ Green  MIL     @IND        NaN    PTS     fanduel   9.5        NaN   \n",
      "\n",
      "   under_odds  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4         NaN  \n",
      "\n",
      "Saved value bets to: xgb_outputs\\value_bets\\value_bets_top100_20251103.xlsx\n",
      "380 value bets found across 3 markets.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asof_date</th>\n",
       "      <th>game_date</th>\n",
       "      <th>book</th>\n",
       "      <th>player</th>\n",
       "      <th>team</th>\n",
       "      <th>opponent</th>\n",
       "      <th>market</th>\n",
       "      <th>line</th>\n",
       "      <th>lineup_status</th>\n",
       "      <th>over_odds</th>\n",
       "      <th>...</th>\n",
       "      <th>projection_mean</th>\n",
       "      <th>projection_sd</th>\n",
       "      <th>start_prob</th>\n",
       "      <th>team_odds</th>\n",
       "      <th>opponent_odds</th>\n",
       "      <th>line_odds</th>\n",
       "      <th>book_odds</th>\n",
       "      <th>game_date_odds</th>\n",
       "      <th>over_odds_odds</th>\n",
       "      <th>under_odds_odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>AJ Green</td>\n",
       "      <td>MIL</td>\n",
       "      <td>@IND</td>\n",
       "      <td>PTS</td>\n",
       "      <td>9.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.916030</td>\n",
       "      <td>1.187405</td>\n",
       "      <td>0.9</td>\n",
       "      <td>MIL</td>\n",
       "      <td>@IND</td>\n",
       "      <td>9.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>Aaron Gordon</td>\n",
       "      <td>DEN</td>\n",
       "      <td>SAC</td>\n",
       "      <td>PTS</td>\n",
       "      <td>16.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>16.384876</td>\n",
       "      <td>2.457731</td>\n",
       "      <td>0.9</td>\n",
       "      <td>DEN</td>\n",
       "      <td>SAC</td>\n",
       "      <td>16.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>Aaron Nesmith</td>\n",
       "      <td>IND</td>\n",
       "      <td>MIL</td>\n",
       "      <td>PTS</td>\n",
       "      <td>16.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11.439683</td>\n",
       "      <td>1.715953</td>\n",
       "      <td>0.9</td>\n",
       "      <td>IND</td>\n",
       "      <td>MIL</td>\n",
       "      <td>16.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>Alex Sarr</td>\n",
       "      <td>WAS</td>\n",
       "      <td>@NYK</td>\n",
       "      <td>PTS</td>\n",
       "      <td>15.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13.514768</td>\n",
       "      <td>2.027215</td>\n",
       "      <td>0.9</td>\n",
       "      <td>WAS</td>\n",
       "      <td>@NYK</td>\n",
       "      <td>15.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>Alperen Sengun</td>\n",
       "      <td>HOU</td>\n",
       "      <td>DAL</td>\n",
       "      <td>PTS</td>\n",
       "      <td>21.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.965284</td>\n",
       "      <td>1.194793</td>\n",
       "      <td>0.9</td>\n",
       "      <td>HOU</td>\n",
       "      <td>DAL</td>\n",
       "      <td>21.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>Amen Thompson</td>\n",
       "      <td>HOU</td>\n",
       "      <td>DAL</td>\n",
       "      <td>PTS</td>\n",
       "      <td>16.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12.298719</td>\n",
       "      <td>1.844808</td>\n",
       "      <td>0.9</td>\n",
       "      <td>HOU</td>\n",
       "      <td>DAL</td>\n",
       "      <td>16.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>Andrew Wiggins</td>\n",
       "      <td>MIA</td>\n",
       "      <td>@LAC</td>\n",
       "      <td>PTS</td>\n",
       "      <td>16.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>18.280849</td>\n",
       "      <td>2.742127</td>\n",
       "      <td>0.9</td>\n",
       "      <td>MIA</td>\n",
       "      <td>@LAC</td>\n",
       "      <td>16.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>Anfernee Simons</td>\n",
       "      <td>BOS</td>\n",
       "      <td>UTA</td>\n",
       "      <td>PTS</td>\n",
       "      <td>16.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19.049860</td>\n",
       "      <td>2.857479</td>\n",
       "      <td>0.9</td>\n",
       "      <td>BOS</td>\n",
       "      <td>UTA</td>\n",
       "      <td>16.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>Ausar Thompson</td>\n",
       "      <td>DET</td>\n",
       "      <td>@MEM</td>\n",
       "      <td>PTS</td>\n",
       "      <td>14.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13.016480</td>\n",
       "      <td>1.952472</td>\n",
       "      <td>0.9</td>\n",
       "      <td>DET</td>\n",
       "      <td>@MEM</td>\n",
       "      <td>14.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>Bam Adebayo</td>\n",
       "      <td>MIA</td>\n",
       "      <td>@LAC</td>\n",
       "      <td>PTS</td>\n",
       "      <td>19.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19.007532</td>\n",
       "      <td>2.851130</td>\n",
       "      <td>0.9</td>\n",
       "      <td>MIA</td>\n",
       "      <td>@LAC</td>\n",
       "      <td>19.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hardrock</td>\n",
       "      <td>Ben Sheppard</td>\n",
       "      <td>IND</td>\n",
       "      <td>MIL</td>\n",
       "      <td>PTS</td>\n",
       "      <td>7.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.761811</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>IND</td>\n",
       "      <td>MIL</td>\n",
       "      <td>7.5</td>\n",
       "      <td>hardrock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>Bilal Coulibaly</td>\n",
       "      <td>WAS</td>\n",
       "      <td>@NYK</td>\n",
       "      <td>PTS</td>\n",
       "      <td>11.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13.578108</td>\n",
       "      <td>2.036716</td>\n",
       "      <td>0.9</td>\n",
       "      <td>WAS</td>\n",
       "      <td>@NYK</td>\n",
       "      <td>11.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>Bobby Portis</td>\n",
       "      <td>MIL</td>\n",
       "      <td>@IND</td>\n",
       "      <td>PTS</td>\n",
       "      <td>10.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14.826995</td>\n",
       "      <td>2.224049</td>\n",
       "      <td>0.9</td>\n",
       "      <td>MIL</td>\n",
       "      <td>@IND</td>\n",
       "      <td>10.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mgm</td>\n",
       "      <td>Bones Hyland</td>\n",
       "      <td>MIN</td>\n",
       "      <td>@BKN</td>\n",
       "      <td>PTS</td>\n",
       "      <td>4.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.303320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>MIN</td>\n",
       "      <td>@BKN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>mgm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>Bradley Beal</td>\n",
       "      <td>LAC</td>\n",
       "      <td>MIA</td>\n",
       "      <td>PTS</td>\n",
       "      <td>10.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17.015076</td>\n",
       "      <td>2.552261</td>\n",
       "      <td>0.9</td>\n",
       "      <td>LAC</td>\n",
       "      <td>MIA</td>\n",
       "      <td>10.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>caesars</td>\n",
       "      <td>Brook Lopez</td>\n",
       "      <td>LAC</td>\n",
       "      <td>MIA</td>\n",
       "      <td>PTS</td>\n",
       "      <td>6.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12.649896</td>\n",
       "      <td>1.897484</td>\n",
       "      <td>0.9</td>\n",
       "      <td>LAC</td>\n",
       "      <td>MIA</td>\n",
       "      <td>6.5</td>\n",
       "      <td>caesars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mgm</td>\n",
       "      <td>Bruce Brown</td>\n",
       "      <td>DEN</td>\n",
       "      <td>SAC</td>\n",
       "      <td>PTS</td>\n",
       "      <td>5.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.424790</td>\n",
       "      <td>1.263719</td>\n",
       "      <td>0.9</td>\n",
       "      <td>DEN</td>\n",
       "      <td>SAC</td>\n",
       "      <td>5.5</td>\n",
       "      <td>mgm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>caesars</td>\n",
       "      <td>Bub Carrington</td>\n",
       "      <td>WAS</td>\n",
       "      <td>@NYK</td>\n",
       "      <td>PTS</td>\n",
       "      <td>7.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.969417</td>\n",
       "      <td>1.495413</td>\n",
       "      <td>0.9</td>\n",
       "      <td>WAS</td>\n",
       "      <td>@NYK</td>\n",
       "      <td>7.5</td>\n",
       "      <td>caesars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>CJ McCollum</td>\n",
       "      <td>WAS</td>\n",
       "      <td>@NYK</td>\n",
       "      <td>PTS</td>\n",
       "      <td>15.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20.202669</td>\n",
       "      <td>3.030401</td>\n",
       "      <td>0.9</td>\n",
       "      <td>WAS</td>\n",
       "      <td>@NYK</td>\n",
       "      <td>15.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>Cade Cunningham</td>\n",
       "      <td>DET</td>\n",
       "      <td>@MEM</td>\n",
       "      <td>PTS</td>\n",
       "      <td>27.5</td>\n",
       "      <td>EXPECTED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26.492363</td>\n",
       "      <td>3.973855</td>\n",
       "      <td>0.9</td>\n",
       "      <td>DET</td>\n",
       "      <td>@MEM</td>\n",
       "      <td>27.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows √ó 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     asof_date  game_date       book           player team opponent market  \\\n",
       "0   2025-11-03        NaN  betrivers         AJ Green  MIL     @IND    PTS   \n",
       "1   2025-11-03        NaN  betrivers     Aaron Gordon  DEN      SAC    PTS   \n",
       "2   2025-11-03        NaN  betrivers    Aaron Nesmith  IND      MIL    PTS   \n",
       "3   2025-11-03        NaN  betrivers        Alex Sarr  WAS     @NYK    PTS   \n",
       "4   2025-11-03        NaN  betrivers   Alperen Sengun  HOU      DAL    PTS   \n",
       "5   2025-11-03        NaN  betrivers    Amen Thompson  HOU      DAL    PTS   \n",
       "6   2025-11-03        NaN  betrivers   Andrew Wiggins  MIA     @LAC    PTS   \n",
       "7   2025-11-03        NaN  betrivers  Anfernee Simons  BOS      UTA    PTS   \n",
       "8   2025-11-03        NaN  betrivers   Ausar Thompson  DET     @MEM    PTS   \n",
       "9   2025-11-03        NaN  betrivers      Bam Adebayo  MIA     @LAC    PTS   \n",
       "10  2025-11-03        NaN   hardrock     Ben Sheppard  IND      MIL    PTS   \n",
       "11  2025-11-03        NaN  betrivers  Bilal Coulibaly  WAS     @NYK    PTS   \n",
       "12  2025-11-03        NaN  betrivers     Bobby Portis  MIL     @IND    PTS   \n",
       "13  2025-11-03        NaN        mgm     Bones Hyland  MIN     @BKN    PTS   \n",
       "14  2025-11-03        NaN  betrivers     Bradley Beal  LAC      MIA    PTS   \n",
       "15  2025-11-03        NaN    caesars      Brook Lopez  LAC      MIA    PTS   \n",
       "16  2025-11-03        NaN        mgm      Bruce Brown  DEN      SAC    PTS   \n",
       "17  2025-11-03        NaN    caesars   Bub Carrington  WAS     @NYK    PTS   \n",
       "18  2025-11-03        NaN  betrivers      CJ McCollum  WAS     @NYK    PTS   \n",
       "19  2025-11-03        NaN  betrivers  Cade Cunningham  DET     @MEM    PTS   \n",
       "\n",
       "    line lineup_status  over_odds  ...  projection_mean  projection_sd  \\\n",
       "0    9.5      EXPECTED        NaN  ...         7.916030       1.187405   \n",
       "1   16.5      EXPECTED        NaN  ...        16.384876       2.457731   \n",
       "2   16.5      EXPECTED        NaN  ...        11.439683       1.715953   \n",
       "3   15.5      EXPECTED        NaN  ...        13.514768       2.027215   \n",
       "4   21.5      EXPECTED        NaN  ...         7.965284       1.194793   \n",
       "5   16.5      EXPECTED        NaN  ...        12.298719       1.844808   \n",
       "6   16.5      EXPECTED        NaN  ...        18.280849       2.742127   \n",
       "7   16.5      EXPECTED        NaN  ...        19.049860       2.857479   \n",
       "8   14.5      EXPECTED        NaN  ...        13.016480       1.952472   \n",
       "9   19.5      EXPECTED        NaN  ...        19.007532       2.851130   \n",
       "10   7.5      EXPECTED        NaN  ...         4.761811       1.000000   \n",
       "11  11.5      EXPECTED        NaN  ...        13.578108       2.036716   \n",
       "12  10.5      EXPECTED        NaN  ...        14.826995       2.224049   \n",
       "13   4.5      EXPECTED        NaN  ...         3.303320       1.000000   \n",
       "14  10.5      EXPECTED        NaN  ...        17.015076       2.552261   \n",
       "15   6.5      EXPECTED        NaN  ...        12.649896       1.897484   \n",
       "16   5.5      EXPECTED        NaN  ...         8.424790       1.263719   \n",
       "17   7.5      EXPECTED        NaN  ...         9.969417       1.495413   \n",
       "18  15.5      EXPECTED        NaN  ...        20.202669       3.030401   \n",
       "19  27.5      EXPECTED        NaN  ...        26.492363       3.973855   \n",
       "\n",
       "    start_prob  team_odds  opponent_odds  line_odds  book_odds  \\\n",
       "0          0.9        MIL           @IND        9.5  betrivers   \n",
       "1          0.9        DEN            SAC       16.5  betrivers   \n",
       "2          0.9        IND            MIL       16.5  betrivers   \n",
       "3          0.9        WAS           @NYK       15.5  betrivers   \n",
       "4          0.9        HOU            DAL       21.5  betrivers   \n",
       "5          0.9        HOU            DAL       16.5  betrivers   \n",
       "6          0.9        MIA           @LAC       16.5  betrivers   \n",
       "7          0.9        BOS            UTA       16.5  betrivers   \n",
       "8          0.9        DET           @MEM       14.5  betrivers   \n",
       "9          0.9        MIA           @LAC       19.5  betrivers   \n",
       "10         0.9        IND            MIL        7.5   hardrock   \n",
       "11         0.9        WAS           @NYK       11.5  betrivers   \n",
       "12         0.9        MIL           @IND       10.5  betrivers   \n",
       "13         0.9        MIN           @BKN        4.5        mgm   \n",
       "14         0.9        LAC            MIA       10.5  betrivers   \n",
       "15         0.9        LAC            MIA        6.5    caesars   \n",
       "16         0.9        DEN            SAC        5.5        mgm   \n",
       "17         0.9        WAS           @NYK        7.5    caesars   \n",
       "18         0.9        WAS           @NYK       15.5  betrivers   \n",
       "19         0.9        DET           @MEM       27.5  betrivers   \n",
       "\n",
       "    game_date_odds  over_odds_odds  under_odds_odds  \n",
       "0              NaN             NaN              NaN  \n",
       "1              NaN             NaN              NaN  \n",
       "2              NaN             NaN              NaN  \n",
       "3              NaN             NaN              NaN  \n",
       "4              NaN             NaN              NaN  \n",
       "5              NaN             NaN              NaN  \n",
       "6              NaN             NaN              NaN  \n",
       "7              NaN             NaN              NaN  \n",
       "8              NaN             NaN              NaN  \n",
       "9              NaN             NaN              NaN  \n",
       "10             NaN             NaN              NaN  \n",
       "11             NaN             NaN              NaN  \n",
       "12             NaN             NaN              NaN  \n",
       "13             NaN             NaN              NaN  \n",
       "14             NaN             NaN              NaN  \n",
       "15             NaN             NaN              NaN  \n",
       "16             NaN             NaN              NaN  \n",
       "17             NaN             NaN              NaN  \n",
       "18             NaN             NaN              NaN  \n",
       "19             NaN             NaN              NaN  \n",
       "\n",
       "[20 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 17: wide_raw ‚Üí long (PTS/REB/AST) ‚Üí join ‚Üí export ===\n",
    "from datetime import datetime\n",
    "import re, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "scraper = NBAOddsAndLineupsScraper()\n",
    "\n",
    "# 1) Raw wide odds (no grouping)\n",
    "wide_raw = scraper.get_player_props_odds_wide_raw(book=\"mgm\")\n",
    "if wide_raw.empty:\n",
    "    raise RuntimeError(\"Raw wide odds are empty. The page structure may have changed or was blocked.\")\n",
    "\n",
    "print(\"Total rows in raw wide:\", len(wide_raw))\n",
    "\n",
    "# 2) Detect books present\n",
    "books_seen = sorted({\n",
    "    m.group(1) for c in wide_raw.columns\n",
    "    if (m := re.match(r\"^(draftkings|fanduel|caesars|betrivers|espnbet|hardrock|mgm)_(.+)$\", c))\n",
    "})\n",
    "print(\"Books present in columns:\", books_seen)\n",
    "\n",
    "def _col_exists_nonnull(df, col):\n",
    "    return (col in df.columns) and df[col].notna().any()\n",
    "\n",
    "# Which books have each market today?\n",
    "market_suffix = {\"PTS\":\"pts\",\"REB\":\"reb\",\"AST\":\"ast\"}\n",
    "books_by_market = {\n",
    "    m: [b for b in books_seen if _col_exists_nonnull(wide_raw, f\"{b}_{market_suffix[m]}\")]\n",
    "    for m in [\"PTS\",\"REB\",\"AST\"]\n",
    "}\n",
    "print(\"Books with lines:\", {m: v for m, v in books_by_market.items() if v})\n",
    "\n",
    "# 3) Convert wide ‚Üí long for markets that actually have any lines\n",
    "target_markets = tuple([m for m, bs in books_by_market.items() if bs])\n",
    "if not target_markets:\n",
    "    raise RuntimeError(\"No books have non-null PTS/REB/AST lines today.\")\n",
    "\n",
    "odds_long = odds_wide_to_long_from_columns(\n",
    "    wide_raw,\n",
    "    books=tuple(sorted({b for bs in books_by_market.values() for b in bs})),\n",
    "    markets=target_markets\n",
    ")\n",
    "if odds_long.empty:\n",
    "    raise RuntimeError(\"odds_long is empty after conversion. Verify your `odds_wide_to_long_from_columns` mapping.\")\n",
    "\n",
    "# Normalize obvious numerics\n",
    "def _num_float(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    m = re.search(r\"[-+]?\\d+(?:\\.\\d+)?\", str(x))\n",
    "    return float(m.group()) if m else np.nan\n",
    "\n",
    "def _num_int(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    m = re.search(r\"[-+]?\\d+\", str(x))\n",
    "    return int(m.group()) if m else np.nan\n",
    "\n",
    "odds_long[\"line\"] = odds_long[\"line\"].apply(_num_float)\n",
    "odds_long[\"over_odds\"] = odds_long[\"over_odds\"].apply(_num_int)\n",
    "odds_long[\"under_odds\"] = odds_long[\"under_odds\"].apply(_num_int)\n",
    "\n",
    "print(\"odds_long rows:\", len(odds_long))\n",
    "print(\"odds_long columns:\", odds_long.columns.tolist())\n",
    "print(odds_long.head(5))\n",
    "\n",
    "# 4) Prepare projections union (must be created earlier, e.g., Cell 16e)\n",
    "if \"df_projections_all\" not in globals() or df_projections_all.empty:\n",
    "    raise RuntimeError(\"df_projections_all not found or empty (run Cell 16 that builds PTS/REB/AST projections).\")\n",
    "\n",
    "# Light name normalizer\n",
    "def _norm_player(name: str) -> str:\n",
    "    if not isinstance(name, str): return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", name)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"[.\\-`'‚Äô]\", \"\", s).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "odds_long = odds_long.copy()\n",
    "df_projections_all = df_projections_all.copy()\n",
    "\n",
    "odds_long[\"player_key\"] = odds_long[\"player\"].map(_norm_player)\n",
    "df_projections_all[\"player_key\"] = df_projections_all[\"player\"].map(_norm_player)\n",
    "\n",
    "# Ensure projection SD exists per market (fallback to 15% of mean, min 1.0)\n",
    "for m in [\"PTS\",\"REB\",\"AST\"]:\n",
    "    mask = df_projections_all[\"market\"].eq(m)\n",
    "    if \"projection_sd\" not in df_projections_all.columns:\n",
    "        df_projections_all[\"projection_sd\"] = np.nan\n",
    "    missing_sd = df_projections_all.loc[mask, \"projection_sd\"].isna() | (df_projections_all.loc[mask, \"projection_sd\"] <= 0)\n",
    "    if missing_sd.any():\n",
    "        df_projections_all.loc[mask, \"projection_sd\"] = (\n",
    "            df_projections_all.loc[mask, \"projection_mean\"].abs() * 0.15\n",
    "        ).clip(lower=1.0)\n",
    "\n",
    "# 5) Split and join per market, then combine\n",
    "joined_frames = []\n",
    "for mkt in target_markets:\n",
    "    odds_m = odds_long.loc[odds_long[\"market\"].eq(mkt)].copy()\n",
    "    proj_m = df_projections_all.loc[df_projections_all[\"market\"].eq(mkt)].copy()\n",
    "    if odds_m.empty or proj_m.empty:\n",
    "        print(f\"‚ö†Ô∏è Skipping {mkt}: odds or projections empty.\")\n",
    "        continue\n",
    "\n",
    "    join_keys = [\"player_key\",\"market\"]\n",
    "    view_cols_odds = join_keys + [\"player\",\"team\",\"opponent\",\"line\",\"book\",\"game_date\",\"over_odds\",\"under_odds\"]\n",
    "    view_cols_odds = [c for c in view_cols_odds if c in odds_m.columns]\n",
    "\n",
    "    view_cols_proj = join_keys + [\n",
    "        \"player\",\"team\",\"opponent\",\"game_date\",\"projection_mean\",\"projection_sd\",\n",
    "        \"projected_minutes\",\"start_prob\",\"lineup_status\"\n",
    "    ]\n",
    "    view_cols_proj = [c for c in view_cols_proj if c in proj_m.columns]\n",
    "\n",
    "    dfj = proj_m[view_cols_proj].merge(\n",
    "        odds_m[view_cols_odds].rename(columns={\"player\":\"player_odds\",\"team\":\"team_odds\",\"opponent\":\"opponent_odds\",\"game_date\":\"game_date_odds\"}),\n",
    "        on=join_keys, how=\"inner\", suffixes=(\"_proj\",\"_odds\")\n",
    "    )\n",
    "\n",
    "    if dfj.empty:\n",
    "        print(f\"‚ö†Ô∏è Join produced 0 rows for {mkt}. Check name variants.\")\n",
    "        continue\n",
    "\n",
    "    # Resolve canonical columns\n",
    "    def _pick_first(df_, names, default=np.nan):\n",
    "        for n in names:\n",
    "            if n in df_.columns:\n",
    "                return df_[n]\n",
    "        return default\n",
    "\n",
    "    dfj = dfj.loc[:, ~dfj.columns.duplicated()].copy()\n",
    "    dfj[\"player\"]    = _pick_first(dfj, [\"player_odds\",\"player_proj\",\"player\"])\n",
    "    dfj[\"team\"]      = _pick_first(dfj, [\"team_odds\",\"team_proj\",\"team\"])\n",
    "    dfj[\"opponent\"]  = _pick_first(dfj, [\"opponent_odds\",\"opponent_proj\",\"opponent\"])\n",
    "    dfj[\"game_date\"] = _pick_first(dfj, [\"game_date_odds\",\"game_date_proj\",\"game_date\"])\n",
    "    # line already numeric above, but if any slipped through:\n",
    "    dfj[\"line\"] = dfj[\"line\"].apply(_num_float)\n",
    "\n",
    "    # Compute model probability P(Over)\n",
    "    if \"p_over_from_normal\" not in globals():\n",
    "        from statistics import NormalDist\n",
    "        def p_over_from_normal(mu, sd, line):\n",
    "            if pd.isna(mu) or pd.isna(sd) or pd.isna(line) or float(sd) <= 0: return np.nan\n",
    "            z = (float(line) - float(mu)) / float(sd)\n",
    "            return 1.0 - NormalDist().cdf(z)\n",
    "\n",
    "    dfj[\"p_over_model\"] = dfj.apply(\n",
    "        lambda r: p_over_from_normal(r.get(\"projection_mean\"), r.get(\"projection_sd\"), r.get(\"line\")), axis=1\n",
    "    )\n",
    "\n",
    "    # Implied/fair probabilities + edge (so we can pick best book later)\n",
    "    def implied_prob(american):\n",
    "        o = _num_int(american)\n",
    "        if pd.isna(o): return np.nan\n",
    "        return (-o)/(-o+100.0) if o < 0 else 100.0/(o+100.0)\n",
    "\n",
    "    dfj[\"p_over_imp\"]  = dfj[\"over_odds\"].apply(implied_prob)\n",
    "    dfj[\"p_under_imp\"] = dfj[\"under_odds\"].apply(implied_prob)\n",
    "\n",
    "    def devig_pair(p_over_imp, p_under_imp):\n",
    "        if pd.isna(p_over_imp) or pd.isna(p_under_imp):\n",
    "            return (np.nan, np.nan)\n",
    "        s = p_over_imp + p_under_imp\n",
    "        if s <= 0:\n",
    "            return (np.nan, np.nan)\n",
    "        return (p_over_imp/s, p_under_imp/s)\n",
    "\n",
    "    fair = dfj.apply(\n",
    "        lambda r: pd.Series(devig_pair(r[\"p_over_imp\"], r[\"p_under_imp\"]), index=[\"p_over_fair\",\"p_under_fair\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    dfj = pd.concat([dfj, fair], axis=1)\n",
    "\n",
    "    dfj[\"edge_over\"] = np.where(\n",
    "        dfj[\"p_over_fair\"].notna(),\n",
    "        dfj[\"p_over_model\"] - dfj[\"p_over_fair\"],\n",
    "        dfj[\"p_over_model\"] - dfj[\"p_over_imp\"]\n",
    "    )\n",
    "\n",
    "    # Keep the best book per player/market (highest edge)\n",
    "    dfj = dfj.sort_values([\"player\",\"market\",\"edge_over\"], ascending=[True, True, False])\n",
    "    dfj = dfj.drop_duplicates(subset=[\"player\",\"market\"], keep=\"first\")\n",
    "\n",
    "    joined_frames.append(\n",
    "        dfj[[\n",
    "            \"player\",\"team\",\"opponent\",\"market\",\"line\",\"book\",\"game_date\",\n",
    "            \"over_odds\",\"under_odds\",\"projection_mean\",\"projection_sd\",\n",
    "            \"projected_minutes\",\"start_prob\",\"lineup_status\",\"p_over_model\",\"edge_over\"\n",
    "        ]]\n",
    "    )\n",
    "\n",
    "# Combined joined frame for all markets\n",
    "if not joined_frames:\n",
    "    raise RuntimeError(\"No joined rows produced for any market.\")\n",
    "df_proj_join_all = pd.concat(joined_frames, ignore_index=True)\n",
    "\n",
    "# 6) Prepare odds slice for Excel builder\n",
    "df_odds_for_excel = df_proj_join_all[[\n",
    "    \"player\",\"team\",\"opponent\",\"market\",\"line\",\"book\",\"game_date\",\"over_odds\",\"under_odds\"\n",
    "]].copy()\n",
    "\n",
    "# 7) Export full bets workbook\n",
    "bets, excel_path = build_value_bets_excel(\n",
    "    df_projections=df_proj_join_all,\n",
    "    df_odds=df_odds_for_excel,\n",
    "    outfile_path=os.path.join(FOLDER_BETS, f\"value_bets_top100_{TODAY}.xlsx\"),\n",
    "    join_keys=(\"player\",\"market\")  # permissive merge\n",
    ")\n",
    "\n",
    "print(f\"\\nSaved value bets to: {excel_path}\")\n",
    "print(len(bets), \"value bets found across\", df_proj_join_all['market'].nunique(), \"markets.\")\n",
    "display(bets.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4777a9df",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data/bets\\\\value_bets_top100_20251103.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# ---- 6) Write both to Excel --------------------------------------------------\u001b[39;00m\n\u001b[0;32m    126\u001b[0m out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/bets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_bets_top100_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mutcnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenpyxl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m w:\n\u001b[0;32m    128\u001b[0m     priced\u001b[38;5;241m.\u001b[39mto_excel(w, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop100_priced\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    129\u001b[0m     allp\u001b[38;5;241m.\u001b[39mto_excel(w, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop100_all\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\excel\\_openpyxl.py:61\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[0;32m     59\u001b[0m engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_sheet_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_sheet_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# ExcelWriter replaced \"a\" by \"r+\" to allow us to first read the excel file from\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# the file and later write to it\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode:  \u001b[38;5;66;03m# Load from existing workbook\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\excel\\_base.py:1263\u001b[0m, in \u001b[0;36mExcelWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs)\u001b[0m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles \u001b[38;5;241m=\u001b[39m IOHandles(\n\u001b[0;32m   1260\u001b[0m     cast(IO[\u001b[38;5;28mbytes\u001b[39m], path), compression\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m   1261\u001b[0m )\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, ExcelWriter):\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cur_sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m date_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'data/bets\\\\value_bets_top100_20251103.xlsx'"
     ]
    }
   ],
   "source": [
    "# === BUILD & SAVE VALUE BETS (Top-100) TO /data/bets ===\n",
    "import os, re\n",
    "import numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "from statistics import NormalDist\n",
    "\n",
    "# --- ensure folder structure ---\n",
    "os.makedirs(\"data/bets\", exist_ok=True)\n",
    "\n",
    "# ---- 0) Source table ---------------------------------------------------------\n",
    "if \"df_proj_join_all\" in globals() and isinstance(df_proj_join_all, pd.DataFrame) and not df_proj_join_all.empty:\n",
    "    df = df_proj_join_all.copy()\n",
    "elif \"bets\" in globals() and isinstance(bets, pd.DataFrame) and not bets.empty:\n",
    "    df = bets.copy()\n",
    "elif \"df_proj_join\" in globals() and isinstance(df_proj_join, pd.DataFrame) and not df_proj_join.empty:\n",
    "    df = df_proj_join.copy()\n",
    "else:\n",
    "    raise RuntimeError(\"No joined dataset found (df_proj_join_all/bets/df_proj_join). Run the join cell first.\")\n",
    "\n",
    "# ---- 1) Canonical minimal fields --------------------------------------------\n",
    "def _pick(df_, names):\n",
    "    for n in names:\n",
    "        if n in df_.columns:\n",
    "            return df_[n]\n",
    "    return pd.Series([np.nan]*len(df_))\n",
    "\n",
    "df = df.copy()\n",
    "df[\"player\"]          = _pick(df, [\"player\",\"player_proj\",\"player_odds\"])\n",
    "df[\"team\"]            = _pick(df, [\"team\",\"team_proj\",\"team_odds\"])\n",
    "df[\"opponent\"]        = _pick(df, [\"opponent\",\"opponent_proj\",\"opponent_odds\"])\n",
    "df[\"market\"]          = _pick(df, [\"market\"])\n",
    "df[\"line\"]            = _pick(df, [\"line\",\"posted_line\",\"book_line\"])\n",
    "df[\"book\"]            = _pick(df, [\"book\"])\n",
    "df[\"over_odds\"]       = _pick(df, [\"over_odds\"])\n",
    "df[\"under_odds\"]      = _pick(df, [\"under_odds\"])\n",
    "df[\"projection_mean\"] = _pick(df, [\"projection_mean\",\"expected_line\"])\n",
    "df[\"projection_sd\"]   = _pick(df, [\"projection_sd\"])\n",
    "\n",
    "# ---- 2) Coerce numerics ------------------------------------------------------\n",
    "def _first_float(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    m = re.search(r\"[-+]?\\d+(?:\\.\\d+)?\", str(x))\n",
    "    return float(m.group()) if m else np.nan\n",
    "\n",
    "def _first_int(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    m = re.search(r\"[-+]?\\d+\", str(x))\n",
    "    return float(m.group()) if m else np.nan\n",
    "\n",
    "for col in [\"line\",\"projection_mean\",\"projection_sd\"]:\n",
    "    df[col] = df[col].apply(_first_float)\n",
    "\n",
    "# ---- 3) Model P(Over) with fallback SD --------------------------------------\n",
    "if \"p_over_model\" not in df.columns or df[\"p_over_model\"].isna().all():\n",
    "    sd_missing = (\"projection_sd\" not in df.columns) or df[\"projection_sd\"].fillna(0).eq(0).all()\n",
    "    if sd_missing:\n",
    "        df[\"projection_sd\"] = (df[\"projection_mean\"].abs() * 0.15).clip(lower=1.0)\n",
    "\n",
    "    def p_over_from_normal(mean, sd, line):\n",
    "        if pd.isna(mean) or pd.isna(sd) or pd.isna(line) or float(sd) <= 0:\n",
    "            return np.nan\n",
    "        z = (float(line) - float(mean)) / float(sd)\n",
    "        return 1.0 - NormalDist().cdf(z)\n",
    "\n",
    "    df[\"p_over_model\"] = df.apply(\n",
    "        lambda r: p_over_from_normal(r[\"projection_mean\"], r[\"projection_sd\"], r[\"line\"]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# ---- 4) Implied / fair probs + edge ------------------------------------------\n",
    "def implied_prob_from_american(american):\n",
    "    o = _first_int(american)\n",
    "    if pd.isna(o): return np.nan\n",
    "    return (-o)/(-o+100.0) if o < 0 else 100.0/(o+100.0)\n",
    "\n",
    "if \"p_over_imp\" not in df.columns:\n",
    "    df[\"p_over_imp\"] = df[\"over_odds\"].apply(implied_prob_from_american)\n",
    "if \"p_under_imp\" not in df.columns:\n",
    "    df[\"p_under_imp\"] = df[\"under_odds\"].apply(implied_prob_from_american)\n",
    "\n",
    "fair = df.apply(\n",
    "    lambda r: pd.Series(\n",
    "        (np.nan, np.nan) if (pd.isna(r[\"p_over_imp\"]) or pd.isna(r[\"p_under_imp\"])) else\n",
    "        (r[\"p_over_imp\"]/(r[\"p_over_imp\"]+r[\"p_under_imp\"]),\n",
    "         r[\"p_under_imp\"]/(r[\"p_over_imp\"]+r[\"p_under_imp\"]))\n",
    "    , index=[\"p_over_fair\",\"p_under_fair\"]), axis=1)\n",
    "df = pd.concat([df, fair], axis=1)\n",
    "\n",
    "df[\"edge_over\"] = np.where(\n",
    "    df[\"p_over_fair\"].notna(),\n",
    "    df[\"p_over_model\"] - df[\"p_over_fair\"],\n",
    "    df[\"p_over_model\"] - df[\"p_over_imp\"]\n",
    ")\n",
    "\n",
    "# Fallback z-score edge\n",
    "nd = NormalDist()\n",
    "df[\"z_score\"] = (df[\"projection_mean\"] - df[\"line\"]) / df[\"projection_sd\"].replace(0, np.nan)\n",
    "df[\"edge_fallback\"] = df[\"z_score\"].map(lambda z: (nd.cdf(z) - 0.5)*2 if pd.notna(z) else np.nan)\n",
    "df[\"edge_rank\"] = np.where(df[\"edge_over\"].notna(), df[\"edge_over\"], df[\"edge_fallback\"])\n",
    "\n",
    "# ---- 5) Split: priced vs all -------------------------------------------------\n",
    "base_cols = [\n",
    "    \"player\",\"team\",\"opponent\",\"market\",\"line\",\"book\",\n",
    "    \"over_odds\",\"under_odds\",\"p_over_imp\",\"p_under_imp\",\n",
    "    \"p_over_fair\",\"p_under_fair\",\n",
    "    \"projection_mean\",\"projection_sd\",\"p_over_model\",\"edge_over\",\"edge_rank\"\n",
    "]\n",
    "lean = df.loc[:, [c for c in base_cols if c in df.columns]].copy()\n",
    "lean = lean.dropna(subset=[\"player\",\"market\",\"line\",\"projection_mean\"], how=\"any\")\n",
    "lean = lean.sort_values([\"player\",\"market\",\"edge_rank\"], ascending=[True, True, False])\n",
    "lean = lean.drop_duplicates(subset=[\"player\",\"market\"], keep=\"first\")\n",
    "\n",
    "priced = lean.dropna(subset=[\"over_odds\",\"under_odds\",\"p_over_imp\",\"p_under_imp\"], how=\"any\").copy()\n",
    "allp   = lean.copy()\n",
    "\n",
    "# Expected value for $100 bet\n",
    "def ev_over_100(p_over, american):\n",
    "    o = _first_int(american)\n",
    "    if pd.isna(p_over) or pd.isna(o): return np.nan\n",
    "    dec = (1 + o/100.0) if o > 0 else (1 + 100.0/abs(o))\n",
    "    return p_over * (dec - 1) * 100 - (1 - p_over) * 100\n",
    "\n",
    "priced[\"EV_over_$100\"] = priced.apply(lambda r: ev_over_100(r[\"p_over_model\"], r[\"over_odds\"]), axis=1)\n",
    "\n",
    "# ---- 6) Write both to Excel --------------------------------------------------\n",
    "out_path = os.path.join(\"data/bets\", f\"value_bets_top100_{datetime.utcnow().strftime('%Y%m%d')}.xlsx\")\n",
    "with pd.ExcelWriter(out_path, engine=\"openpyxl\") as w:\n",
    "    priced.to_excel(w, sheet_name=\"Top100_priced\", index=False)\n",
    "    allp.to_excel(w, sheet_name=\"Top100_all\", index=False)\n",
    "\n",
    "print(f\"‚úÖ Saved Top-100 value bets to: {out_path}\")\n",
    "print(f\"Rows ‚Üí priced: {len(priced)} | all: {len(allp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18cde6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating bets for YESTERDAY (Europe/Athens): 2025-11-03 (20251103)\n",
      "Using bets workbook: value_bets_top100_20251103.xlsx\n",
      "Loaded bets: sheet [Top100_all], rows=377\n",
      "‚ö†Ô∏è No boxscore file for 20251103; using latest: nba_boxscores_2025-26.csv (2313 rows)\n",
      "Filtered boxscores to 2025-11-03 by column 'GAME_DATE': 198 rows\n",
      "\n",
      "‚úÖ Evaluated 353 bets for 2025-11-03  (WIN=180, LOSS=173, PUSH=0)\n",
      "üéØ Hit rate: 51.0%\n",
      "üìä Saved evaluation to: xgb_outputs/evaluations\\value_bets_top100_20251103_EVAL.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>team</th>\n",
       "      <th>opponent</th>\n",
       "      <th>market</th>\n",
       "      <th>line</th>\n",
       "      <th>book</th>\n",
       "      <th>over_odds</th>\n",
       "      <th>under_odds</th>\n",
       "      <th>p_over_imp</th>\n",
       "      <th>p_under_imp</th>\n",
       "      <th>...</th>\n",
       "      <th>edge_over</th>\n",
       "      <th>edge_rank</th>\n",
       "      <th>player_key</th>\n",
       "      <th>PTS</th>\n",
       "      <th>REB</th>\n",
       "      <th>AST</th>\n",
       "      <th>TEAM_ABBREVIATION</th>\n",
       "      <th>OPPONENT_ABBREVIATION</th>\n",
       "      <th>actual</th>\n",
       "      <th>result_over</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Isaiah Collier</td>\n",
       "      <td>UTA</td>\n",
       "      <td>@BOS</td>\n",
       "      <td>PTS</td>\n",
       "      <td>6.5</td>\n",
       "      <td>caesars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>isaiah collier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bub Carrington</td>\n",
       "      <td>WAS</td>\n",
       "      <td>@NYK</td>\n",
       "      <td>AST</td>\n",
       "      <td>3.5</td>\n",
       "      <td>caesars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>bub carrington</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>NYK</td>\n",
       "      <td>7.0</td>\n",
       "      <td>WIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brook Lopez</td>\n",
       "      <td>LAC</td>\n",
       "      <td>MIA</td>\n",
       "      <td>PTS</td>\n",
       "      <td>6.5</td>\n",
       "      <td>caesars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999482</td>\n",
       "      <td>brook lopez</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LAC</td>\n",
       "      <td>MIA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>LOSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Isaiah Collier</td>\n",
       "      <td>UTA</td>\n",
       "      <td>@BOS</td>\n",
       "      <td>AST</td>\n",
       "      <td>4.5</td>\n",
       "      <td>caesars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999007</td>\n",
       "      <td>isaiah collier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kyle Filipowski</td>\n",
       "      <td>UTA</td>\n",
       "      <td>@BOS</td>\n",
       "      <td>PTS</td>\n",
       "      <td>6.5</td>\n",
       "      <td>mgm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.998622</td>\n",
       "      <td>kyle filipowski</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>UTA</td>\n",
       "      <td></td>\n",
       "      <td>13.0</td>\n",
       "      <td>WIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cameron Johnson</td>\n",
       "      <td>DEN</td>\n",
       "      <td>SAC</td>\n",
       "      <td>AST</td>\n",
       "      <td>2.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997917</td>\n",
       "      <td>cameron johnson</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>DEN</td>\n",
       "      <td>AC</td>\n",
       "      <td>2.0</td>\n",
       "      <td>LOSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jalen Wilson</td>\n",
       "      <td>BKN</td>\n",
       "      <td>MIN</td>\n",
       "      <td>PTS</td>\n",
       "      <td>5.5</td>\n",
       "      <td>mgm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997296</td>\n",
       "      <td>jalen wilson</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BKN</td>\n",
       "      <td>MIN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>WIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kyle Kuzma</td>\n",
       "      <td>MIL</td>\n",
       "      <td>@IND</td>\n",
       "      <td>PTS</td>\n",
       "      <td>8.5</td>\n",
       "      <td>draftkings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.994089</td>\n",
       "      <td>kyle kuzma</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MIL</td>\n",
       "      <td>IND</td>\n",
       "      <td>15.0</td>\n",
       "      <td>WIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shaedon Sharpe</td>\n",
       "      <td>POR</td>\n",
       "      <td>LAL</td>\n",
       "      <td>REB</td>\n",
       "      <td>3.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993276</td>\n",
       "      <td>shaedon sharpe</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>POR</td>\n",
       "      <td>LAL</td>\n",
       "      <td>7.0</td>\n",
       "      <td>WIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kel'el Ware</td>\n",
       "      <td>MIA</td>\n",
       "      <td>@LAC</td>\n",
       "      <td>PTS</td>\n",
       "      <td>7.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.989611</td>\n",
       "      <td>kelel ware</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MIA</td>\n",
       "      <td>LAC</td>\n",
       "      <td>16.0</td>\n",
       "      <td>WIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kel'el Ware</td>\n",
       "      <td>MIA</td>\n",
       "      <td>@LAC</td>\n",
       "      <td>REB</td>\n",
       "      <td>5.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.989395</td>\n",
       "      <td>kelel ware</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MIA</td>\n",
       "      <td>LAC</td>\n",
       "      <td>5.0</td>\n",
       "      <td>LOSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CJ McCollum</td>\n",
       "      <td>WAS</td>\n",
       "      <td>@NYK</td>\n",
       "      <td>PTS</td>\n",
       "      <td>15.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.989254</td>\n",
       "      <td>cj mccollum</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>NYK</td>\n",
       "      <td>5.0</td>\n",
       "      <td>LOSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>John Collins</td>\n",
       "      <td>LAC</td>\n",
       "      <td>MIA</td>\n",
       "      <td>PTS</td>\n",
       "      <td>12.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.988407</td>\n",
       "      <td>john collins</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LAC</td>\n",
       "      <td>MIA</td>\n",
       "      <td>12.0</td>\n",
       "      <td>LOSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bobby Portis</td>\n",
       "      <td>MIL</td>\n",
       "      <td>@IND</td>\n",
       "      <td>REB</td>\n",
       "      <td>5.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.986904</td>\n",
       "      <td>bobby portis</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MIL</td>\n",
       "      <td>IND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>LOSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Jalen Wilson</td>\n",
       "      <td>BKN</td>\n",
       "      <td>MIN</td>\n",
       "      <td>REB</td>\n",
       "      <td>1.5</td>\n",
       "      <td>mgm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.986790</td>\n",
       "      <td>jalen wilson</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BKN</td>\n",
       "      <td>MIN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LOSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Giannis Antetokounmpo</td>\n",
       "      <td>MIL</td>\n",
       "      <td>@IND</td>\n",
       "      <td>AST</td>\n",
       "      <td>7.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.986530</td>\n",
       "      <td>giannis antetokounmpo</td>\n",
       "      <td>33.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>MIL</td>\n",
       "      <td>IND</td>\n",
       "      <td>5.0</td>\n",
       "      <td>LOSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Corey Kispert</td>\n",
       "      <td>WAS</td>\n",
       "      <td>@NYK</td>\n",
       "      <td>PTS</td>\n",
       "      <td>7.5</td>\n",
       "      <td>hardrock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.984264</td>\n",
       "      <td>corey kispert</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>NYK</td>\n",
       "      <td>15.0</td>\n",
       "      <td>WIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Noah Clowney</td>\n",
       "      <td>BKN</td>\n",
       "      <td>MIN</td>\n",
       "      <td>PTS</td>\n",
       "      <td>6.5</td>\n",
       "      <td>caesars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.984041</td>\n",
       "      <td>noah clowney</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BKN</td>\n",
       "      <td>MIN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>WIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bub Carrington</td>\n",
       "      <td>WAS</td>\n",
       "      <td>@NYK</td>\n",
       "      <td>PTS</td>\n",
       "      <td>7.5</td>\n",
       "      <td>caesars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.980041</td>\n",
       "      <td>bub carrington</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>NYK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LOSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bradley Beal</td>\n",
       "      <td>LAC</td>\n",
       "      <td>MIA</td>\n",
       "      <td>PTS</td>\n",
       "      <td>10.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.979455</td>\n",
       "      <td>bradley beal</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>LAC</td>\n",
       "      <td>MIA</td>\n",
       "      <td>12.0</td>\n",
       "      <td>WIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Peyton Watson</td>\n",
       "      <td>DEN</td>\n",
       "      <td>SAC</td>\n",
       "      <td>PTS</td>\n",
       "      <td>6.5</td>\n",
       "      <td>caesars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971756</td>\n",
       "      <td>peyton watson</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DEN</td>\n",
       "      <td>AC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LOSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Jordan Clarkson</td>\n",
       "      <td>NYK</td>\n",
       "      <td>WAS</td>\n",
       "      <td>PTS</td>\n",
       "      <td>8.5</td>\n",
       "      <td>caesars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971284</td>\n",
       "      <td>jordan clarkson</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NYK</td>\n",
       "      <td></td>\n",
       "      <td>15.0</td>\n",
       "      <td>WIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Landry Shamet</td>\n",
       "      <td>NYK</td>\n",
       "      <td>WAS</td>\n",
       "      <td>PTS</td>\n",
       "      <td>5.5</td>\n",
       "      <td>mgm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971042</td>\n",
       "      <td>landry shamet</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NYK</td>\n",
       "      <td></td>\n",
       "      <td>11.0</td>\n",
       "      <td>WIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Rudy Gobert</td>\n",
       "      <td>MIN</td>\n",
       "      <td>@BKN</td>\n",
       "      <td>PTS</td>\n",
       "      <td>12.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.970783</td>\n",
       "      <td>rudy gobert</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MIN</td>\n",
       "      <td>BKN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>WIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Christian Braun</td>\n",
       "      <td>DEN</td>\n",
       "      <td>SAC</td>\n",
       "      <td>REB</td>\n",
       "      <td>3.5</td>\n",
       "      <td>betrivers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.969475</td>\n",
       "      <td>christian braun</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>DEN</td>\n",
       "      <td>AC</td>\n",
       "      <td>4.0</td>\n",
       "      <td>WIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows √ó 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   player team opponent market  line        book  over_odds  \\\n",
       "0          Isaiah Collier  UTA     @BOS    PTS   6.5     caesars        NaN   \n",
       "1          Bub Carrington  WAS     @NYK    AST   3.5     caesars        NaN   \n",
       "2             Brook Lopez  LAC      MIA    PTS   6.5     caesars        NaN   \n",
       "3          Isaiah Collier  UTA     @BOS    AST   4.5     caesars        NaN   \n",
       "4         Kyle Filipowski  UTA     @BOS    PTS   6.5         mgm        NaN   \n",
       "5         Cameron Johnson  DEN      SAC    AST   2.5   betrivers        NaN   \n",
       "6            Jalen Wilson  BKN      MIN    PTS   5.5         mgm        NaN   \n",
       "7              Kyle Kuzma  MIL     @IND    PTS   8.5  draftkings        NaN   \n",
       "8          Shaedon Sharpe  POR      LAL    REB   3.5   betrivers        NaN   \n",
       "9             Kel'el Ware  MIA     @LAC    PTS   7.5   betrivers        NaN   \n",
       "10            Kel'el Ware  MIA     @LAC    REB   5.5   betrivers        NaN   \n",
       "11            CJ McCollum  WAS     @NYK    PTS  15.5   betrivers        NaN   \n",
       "12           John Collins  LAC      MIA    PTS  12.5   betrivers        NaN   \n",
       "13           Bobby Portis  MIL     @IND    REB   5.5   betrivers        NaN   \n",
       "14           Jalen Wilson  BKN      MIN    REB   1.5         mgm        NaN   \n",
       "15  Giannis Antetokounmpo  MIL     @IND    AST   7.5   betrivers        NaN   \n",
       "16          Corey Kispert  WAS     @NYK    PTS   7.5    hardrock        NaN   \n",
       "17           Noah Clowney  BKN      MIN    PTS   6.5     caesars        NaN   \n",
       "18         Bub Carrington  WAS     @NYK    PTS   7.5     caesars        NaN   \n",
       "19           Bradley Beal  LAC      MIA    PTS  10.5   betrivers        NaN   \n",
       "20          Peyton Watson  DEN      SAC    PTS   6.5     caesars        NaN   \n",
       "21        Jordan Clarkson  NYK      WAS    PTS   8.5     caesars        NaN   \n",
       "22          Landry Shamet  NYK      WAS    PTS   5.5         mgm        NaN   \n",
       "23            Rudy Gobert  MIN     @BKN    PTS  12.5   betrivers        NaN   \n",
       "24        Christian Braun  DEN      SAC    REB   3.5   betrivers        NaN   \n",
       "\n",
       "    under_odds  p_over_imp  p_under_imp  ...  edge_over  edge_rank  \\\n",
       "0          NaN         NaN          NaN  ...        NaN   0.999799   \n",
       "1          NaN         NaN          NaN  ...        NaN   0.999731   \n",
       "2          NaN         NaN          NaN  ...        NaN   0.999482   \n",
       "3          NaN         NaN          NaN  ...        NaN   0.999007   \n",
       "4          NaN         NaN          NaN  ...        NaN   0.998622   \n",
       "5          NaN         NaN          NaN  ...        NaN   0.997917   \n",
       "6          NaN         NaN          NaN  ...        NaN   0.997296   \n",
       "7          NaN         NaN          NaN  ...        NaN   0.994089   \n",
       "8          NaN         NaN          NaN  ...        NaN   0.993276   \n",
       "9          NaN         NaN          NaN  ...        NaN   0.989611   \n",
       "10         NaN         NaN          NaN  ...        NaN   0.989395   \n",
       "11         NaN         NaN          NaN  ...        NaN   0.989254   \n",
       "12         NaN         NaN          NaN  ...        NaN   0.988407   \n",
       "13         NaN         NaN          NaN  ...        NaN   0.986904   \n",
       "14         NaN         NaN          NaN  ...        NaN   0.986790   \n",
       "15         NaN         NaN          NaN  ...        NaN   0.986530   \n",
       "16         NaN         NaN          NaN  ...        NaN   0.984264   \n",
       "17         NaN         NaN          NaN  ...        NaN   0.984041   \n",
       "18         NaN         NaN          NaN  ...        NaN   0.980041   \n",
       "19         NaN         NaN          NaN  ...        NaN   0.979455   \n",
       "20         NaN         NaN          NaN  ...        NaN   0.971756   \n",
       "21         NaN         NaN          NaN  ...        NaN   0.971284   \n",
       "22         NaN         NaN          NaN  ...        NaN   0.971042   \n",
       "23         NaN         NaN          NaN  ...        NaN   0.970783   \n",
       "24         NaN         NaN          NaN  ...        NaN   0.969475   \n",
       "\n",
       "               player_key   PTS   REB  AST  TEAM_ABBREVIATION  \\\n",
       "0          isaiah collier   NaN   NaN  NaN                NaN   \n",
       "1          bub carrington   0.0   4.0  7.0                WAS   \n",
       "2             brook lopez   5.0   1.0  0.0                LAC   \n",
       "3          isaiah collier   NaN   NaN  NaN                NaN   \n",
       "4         kyle filipowski  13.0   8.0  0.0                UTA   \n",
       "5         cameron johnson  10.0   3.0  2.0                DEN   \n",
       "6            jalen wilson   7.0   1.0  1.0                BKN   \n",
       "7              kyle kuzma  15.0   5.0  0.0                MIL   \n",
       "8          shaedon sharpe  23.0   7.0  2.0                POR   \n",
       "9              kelel ware  16.0   5.0  0.0                MIA   \n",
       "10             kelel ware  16.0   5.0  0.0                MIA   \n",
       "11            cj mccollum   5.0   1.0  2.0                WAS   \n",
       "12           john collins  12.0   3.0  0.0                LAC   \n",
       "13           bobby portis   8.0   2.0  1.0                MIL   \n",
       "14           jalen wilson   7.0   1.0  1.0                BKN   \n",
       "15  giannis antetokounmpo  33.0  13.0  5.0                MIL   \n",
       "16          corey kispert  15.0   0.0  4.0                WAS   \n",
       "17           noah clowney  15.0   6.0  1.0                BKN   \n",
       "18         bub carrington   0.0   4.0  7.0                WAS   \n",
       "19           bradley beal  12.0   1.0  2.0                LAC   \n",
       "20          peyton watson   0.0   1.0  1.0                DEN   \n",
       "21        jordan clarkson  15.0   1.0  2.0                NYK   \n",
       "22          landry shamet  11.0   5.0  0.0                NYK   \n",
       "23            rudy gobert  15.0  12.0  1.0                MIN   \n",
       "24        christian braun  21.0   4.0  3.0                DEN   \n",
       "\n",
       "   OPPONENT_ABBREVIATION  actual  result_over  \n",
       "0                    NaN     NaN           NA  \n",
       "1                    NYK     7.0          WIN  \n",
       "2                    MIA     5.0         LOSS  \n",
       "3                    NaN     NaN           NA  \n",
       "4                           13.0          WIN  \n",
       "5                     AC     2.0         LOSS  \n",
       "6                    MIN     7.0          WIN  \n",
       "7                    IND    15.0          WIN  \n",
       "8                    LAL     7.0          WIN  \n",
       "9                    LAC    16.0          WIN  \n",
       "10                   LAC     5.0         LOSS  \n",
       "11                   NYK     5.0         LOSS  \n",
       "12                   MIA    12.0         LOSS  \n",
       "13                   IND     2.0         LOSS  \n",
       "14                   MIN     1.0         LOSS  \n",
       "15                   IND     5.0         LOSS  \n",
       "16                   NYK    15.0          WIN  \n",
       "17                   MIN    15.0          WIN  \n",
       "18                   NYK     0.0         LOSS  \n",
       "19                   MIA    12.0          WIN  \n",
       "20                    AC     0.0         LOSS  \n",
       "21                          15.0          WIN  \n",
       "22                          11.0          WIN  \n",
       "23                   BKN    15.0          WIN  \n",
       "24                    AC     4.0          WIN  \n",
       "\n",
       "[25 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === EVALUATE YESTERDAY'S BETS (Europe/Athens) ‚Üí saves to /data/eval ===\n",
    "import os, re, numpy as np, pandas as pd\n",
    "from glob import glob\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# ------------------ settings: always \"day before\" in Europe/Athens ------------------\n",
    "TZ = ZoneInfo(\"Europe/Athens\")\n",
    "today_local = datetime.now(TZ).date()\n",
    "ydate = today_local - timedelta(days=1)                 # <-- Yesterday (local)\n",
    "ystr = ydate.strftime(\"%Y%m%d\")\n",
    "print(f\"Evaluating bets for YESTERDAY (Europe/Athens): {ydate} ({ystr})\")\n",
    "\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "\n",
    "# ------------------ helpers ------------------\n",
    "def _norm_player(s):\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    return re.sub(r\"[.`'‚Äô\\-]\", \"\", s.strip()).lower()\n",
    "\n",
    "def pick_col(df, candidates, default=np.nan):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return df[c]\n",
    "    return pd.Series([default]*len(df))\n",
    "\n",
    "def _first_float(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    m = re.search(r\"[-+]?\\d+(?:\\.\\d+)?\", str(x))\n",
    "    return float(m.group()) if m else np.nan\n",
    "\n",
    "def infer_opponent(df):\n",
    "    if \"OPPONENT_ABBREVIATION\" in df.columns:\n",
    "        return df[\"OPPONENT_ABBREVIATION\"]\n",
    "    matchup = pick_col(df, [\"MATCHUP\",\"Matchup\"])\n",
    "    team = pick_col(df, [\"TEAM_ABBREVIATION\",\"TEAM\"])\n",
    "    out = []\n",
    "    for t, m in zip(team.fillna(\"\"), matchup.fillna(\"\")):\n",
    "        opp = np.nan\n",
    "        if isinstance(m, str) and m:\n",
    "            parts = re.split(r\"[@vVsS]+\\.*\", m)\n",
    "            if len(parts) >= 2:\n",
    "                cand = parts[-1].strip().upper()\n",
    "                if cand == str(t).upper() and len(parts) >= 2:\n",
    "                    cand = parts[0].strip().upper()\n",
    "                opp = cand\n",
    "        out.append(opp)\n",
    "    return pd.Series(out, index=df.index)\n",
    "\n",
    "def _parse_date_from_filename(path, pattern):\n",
    "    m = re.search(pattern, os.path.basename(path))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# ------------------ 1) Pick yesterday's bets workbook ------------------\n",
    "bet_files = sorted(glob(\"data/bets/value_bets_top100_*.xlsx\"))\n",
    "if not bet_files:\n",
    "    raise FileNotFoundError(\"No bets files found in data/bets/. Run the Top-100 export first.\")\n",
    "\n",
    "# Prefer exact yesterday; if missing, use nearest earlier\n",
    "dated = []\n",
    "for p in bet_files:\n",
    "    ds = _parse_date_from_filename(p, r\"value_bets_top100_(\\d{8})\\.xlsx\")\n",
    "    if ds:\n",
    "        dated.append((ds, p))\n",
    "dated = sorted(dated, key=lambda x: x[0])\n",
    "\n",
    "bets_path = None\n",
    "for ds, p in reversed(dated):\n",
    "    if ds <= ystr:\n",
    "        bets_path = p\n",
    "        break\n",
    "if bets_path is None:\n",
    "    # fall back to earliest (shouldn't usually happen)\n",
    "    bets_path = dated[0][1]\n",
    "    print(\"‚ö†Ô∏è No bets file on/before yesterday; using earliest available:\", os.path.basename(bets_path))\n",
    "else:\n",
    "    print(\"Using bets workbook:\", os.path.basename(bets_path))\n",
    "\n",
    "# ------------------ 2) Load a non-empty Top100 sheet ------------------\n",
    "with pd.ExcelFile(bets_path) as xf:\n",
    "    sheet = None\n",
    "    for s in [\"Top100_priced\",\"Top100_all\",\"Top100\"]:\n",
    "        if s in xf.sheet_names:\n",
    "            tmp = pd.read_excel(bets_path, sheet_name=s)\n",
    "            if not tmp.empty:\n",
    "                sheet, bets = s, tmp\n",
    "                break\n",
    "if sheet is None:\n",
    "    raise RuntimeError(\"All Top100 sheets empty in bets workbook.\")\n",
    "\n",
    "print(f\"Loaded bets: sheet [{sheet}], rows={len(bets)}\")\n",
    "\n",
    "# ------------------ 3) Load boxscores for YESTERDAY ------------------\n",
    "# Priority: in-memory `box_d` filtered to ydate ‚Üí file nba_boxscores_YYYYMMDD.csv ‚Üí latest fallback\n",
    "def _as_date(s):\n",
    "    try:\n",
    "        return pd.to_datetime(s).date()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "box = None\n",
    "if \"box_d\" in globals() and isinstance(box_d, pd.DataFrame) and not box_d.empty:\n",
    "    bd = box_d.copy()\n",
    "    # Try to locate a date column and filter to yesterday\n",
    "    date_col = None\n",
    "    for c in [\"GAME_DATE\", \"GAME_DATE_EST\", \"GAME_DATE_LCL\", \"Date\", \"date\"]:\n",
    "        if c in bd.columns:\n",
    "            date_col = c\n",
    "            break\n",
    "    if date_col is not None:\n",
    "        bd[\"_gdate\"] = bd[date_col].apply(_as_date)\n",
    "        box = bd.loc[bd[\"_gdate\"].eq(ydate)].copy()\n",
    "        print(f\"box_d in memory ‚Üí filtered rows for {ydate}: {len(box)}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è box_d found but no date column to filter; using all rows.\")\n",
    "        box = bd.copy()\n",
    "\n",
    "if box is None or box.empty:\n",
    "    # Try file with exact date\n",
    "    exact_csv = f\"nba_boxscores_{ystr}.csv\"\n",
    "    if os.path.exists(exact_csv):\n",
    "        box = pd.read_csv(exact_csv)\n",
    "        print(f\"Loaded boxscores from file: {exact_csv} ({len(box)} rows)\")\n",
    "    else:\n",
    "        # pick latest available matching pattern\n",
    "        csv_files = sorted(glob(\"nba_boxscores_*.csv\"))\n",
    "        if csv_files:\n",
    "            box = pd.read_csv(csv_files[-1])\n",
    "            print(f\"‚ö†Ô∏è No boxscore file for {ystr}; using latest: {os.path.basename(csv_files[-1])} ({len(box)} rows)\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No boxscore CSV found (nba_boxscores_YYYYMMDD.csv).\")\n",
    "\n",
    "# ------------------ 4) Normalize & (optionally) filter box by yesterday ------------------\n",
    "# Try filter again if a date column exists (helps when we loaded a combined CSV)\n",
    "for c in [\"GAME_DATE\", \"GAME_DATE_EST\", \"GAME_DATE_LCL\", \"Date\", \"date\"]:\n",
    "    if c in box.columns:\n",
    "        _dates = pd.to_datetime(box[c], errors=\"coerce\").dt.date\n",
    "        if _dates.notna().any():\n",
    "            box = box.loc[_dates.eq(ydate)].copy()\n",
    "            print(f\"Filtered boxscores to {ydate} by column '{c}': {len(box)} rows\")\n",
    "        break\n",
    "\n",
    "# ------------------ 5) Prepare bets & box for join ------------------\n",
    "bets = bets.copy()\n",
    "bets[\"player\"] = pick_col(bets, [\"player\",\"Player\"])\n",
    "bets[\"market\"] = pick_col(bets, [\"market\",\"Market\"])\n",
    "bets[\"line\"]   = pd.to_numeric(pick_col(bets, [\"posted_line\",\"line\"]), errors=\"coerce\")\n",
    "bets[\"player_key\"] = bets[\"player\"].map(_norm_player)\n",
    "\n",
    "box = box.copy()\n",
    "box[\"player\"] = pick_col(box, [\"PLAYER_NAME\",\"Player\"])\n",
    "box[\"player_key\"] = box[\"player\"].map(_norm_player)\n",
    "box[\"PTS\"] = pd.to_numeric(pick_col(box, [\"PTS\",\"Points\"]), errors=\"coerce\")\n",
    "box[\"REB\"] = pd.to_numeric(pick_col(box, [\"REB\",\"Rebounds\"]), errors=\"coerce\")\n",
    "box[\"AST\"] = pd.to_numeric(pick_col(box, [\"AST\",\"Assists\"]), errors=\"coerce\")\n",
    "box[\"TEAM_ABBREVIATION\"] = pick_col(box, [\"TEAM_ABBREVIATION\",\"TEAM\"])\n",
    "box[\"OPPONENT_ABBREVIATION\"] = infer_opponent(box)\n",
    "\n",
    "# ------------------ 6) Join & grade ------------------\n",
    "joined = bets.merge(\n",
    "    box[[\"player_key\",\"PTS\",\"REB\",\"AST\",\"TEAM_ABBREVIATION\",\"OPPONENT_ABBREVIATION\"]],\n",
    "    on=\"player_key\", how=\"left\", suffixes=(\"\",\"_box\")\n",
    ")\n",
    "\n",
    "def pick_actual(row):\n",
    "    m = str(row.get(\"market\",\"\")).upper()\n",
    "    return row.get(m, np.nan) if m in [\"PTS\",\"REB\",\"AST\"] else np.nan\n",
    "\n",
    "joined[\"actual\"] = joined.apply(pick_actual, axis=1)\n",
    "joined[\"result_over\"] = np.where(\n",
    "    joined[\"actual\"].notna() & joined[\"line\"].notna(),\n",
    "    np.where(joined[\"actual\"] > joined[\"line\"], \"WIN\",\n",
    "    np.where(joined[\"actual\"] == joined[\"line\"], \"PUSH\", \"LOSS\")),\n",
    "    \"NA\"\n",
    ")\n",
    "\n",
    "# ------------------ 7) Summary ------------------\n",
    "graded = joined[\"result_over\"].isin([\"WIN\",\"LOSS\",\"PUSH\"]).sum()\n",
    "wins = (joined[\"result_over\"]==\"WIN\").sum()\n",
    "losses = (joined[\"result_over\"]==\"LOSS\").sum()\n",
    "pushes = (joined[\"result_over\"]==\"PUSH\").sum()\n",
    "hitrate = wins / max(wins+losses, 1)\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluated {graded} bets for {ydate}  (WIN={wins}, LOSS={losses}, PUSH={pushes})\")\n",
    "print(f\"üéØ Hit rate: {hitrate:.1%}\")\n",
    "\n",
    "# ------------------ 8) Save evaluation ------------------\n",
    "eval_out = os.path.join(\"xgb_outputs/evaluations\", f\"value_bets_top100_{ystr}_EVAL.csv\")\n",
    "joined.to_csv(eval_out, index=False)\n",
    "print(f\"üìä Saved evaluation to: {eval_out}\")\n",
    "\n",
    "display(joined.head(25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f4d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22b849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
